{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bbf242",
   "metadata": {},
   "source": [
    "# JUNO SCHOOL CPP LINK EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebe853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base configuration\n",
    "base_url = \"https://junoschool.org\"\n",
    "visited_urls = set()\n",
    "course_links = []\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def is_course_url(url):\n",
    "    \"\"\"Check if URL is a course page and not a pagination or listing page\"\"\"\n",
    "    url = url.lower()\n",
    "\n",
    "    # Must contain these patterns\n",
    "    course_patterns = [\n",
    "        '/free-certificate-course/',\n",
    "        '/course/',\n",
    "        '/certificate/',\n",
    "        '/training/',\n",
    "        '/learn/'\n",
    "    ]\n",
    "\n",
    "    # Must NOT contain these patterns\n",
    "    exclude_patterns = [\n",
    "        '/tag/',\n",
    "        '/category/',\n",
    "        '/page/',\n",
    "        '/author/',\n",
    "        '/archive/',\n",
    "        '/search/'\n",
    "    ]\n",
    "\n",
    "    # Check for at least one course pattern and no exclude patterns\n",
    "    return (any(pattern in url for pattern in course_patterns) and\n",
    "            not any(pattern in url for pattern in exclude_patterns) and\n",
    "            not url.endswith(('/courses/', '/all-courses/')))\n",
    "\n",
    "def extract_courses_from_url(url):\n",
    "    try:\n",
    "        print(f\"üîç Scanning: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Failed to access {url} - Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all links that might be courses\n",
    "        potential_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "        for a in potential_links:\n",
    "            href = a[\"href\"]\n",
    "            text = a.get_text(strip=True)\n",
    "\n",
    "            # Skip if no text or obviously not a course\n",
    "            if not text or len(text) < 3 or text.lower() in [\"home\", \"about\", \"contact\", \"login\", \"sign up\"]:\n",
    "                continue\n",
    "\n",
    "            # Construct full URL\n",
    "            full_url = urljoin(base_url, href)\n",
    "\n",
    "            # Check if this is a course URL and not already collected\n",
    "            if (full_url.startswith(base_url) and is_course_url(full_url) and \n",
    "                not any(x['Course Link'] == full_url for x in course_links)):\n",
    "\n",
    "                # Clean the course name\n",
    "                clean_name = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "                # If the name is too short, try getting a better title from the page\n",
    "                if len(clean_name) < 5:\n",
    "                    title_tag = soup.find('h1') or soup.find('h2') or soup.find('title')\n",
    "                    if title_tag:\n",
    "                        clean_name = title_tag.get_text(strip=True)\n",
    "\n",
    "                if clean_name:\n",
    "                    course_links.append({\"Course Name\": clean_name, \"Course Link\": full_url})\n",
    "                    print(f\"‚úÖ Found course: {clean_name} - {full_url}\")\n",
    "\n",
    "        # Queue more internal links for scanning\n",
    "        internal_links = soup.find_all(\"a\", href=re.compile(r\"^/|^\" + re.escape(base_url)))\n",
    "        for link in internal_links:\n",
    "            href = link[\"href\"]\n",
    "            full_internal_url = urljoin(base_url, href)\n",
    "\n",
    "            # Skip already visited or irrelevant URLs\n",
    "            if (full_internal_url.startswith(base_url) and\n",
    "                full_internal_url not in visited_urls and\n",
    "                not any(x in full_internal_url.lower() for x in [\"wp-admin\", \"wp-login\", \"feed\", \".jpg\", \".png\", \".pdf\"]) and\n",
    "                not re.search(r'/page/\\d+', full_internal_url.lower())):\n",
    "\n",
    "                visited_urls.add(full_internal_url)\n",
    "                extract_courses_from_url(full_internal_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {url}: {str(e)}\")\n",
    "\n",
    "# Start crawling from known pages\n",
    "start_urls = [\n",
    "    base_url,\n",
    "    base_url + \"/free-certificate-courses/\",\n",
    "    base_url + \"/courses/\",\n",
    "    base_url + \"/all-courses/\",\n",
    "    base_url + \"/free-certificate-course/\"\n",
    "]\n",
    "\n",
    "for url in start_urls:\n",
    "    if url not in visited_urls:\n",
    "        visited_urls.add(url)\n",
    "        extract_courses_from_url(url)\n",
    "\n",
    "# Save results to Excel\n",
    "if course_links:\n",
    "    df = pd.DataFrame(course_links)\n",
    "    df = df.drop_duplicates(subset=[\"Course Link\"]).reset_index(drop=True)\n",
    "\n",
    "    output_file = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\Juno_School_All_Courses.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully extracted {len(df)} unique courses!\")\n",
    "    print(f\"üìÅ Saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"‚ùå No courses found. Please check the website structure.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
