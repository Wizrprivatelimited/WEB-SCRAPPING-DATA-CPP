{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a72714e",
   "metadata": {},
   "source": [
    "# gitinfo.com CPPP LINK EXTRACTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d828f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base configuration\n",
    "base_url = \"https://gitinfo.com\"\n",
    "visited_urls = set()\n",
    "course_links = []\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def is_course_url(url):\n",
    "    \"\"\"Check if URL is a course page based on GIT Academy structure\"\"\"\n",
    "    url = url.lower()\n",
    "    \n",
    "    # Must contain these patterns\n",
    "    course_patterns = [\n",
    "        '/courses/',\n",
    "        '/course/',\n",
    "        '/training/',\n",
    "        '/learn/',\n",
    "        '/comptia-',\n",
    "        '/ccna/',\n",
    "        '/ccnp-',\n",
    "        '/ceh-',\n",
    "        '/chfi-',\n",
    "        '/certified-',\n",
    "        '/exam-'\n",
    "    ]\n",
    "    \n",
    "    # Must NOT contain these patterns\n",
    "    exclude_patterns = [\n",
    "        '/tag/',\n",
    "        '/category/',\n",
    "        '/page/',\n",
    "        '/author/',\n",
    "        '/search/',\n",
    "        '/wp-',\n",
    "        '/feed/'\n",
    "    ]\n",
    "    \n",
    "    return any(pattern in url for pattern in course_patterns) and \\\n",
    "           not any(pattern in url for pattern in exclude_patterns)\n",
    "\n",
    "def extract_courses_from_page(url):\n",
    "    try:\n",
    "        print(f\"üîç Scanning: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è Failed to access {url} - Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Extract course title from the page\n",
    "        title = soup.find('h1')\n",
    "        if title:\n",
    "            course_name = title.get_text(strip=True)\n",
    "            \n",
    "            # Only add if we don't already have this course\n",
    "            if not any(x['Course Link'] == url for x in course_links):\n",
    "                course_links.append({\n",
    "                    \"Course Name\": course_name,\n",
    "                    \"Course Link\": url\n",
    "                })\n",
    "                print(f\"‚úÖ Found course: {course_name} - {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {url}: {str(e)}\")\n",
    "\n",
    "def extract_courses_from_menu():\n",
    "    try:\n",
    "        print(\"üçî Extracting courses from main menu...\")\n",
    "        response = requests.get(base_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the main menu items\n",
    "        menu_items = soup.select('#menu-main-menu > li.menu-item')\n",
    "        \n",
    "        for item in menu_items:\n",
    "            # Check if this is a training courses menu item\n",
    "            if 'Training Courses' in item.get_text():\n",
    "                # Find all submenu items\n",
    "                sub_menus = item.select('ul.gm-dropdown-menu--lvl-1 li.gm-menu-item a')\n",
    "                \n",
    "                for menu in sub_menus:\n",
    "                    href = menu['href']\n",
    "                    text = menu.get_text(strip=True)\n",
    "                    \n",
    "                    # Skip empty or non-course links\n",
    "                    if not text or not href:\n",
    "                        continue\n",
    "                        \n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    # If this is a category page, we'll need to scan it\n",
    "                    if '/courses/' in full_url.lower():\n",
    "                        extract_courses_from_url(full_url)\n",
    "                    else:\n",
    "                        # Direct course link\n",
    "                        if not any(x['Course Link'] == full_url for x in course_links):\n",
    "                            course_links.append({\n",
    "                                \"Course Name\": text,\n",
    "                                \"Course Link\": full_url\n",
    "                            })\n",
    "                            print(f\"‚úÖ Found course: {text} - {full_url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error extracting menu: {str(e)}\")\n",
    "\n",
    "def extract_courses_from_url(url):\n",
    "    try:\n",
    "        if url in visited_urls:\n",
    "            return\n",
    "            \n",
    "        visited_urls.add(url)\n",
    "        print(f\"üåê Scanning category: {url}\")\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find course links on category pages\n",
    "        course_anchors = soup.select('a[href*=\"/courses/\"], a[href*=\"/course/\"]')\n",
    "        \n",
    "        for a in course_anchors:\n",
    "            href = a['href']\n",
    "            text = a.get_text(strip=True)\n",
    "            \n",
    "            if not text or not href:\n",
    "                continue\n",
    "                \n",
    "            full_url = urljoin(base_url, href)\n",
    "            \n",
    "            if is_course_url(full_url) and not any(x['Course Link'] == full_url for x in course_links):\n",
    "                course_links.append({\n",
    "                    \"Course Name\": text,\n",
    "                    \"Course Link\": full_url\n",
    "                })\n",
    "                print(f\"‚úÖ Found course: {text} - {full_url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing category {url}: {str(e)}\")\n",
    "\n",
    "# Start scraping\n",
    "extract_courses_from_menu()\n",
    "\n",
    "# Also scan known important pages\n",
    "important_pages = [\n",
    "    base_url + \"/courses/\",\n",
    "    base_url + \"/courses/cisco/\",\n",
    "    base_url + \"/courses/comptia/\",\n",
    "    base_url + \"/courses/ec-council/\",\n",
    "    base_url + \"/courses/certnexus/\",\n",
    "    base_url + \"/courses/microsoft/\",\n",
    "    base_url + \"/courses/cyber-security-master-program/\"\n",
    "]\n",
    "\n",
    "for page in important_pages:\n",
    "    extract_courses_from_url(page)\n",
    "\n",
    "# Save results to Excel\n",
    "if course_links:\n",
    "    df = pd.DataFrame(course_links)\n",
    "    df = df.drop_duplicates(subset=[\"Course Link\"]).reset_index(drop=True)\n",
    "\n",
    "    output_file = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\GIT_Academy_Courses.xlsx\"\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully extracted {len(df)} unique courses!\")\n",
    "    print(f\"üìÅ Saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"‚ùå No courses found. Please check the website structure.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
