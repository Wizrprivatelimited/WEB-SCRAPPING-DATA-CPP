{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad03c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Skillset Arena Course Extractor\n",
      "Targeting format: https://skillsetarena.com/topic-program/\n",
      "======================================================================\n",
      "üéØ Starting COMPREHENSIVE course extraction...\n",
      "======================================================================\n",
      "Target format: https://skillsetarena.com/topic-program/\n",
      "======================================================================\n",
      "üó∫Ô∏è  Extracting from sitemaps...\n",
      "   Checking: https://skillsetarena.com/sitemap.xml\n",
      "   Checking: https://skillsetarena.com/sitemap_index.xml\n",
      "   Checking: https://skillsetarena.com/wp-sitemap.xml\n",
      "   Checking: https://skillsetarena.com/sitemap.php\n",
      "   Checking: https://skillsetarena.com/post-sitemap.xml\n",
      "   Checking: https://skillsetarena.com/page-sitemap.xml\n",
      "üîß Generating potential course URLs...\n",
      "   Testing 360 generated URLs...\n",
      "   Progress: 50/360\n",
      "   Progress: 100/360\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class ComprehensiveCourseExtractor:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://skillsetarena.com\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Referer': 'https://www.google.com/',\n",
    "            'DNT': '1'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.visited = set()\n",
    "        self.course_links = set()\n",
    "        self.all_links = set()\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL is valid and belongs to the domain\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.netloc:\n",
    "                return False\n",
    "            return 'skillsetarena.com' in parsed.netloc\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def should_crawl(self, url):\n",
    "        \"\"\"Determine if we should crawl this URL\"\"\"\n",
    "        # Skip non-html files\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.exe', '.mp4', '.avi', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        \n",
    "        # Skip admin and utility pages\n",
    "        skip_keywords = [\n",
    "            '/wp-admin/', '/wp-json/', '/login', '/register', '/signin', '/signup',\n",
    "            '/cart/', '/checkout/', '/my-account/', '/account/', '/dashboard/',\n",
    "            '/tag/', '/category/', '/author/', '/feed/', '/wp-content/',\n",
    "            '/privacy-policy', '/terms-conditions', '/refund-policy',\n",
    "            'replytocom', 'share=', 'comment-', '?s=', '/page/', '/blog/',\n",
    "            '/about', '/contact', '/career', '/job', '/testimonial', '/faq',\n",
    "            '/support', '/tnc', '/ref-policy'\n",
    "        ]\n",
    "        if any(keyword in url.lower() for keyword in skip_keywords):\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def is_course_page(self, url, title=\"\", link_text=\"\"):\n",
    "        \"\"\"Enhanced course detection focusing on program/course URLs\"\"\"\n",
    "        url_lower = url.lower()\n",
    "        title_lower = title.lower()\n",
    "        link_text_lower = link_text.lower()\n",
    "        \n",
    "        # Course URL patterns (expanded to include all found patterns)\n",
    "        url_patterns = [\n",
    "            # Pattern: /something-program/ \n",
    "            re.search(r'/([a-z-]+)-program/$', url_lower),\n",
    "            # Pattern: /something-course/\n",
    "            re.search(r'/([a-z-]+)-course/$', url_lower),\n",
    "            # Pattern: /something-certification-program/\n",
    "            re.search(r'/([a-z-]+)-certification-program/$', url_lower),\n",
    "            # Pattern: /something-training/\n",
    "            re.search(r'/([a-z-]+)-training/$', url_lower),\n",
    "            # Pattern with online-certification\n",
    "            re.search(r'/([a-z-]+)-online-certification-program/$', url_lower),\n",
    "            # Simple program pages\n",
    "            re.search(r'/([a-z-]+)/$', url_lower) and any(word in url_lower for word in ['program', 'course', 'training']),\n",
    "        ]\n",
    "        \n",
    "        # Check if URL matches any pattern\n",
    "        url_matches_pattern = any(pattern for pattern in url_patterns if pattern)\n",
    "        \n",
    "        # Content indicators (expanded)\n",
    "        content_indicators = [\n",
    "            url_matches_pattern,\n",
    "            '-program/' in url_lower,\n",
    "            '-course/' in url_lower,\n",
    "            '-training/' in url_lower,\n",
    "            '-certification/' in url_lower,\n",
    "            'program/' in url_lower and len(url_lower.split('/')) > 2,\n",
    "            'course/' in url_lower and len(url_lower.split('/')) > 2,\n",
    "            'training/' in url_lower and len(url_lower.split('/')) > 2,\n",
    "            \n",
    "            # Title patterns\n",
    "            'program' in title_lower and len(title_lower) > 10,\n",
    "            'course' in title_lower and len(title_lower) > 10,\n",
    "            'training' in title_lower and len(title_lower) > 10,\n",
    "            'certification' in title_lower,\n",
    "            \n",
    "            # Link text patterns\n",
    "            'program' in link_text_lower and len(link_text_lower) > 5,\n",
    "            'course' in link_text_lower and len(link_text_lower) > 5,\n",
    "            'training' in link_text_lower and len(link_text_lower) > 5,\n",
    "            'certification' in link_text_lower,\n",
    "            'enroll' in link_text_lower,\n",
    "            'register' in link_text_lower,\n",
    "            'apply now' in link_text_lower,\n",
    "            'view more' in link_text_lower,\n",
    "        ]\n",
    "        \n",
    "        return any(content_indicators)\n",
    "\n",
    "    def extract_all_links_from_page(self, url):\n",
    "        \"\"\"Extract ALL links from a page with comprehensive parsing\"\"\"\n",
    "        try:\n",
    "            print(f\"üîç Extracting links from: {url}\")\n",
    "            response = self.session.get(url, timeout=15, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Check content type\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            if 'text/html' not in content_type:\n",
    "                return []\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Get page title\n",
    "            title = soup.find('title')\n",
    "            page_title = title.get_text().strip() if title else \"\"\n",
    "            \n",
    "            links_found = set()\n",
    "            \n",
    "            # Extract from all anchor tags\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href')\n",
    "                if not href or href.startswith(('javascript:', 'mailto:', 'tel:', '#')):\n",
    "                    continue\n",
    "                \n",
    "                # Convert to absolute URL\n",
    "                full_url = urljoin(url, href)\n",
    "                \n",
    "                # Normalize URL (remove fragments, sort query params)\n",
    "                parsed = urlparse(full_url)\n",
    "                normalized_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "                if parsed.query:\n",
    "                    normalized_url += f\"?{parsed.query}\"\n",
    "                \n",
    "                links_found.add(normalized_url)\n",
    "                \n",
    "                # Get link text for better course detection\n",
    "                link_text = link.get_text(strip=True)\n",
    "                \n",
    "                # Check if this is a course page (focus on URL structure)\n",
    "                if self.is_course_page(normalized_url, page_title, link_text):\n",
    "                    if normalized_url not in self.course_links:\n",
    "                        self.course_links.add(normalized_url)\n",
    "                        print(f\"üéØ COURSE FOUND: {normalized_url}\")\n",
    "                        if link_text:\n",
    "                            print(f\"   Link Text: {link_text}\")\n",
    "            \n",
    "            return list(links_found)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting from {url}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def deep_crawl_website(self, max_pages=100):\n",
    "        \"\"\"Deep crawl the entire website focusing on course pages\"\"\"\n",
    "        print(\"üöÄ Starting DEEP website crawl...\")\n",
    "        \n",
    "        # Start with multiple entry points that might contain course links\n",
    "        start_urls = [\n",
    "            self.base_url,\n",
    "            f\"{self.base_url}/programs/\",\n",
    "            f\"{self.base_url}/programs\",\n",
    "        ]\n",
    "        \n",
    "        queue = deque(start_urls)\n",
    "        for url in start_urls:\n",
    "            self.visited.add(url)\n",
    "        \n",
    "        page_count = 0\n",
    "        \n",
    "        while queue and page_count < max_pages:\n",
    "            url = queue.popleft()\n",
    "            page_count += 1\n",
    "            \n",
    "            print(f\"\\nüìñ [{page_count}/{max_pages}] Crawling: {url}\")\n",
    "            \n",
    "            links = self.extract_all_links_from_page(url)\n",
    "            \n",
    "            # Add new links to queue\n",
    "            new_links_added = 0\n",
    "            for link in links:\n",
    "                if self.is_valid_url(link) and self.should_crawl(link):\n",
    "                    self.all_links.add(link)\n",
    "                    \n",
    "                    if link not in self.visited and link not in queue:\n",
    "                        # HIGH PRIORITY: Links that match our desired course URL pattern\n",
    "                        if any(pattern in link.lower() for pattern in \n",
    "                              ['-program/', '-course/', '-training/', '-certification/']):\n",
    "                            queue.appendleft(link)  # Highest priority\n",
    "                            new_links_added += 1\n",
    "                        # MEDIUM PRIORITY: Other course-related directories\n",
    "                        elif any(pattern in link.lower() for pattern in \n",
    "                                ['/program/', '/course/', '/training/', '/certification/']):\n",
    "                            queue.appendleft(link)  # High priority\n",
    "                            new_links_added += 1\n",
    "                        # LOW PRIORITY: All other valid links\n",
    "                        else:\n",
    "                            queue.append(link)\n",
    "                            new_links_added += 1\n",
    "                        \n",
    "                        self.visited.add(link)\n",
    "            \n",
    "            print(f\"   Found {len(links)} links, added {new_links_added} new URLs\")\n",
    "            print(f\"   Total course links so far: {len(self.course_links)}\")\n",
    "            \n",
    "            # Random delay to be respectful\n",
    "            time.sleep(random.uniform(0.3, 1.0))\n",
    "\n",
    "    def generate_course_urls(self):\n",
    "        \"\"\"Generate potential course URLs based on common patterns\"\"\"\n",
    "        print(\"üîß Generating potential course URLs...\")\n",
    "        \n",
    "        # Based on the found courses, let's generate similar patterns\n",
    "        course_topics = [\n",
    "            'sql', 'structured-query-language', 'python', 'java', 'javascript', \n",
    "            'data-science', 'machine-learning', 'artificial-intelligence', 'ai',\n",
    "            'deep-learning', 'nlp', 'natural-language-processing', 'computer-vision',\n",
    "            'data-analytics', 'business-analytics', 'tableau', 'power-bi', 'excel',\n",
    "            'big-data', 'hadoop', 'spark', 'aws', 'azure', 'gcp', 'cloud-computing',\n",
    "            'devops', 'cybersecurity', 'web-development', 'full-stack', 'frontend',\n",
    "            'backend', 'react', 'angular', 'nodejs', 'html-css', 'php', 'mysql',\n",
    "            'mongodb', 'oracle', 'postgresql', 'linux', 'networking', 'ethical-hacking',\n",
    "            'digital-marketing', 'seo', 'social-media-marketing', 'content-marketing',\n",
    "            'project-management', 'agile', 'scrum', 'product-management',\n",
    "            'generative-ai', 'prompt-engineering', 'looker-studio', 'google-sheet',\n",
    "            'microsoft-excel', 'cyber-security', 'investment-banking', 'life-skill',\n",
    "            'executive'\n",
    "        ]\n",
    "        \n",
    "        # URL patterns that match the found format\n",
    "        url_patterns = [\n",
    "            \"/{topic}-program/\",\n",
    "            \"/{topic}-course/\",\n",
    "            \"/{topic}-training/\",\n",
    "            \"/{topic}-certification/\",\n",
    "            \"/{topic}-online-certification-program/\",\n",
    "            \"/{topic}-certification-program/\",\n",
    "        ]\n",
    "        \n",
    "        generated_urls = set()\n",
    "        for topic in course_topics:\n",
    "            for pattern in url_patterns:\n",
    "                url = f\"{self.base_url}{pattern.format(topic=topic)}\"\n",
    "                generated_urls.add(url)\n",
    "        \n",
    "        # Test all generated URLs\n",
    "        print(f\"   Testing {len(generated_urls)} generated URLs...\")\n",
    "        tested_count = 0\n",
    "        for url in generated_urls:\n",
    "            tested_count += 1\n",
    "            if tested_count % 50 == 0:\n",
    "                print(f\"   Progress: {tested_count}/{len(generated_urls)}\")\n",
    "                \n",
    "            try:\n",
    "                response = self.session.head(url, timeout=5, allow_redirects=True)\n",
    "                if response.status_code == 200:\n",
    "                    final_url = response.url\n",
    "                    if self.is_course_page(final_url):\n",
    "                        if final_url not in self.course_links:\n",
    "                            self.course_links.add(final_url)\n",
    "                            print(f\"‚úÖ Generated URL found: {final_url}\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def extract_from_sitemaps(self):\n",
    "        \"\"\"Extract from all possible sitemap variations - FIXED VERSION\"\"\"\n",
    "        print(\"üó∫Ô∏è  Extracting from sitemaps...\")\n",
    "        \n",
    "        sitemap_urls = [\n",
    "            f\"{self.base_url}/sitemap.xml\",\n",
    "            f\"{self.base_url}/sitemap_index.xml\",\n",
    "            f\"{self.base_url}/wp-sitemap.xml\",\n",
    "            f\"{self.base_url}/sitemap.php\",\n",
    "            f\"{self.base_url}/post-sitemap.xml\",\n",
    "            f\"{self.base_url}/page-sitemap.xml\",\n",
    "        ]\n",
    "        \n",
    "        for sitemap_url in sitemap_urls:\n",
    "            try:\n",
    "                print(f\"   Checking: {sitemap_url}\")\n",
    "                response = self.session.get(sitemap_url, timeout=10)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Try different parsing methods\n",
    "                    try:\n",
    "                        # Try XML parsing first\n",
    "                        soup = BeautifulSoup(response.content, 'lxml')\n",
    "                        urls = soup.find_all('loc')\n",
    "                        \n",
    "                        for url in urls:\n",
    "                            url_text = url.get_text().strip()\n",
    "                            if self.is_valid_url(url_text) and self.is_course_page(url_text):\n",
    "                                if url_text not in self.course_links:\n",
    "                                    self.course_links.add(url_text)\n",
    "                                    print(f\"‚úÖ Course from sitemap: {url_text}\")\n",
    "                    except:\n",
    "                        # Fallback to text parsing\n",
    "                        try:\n",
    "                            urls = response.text.split('\\n')\n",
    "                            for url_text in urls:\n",
    "                                url_text = url_text.strip()\n",
    "                                if url_text and self.is_valid_url(url_text) and self.is_course_page(url_text):\n",
    "                                    if url_text not in self.course_links:\n",
    "                                        self.course_links.add(url_text)\n",
    "                                        print(f\"‚úÖ Course from sitemap: {url_text}\")\n",
    "                        except:\n",
    "                            pass\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not process {sitemap_url}: {e}\")\n",
    "\n",
    "    def get_all_courses(self):\n",
    "        \"\"\"Main method to get ALL course links in the desired format\"\"\"\n",
    "        print(\"üéØ Starting COMPREHENSIVE course extraction...\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Target format: https://skillsetarena.com/topic-program/\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Method 1: Extract from sitemaps (fastest)\n",
    "        self.extract_from_sitemaps()\n",
    "        \n",
    "        # Method 2: Generate and test potential URLs\n",
    "        self.generate_course_urls()\n",
    "        \n",
    "        # Method 3: Deep crawl website\n",
    "        self.deep_crawl_website(max_pages=100)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Extraction complete! Found {len(self.course_links)} course links\")\n",
    "        return sorted(list(self.course_links))\n",
    "\n",
    "    def save_to_excel(self, file_path=None):\n",
    "        \"\"\"Save ALL course links to Excel - FIXED VERSION\"\"\"\n",
    "        if file_path is None:\n",
    "            file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\skill_course_links.xlsx\"\n",
    "        \n",
    "        try:\n",
    "            # Prepare ALL course data (not filtering)\n",
    "            data = []\n",
    "            for i, url in enumerate(sorted(self.course_links), 1):\n",
    "                # Extract course name from URL\n",
    "                course_name = \"\"\n",
    "                # Try to extract meaningful course name\n",
    "                if '-program/' in url:\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                elif '-course/' in url:\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                elif '-training/' in url:\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                elif '-certification/' in url:\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                else:\n",
    "                    course_name = \"Course Program\"\n",
    "                \n",
    "                data.append({\n",
    "                    'S.No': i,\n",
    "                    'Course_Name': course_name,\n",
    "                    'Course_URL': url,\n",
    "                    'Website': 'Skillset Arena',\n",
    "                    'Extraction_Date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Save to Excel\n",
    "            df.to_excel(file_path, index=False)\n",
    "            \n",
    "            print(f\"\\nüíæ RESULTS SAVED!\")\n",
    "            print(f\"   üìÅ File: {file_path}\")\n",
    "            print(f\"   üìä Total Courses Saved: {len(df)}\")\n",
    "            print(f\"   üìÑ Pages Crawled: {len(self.visited)}\")\n",
    "            \n",
    "            # Show ALL found courses\n",
    "            if len(df) > 0:\n",
    "                print(f\"\\nüìã ALL COURSES FOUND ({len(df)} total):\")\n",
    "                print(\"-\" * 80)\n",
    "                for i, row in df.iterrows():\n",
    "                    print(f\"   {row['S.No']:2d}. {row['Course_URL']}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to Excel: {e}\")\n",
    "            return False\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ Skillset Arena Course Extractor\")\n",
    "    print(\"Targeting format: https://skillsetarena.com/topic-program/\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create extractor and get courses\n",
    "    extractor = ComprehensiveCourseExtractor()\n",
    "    all_courses = extractor.get_all_courses()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüéâ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä FINAL SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Total Course Links Found: {len(all_courses)}\")\n",
    "    print(f\"   üîç Total Pages Crawled: {len(extractor.visited)}\")\n",
    "    print(f\"   üåê Total Links Discovered: {len(extractor.all_links)}\")\n",
    "    \n",
    "    # Save to Excel\n",
    "    output_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\skill_course_links.xlsx\"\n",
    "    extractor.save_to_excel(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a64a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "üåê Accessing URL: https://skillsetarena.com/data-science-and-machine-learning-with-ai/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "üìù About Course: Generative AI & Prompt Engineering:...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: ‚Ä¢ Duration: 6 months\n",
      "üéØ Who Should Take: Generative AI & Prompt Engineering:...\n",
      "üéì Learning Mode: LLM ‚ÄìLarge Language Models\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "‚úÖ Successfully scraped: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "üéâ Process completed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    try:\n",
    "        syllabus_text = \"\"\n",
    "        \n",
    "        # Find the Program Structure section\n",
    "        program_structure = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Program Structure\" in text)\n",
    "        \n",
    "        if not program_structure:\n",
    "            print(\"‚ö†Ô∏è Program Structure section not found\")\n",
    "            return \"Syllabus not available\"\n",
    "        \n",
    "        # Find all elements after Program Structure\n",
    "        current_element = program_structure.find_next_sibling()\n",
    "        \n",
    "        while current_element:\n",
    "            # Check for month headings\n",
    "            if current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                month_text = current_element.get_text(strip=True)\n",
    "                if \"Month\" in month_text:\n",
    "                    syllabus_text += f\"\\n{month_text}\\n\"\n",
    "            \n",
    "            # Check for module details\n",
    "            elif current_element.name == \"details\" and \"wp-block-details\" in current_element.get(\"class\", []):\n",
    "                # Get module summary\n",
    "                summary = current_element.find(\"summary\")\n",
    "                if summary:\n",
    "                    syllabus_text += f\"üîπ {summary.get_text(strip=True)}\\n\"\n",
    "                \n",
    "                # Get the list items\n",
    "                lessons_list = current_element.find(\"ul\", class_=\"wp-block-list\")\n",
    "                if lessons_list:\n",
    "                    lessons = lessons_list.find_all(\"li\")\n",
    "                    for lesson in lessons:\n",
    "                        lesson_text = lesson.get_text(strip=True)\n",
    "                        if lesson_text:\n",
    "                            syllabus_text += f\"    . {lesson_text}\\n\"\n",
    "                \n",
    "                syllabus_text += \"\\n\"\n",
    "            \n",
    "            # Stop if we reach the next major section\n",
    "            elif current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                next_section_text = current_element.get_text(strip=True)\n",
    "                if \"Key Features\" in next_section_text or \"Tools & Technologies\" in next_section_text:\n",
    "                    break\n",
    "            \n",
    "            current_element = current_element.find_next_sibling()\n",
    "        \n",
    "        return syllabus_text.strip() if syllabus_text else \"Syllabus not available\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Syllabus extraction error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return \"Syllabus extraction failed\"\n",
    "\n",
    "# -------------------- SYLLABUS CURRICULUM EXTRACTION --------------------\n",
    "def extract_syllabus_curriculum(soup):\n",
    "    try:\n",
    "        curriculum_text = \"\"\n",
    "        \n",
    "        # Find the Detailed Curriculum section\n",
    "        detailed_curriculum = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Detailed Curriculum\" in text)\n",
    "        \n",
    "        if not detailed_curriculum:\n",
    "            print(\"‚ö†Ô∏è Detailed Curriculum section not found\")\n",
    "            return \"Curriculum not available\"\n",
    "        \n",
    "        # Find all elements after Detailed Curriculum\n",
    "        current_element = detailed_curriculum.find_next_sibling()\n",
    "        \n",
    "        while current_element:\n",
    "            # Check for month headings\n",
    "            if current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                month_text = current_element.get_text(strip=True)\n",
    "                if \"Month\" in month_text:\n",
    "                    curriculum_text += f\"\\n{month_text}\\n\"\n",
    "            \n",
    "            # Check for module details\n",
    "            elif current_element.name == \"details\" and \"wp-block-details\" in current_element.get(\"class\", []):\n",
    "                # Get module summary\n",
    "                summary = current_element.find(\"summary\")\n",
    "                if summary:\n",
    "                    curriculum_text += f\"üîπ {summary.get_text(strip=True)}\\n\"\n",
    "                \n",
    "                # Get the list items\n",
    "                lessons_list = current_element.find(\"ul\", class_=\"wp-block-list\")\n",
    "                if lessons_list:\n",
    "                    lessons = lessons_list.find_all(\"li\")\n",
    "                    for lesson in lessons:\n",
    "                        lesson_text = lesson.get_text(strip=True)\n",
    "                        if lesson_text:\n",
    "                            curriculum_text += f\"    . {lesson_text}\\n\"\n",
    "                \n",
    "                curriculum_text += \"\\n\"\n",
    "            \n",
    "            # Stop if we reach the next major section\n",
    "            elif current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                next_section_text = current_element.get_text(strip=True)\n",
    "                if \"Program Summary\" in next_section_text or \"Final Outcome\" in next_section_text or \"Learning Approach\" in next_section_text:\n",
    "                    break\n",
    "            \n",
    "            current_element = current_element.find_next_sibling()\n",
    "        \n",
    "        return curriculum_text.strip() if curriculum_text else \"Curriculum not available\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Syllabus curriculum extraction error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return \"Syllabus curriculum extraction failed\"\n",
    "\n",
    "# -------------------- EXTRACT DURATION --------------------\n",
    "def extract_duration(soup):\n",
    "    try:\n",
    "        # Look for duration in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Duration\" in text:\n",
    "                return text\n",
    "        \n",
    "        # Alternative search\n",
    "        duration_elem = soup.find(string=re.compile(\"Duration\", re.IGNORECASE))\n",
    "        if duration_elem:\n",
    "            return duration_elem.strip()\n",
    "        \n",
    "        return \"Duration not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Duration extraction error: {str(e)}\")\n",
    "        return \"Duration not available\"\n",
    "\n",
    "# -------------------- EXTRACT CERTIFICATE INFO --------------------\n",
    "def extract_certificate(soup):\n",
    "    try:\n",
    "        # Look for certificate information\n",
    "        certificate_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in certificate_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"certificate\" in text.lower():\n",
    "                return text\n",
    "        \n",
    "        # Look in certificate of completion section\n",
    "        cert_section = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Certificate of Completion\" in text)\n",
    "        if cert_section:\n",
    "            cert_list = cert_section.find_next(\"ul\", class_=\"wp-block-list\")\n",
    "            if cert_list:\n",
    "                cert_item = cert_list.find(\"li\", class_=\"has-small-font-size\")\n",
    "                if cert_item:\n",
    "                    return cert_item.get_text(strip=True)\n",
    "        \n",
    "        return \"Certificate information not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Certificate extraction error: {str(e)}\")\n",
    "        return \"Certificate information not available\"\n",
    "\n",
    "# -------------------- EXTRACT WHO SHOULD TAKE --------------------\n",
    "def extract_who_should_take(soup):\n",
    "    try:\n",
    "        # Look for the course description paragraph that describes who should take the course\n",
    "        course_name_elem = soup.find(\"p\", class_=\"has-upper-heading-font-size\")\n",
    "        if course_name_elem:\n",
    "            # Find the next paragraph with the description (this is usually the target audience description)\n",
    "            next_elem = course_name_elem.find_next_sibling()\n",
    "            while next_elem:\n",
    "                if next_elem.name == \"p\" and \"has-small-font-size\" in next_elem.get(\"class\", []):\n",
    "                    description_text = next_elem.get_text(strip=True)\n",
    "                    if description_text and not description_text.startswith(\"Program Overview\"):\n",
    "                        # This is typically the description that explains who the course is for\n",
    "                        return description_text\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "        \n",
    "        # Fallback: Look for prerequisites in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Prerequisites\" in text or \"suitable for beginners\" in text.lower():\n",
    "                return text\n",
    "        \n",
    "        return \"Target audience information not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Who should take extraction error: {str(e)}\")\n",
    "        return \"Target audience information not available\"\n",
    "\n",
    "# -------------------- EXTRACT LEARNING MODE --------------------\n",
    "def extract_learning_mode(soup):\n",
    "    try:\n",
    "        # Look for mode in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Mode:\" in text or \"Mode\" in text:\n",
    "                return text\n",
    "        \n",
    "        # Alternative search for learning mode\n",
    "        mode_elem = soup.find(string=re.compile(r\"Mode:|Instructor-led|live online\", re.IGNORECASE))\n",
    "        if mode_elem:\n",
    "            return mode_elem.strip()\n",
    "        \n",
    "        return \"Learning mode not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Learning mode extraction error: {str(e)}\")\n",
    "        return \"Learning mode not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver(headless=False)  # Set to False for debugging\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        # Scroll multiple times to ensure all content loads\n",
    "        for i in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Debug: Save page source for inspection\n",
    "        with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(\"üíæ Page source saved as page_source.html\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name\n",
    "        course_name_elem = soup.find(\"p\", class_=\"has-upper-heading-font-size\")\n",
    "        course_name = course_name_elem.get_text(strip=True) if course_name_elem else \"Course Name Not Found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. About Course - Get the first paragraph after course name\n",
    "        about_course = \"About course not available\"\n",
    "        if course_name_elem:\n",
    "            # Find the next paragraph with the description\n",
    "            next_elem = course_name_elem.find_next_sibling()\n",
    "            while next_elem:\n",
    "                if next_elem.name == \"p\" and \"has-small-font-size\" in next_elem.get(\"class\", []):\n",
    "                    about_text = next_elem.get_text(strip=True)\n",
    "                    if about_text and not about_text.startswith(\"Program Overview\"):\n",
    "                        about_course = about_text\n",
    "                        break\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        # 3. Syllabus\n",
    "        syllabus = extract_syllabus(soup)\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus)} characters)\")\n",
    "        \n",
    "        # Debug: Print first 500 chars of syllabus\n",
    "        print(f\"üìñ Syllabus preview: {syllabus[:500]}...\")\n",
    "\n",
    "        # 4. Syllabus Curriculum\n",
    "        syllabus_curriculum = extract_syllabus_curriculum(soup)\n",
    "        print(f\"üìò Syllabus Curriculum extracted ({len(syllabus_curriculum)} characters)\")\n",
    "        \n",
    "        # Debug: Print first 500 chars of syllabus curriculum\n",
    "        print(f\"üìñ Syllabus Curriculum preview: {syllabus_curriculum[:500]}...\")\n",
    "\n",
    "        # 5. Certificate\n",
    "        certificate_info = extract_certificate(soup)\n",
    "        print(f\"üèÜ Certificate: {certificate_info}\")\n",
    "\n",
    "        # 6. Duration\n",
    "        duration = extract_duration(soup)\n",
    "        print(f\"‚è≥ Duration: {duration}\")\n",
    "\n",
    "        # 7. Who Should Take\n",
    "        who_should_take = extract_who_should_take(soup)\n",
    "        print(f\"üéØ Who Should Take: {who_should_take[:100]}...\")\n",
    "\n",
    "        # 8. Learning Mode\n",
    "        learning_mode = extract_learning_mode(soup)\n",
    "        print(f\"üéì Learning Mode: {learning_mode}\")\n",
    "\n",
    "        return course_name, about_course, syllabus, syllabus_curriculum, certificate_info, duration, who_should_take, learning_mode\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return [\"Error\"] * 8\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \n",
    "        \"About Course\", \n",
    "        \"Syllabus\", \n",
    "        \"Syllabus Curriculum\", \n",
    "        \"Certificate\", \n",
    "        \"Duration\",\n",
    "        \"Who Should Take\",\n",
    "        \"Learning Mode\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Skip if course already exists\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        # Add new row with matching columns\n",
    "        new_row = pd.DataFrame([dict(zip(columns, [*data[:8], data[-1]]))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "        \"https://skillsetarena.com/data-science-and-machine-learning-with-ai/\",\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\course_smartset.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel((*course_data, course_url), file_path)\n",
    "            print(f\"‚úÖ Successfully scraped: {course_data[0]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"üéâ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b042ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "üìñ Found 17 URLs in the Excel file\n",
      "üîó URLs to scrape: ['https://skillsetarena.com/cyber-security-program/', 'https://skillsetarena.com/data-science-11-online-certification-program/', 'https://skillsetarena.com/data-science-21-online-certification-program/', 'https://skillsetarena.com/data-science-and-machine-learning-with-ai/', 'https://skillsetarena.com/executive-certification-program/', 'https://skillsetarena.com/full-stack-development/', 'https://skillsetarena.com/generative-ai-and-prompt-engineering/', 'https://skillsetarena.com/google-looker-studio-program/', 'https://skillsetarena.com/google-sheet-certification-program/', 'https://skillsetarena.com/investment-banking/', 'https://skillsetarena.com/life-skill-program/', 'https://skillsetarena.com/machine-learning-artificial-intelligence-11-online-certification-program/', 'https://skillsetarena.com/machine-learning-artificial-intelligence-21-online-certification-program/', 'https://skillsetarena.com/microsoft-excel-program/', 'https://skillsetarena.com/powerbi/', 'https://skillsetarena.com/pythoncourse/', 'https://skillsetarena.com/structured-query-language-program/']\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/cyber-security-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/cyber-security-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Cyber Security Certification Program\n",
      "üìù About Course: About course not available...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration:\n",
      "üéØ Who Should Take: Target audience information not available...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Cyber Security Certification Program\n",
      "‚úÖ Successfully scraped: Cyber Security Certification Program\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/data-science-11-online-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/data-science-11-online-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Data Science Short Term Program By SkillSet Arena\n",
      "üìù About Course: Total Duration: 2 Months...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Total Duration: 2 Months\n",
      "üéØ Who Should Take: Total Duration: 2 Months...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Science Short Term Program By SkillSet Arena\n",
      "‚úÖ Successfully scraped: Data Science Short Term Program By SkillSet Arena\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/data-science-21-online-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/data-science-21-online-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Data Science Short Term Program By SkillSet Arena\n",
      "üìù About Course: Total Duration: 3 Months...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Total Duration: 3 Months\n",
      "üéØ Who Should Take: Total Duration: 3 Months...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Science Short Term Program By SkillSet Arena\n",
      "‚úÖ Successfully scraped: Data Science Short Term Program By SkillSet Arena\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/data-science-and-machine-learning-with-ai/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/data-science-and-machine-learning-with-ai/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "üìù About Course: Generative AI & Prompt Engineering:...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: ‚Ä¢ Duration: 6 months\n",
      "üéØ Who Should Take: Generative AI & Prompt Engineering:...\n",
      "üéì Learning Mode: LLM ‚ÄìLarge Language Models\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "‚úÖ Successfully scraped: Data Science and Machine learning Certification Program With AI(Gen AI & Prompt Engineering)\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/executive-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/executive-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Executive Certification Program in Elite Business Development, Sales Leadership & Growth Management\n",
      "üìù About Course: The Executive Program in Elite Business, Sales Leadership, and Growth Management is designed for hig...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration\n",
      "üéØ Who Should Take: The Executive Program in Elite Business, Sales Leadership, and Growth Management is designed for hig...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Executive Certification Program in Elite Business Development, Sales Leadership & Growth Management\n",
      "‚úÖ Successfully scraped: Executive Certification Program in Elite Business Development, Sales Leadership & Growth Management\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/full-stack-development/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/full-stack-development/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Full Stack Certification Program with SkillSet Arena\n",
      "üìù About Course: The 6-month Full Stack Online Certification Program with SkillSet Arena is designed for beginners wh...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "üìò Syllabus Curriculum extracted (4216 characters)\n",
      "üìñ Syllabus Curriculum preview: Month 1: Introduction to Web Development:\n",
      "üîπ Module 1-2: HTML & CSS Fundamentals\n",
      "    . Semantic HTML: headers, footers, articles.\n",
      "    . CSS basics: Flexbox, Grid for layout design.\n",
      "    . Responsive Design: Media queries, mobile-first approach.\n",
      "    . Learning Outcome: Build a simple, responsive web page using HTML and CSS.\n",
      "\n",
      "üîπ Module 3-4: JavaScript Basics\n",
      "    . JavaScript syntax, variables, functions, conditions, loops.\n",
      "    . DOM Manipulation: Selectors, events, basic interactions.\n",
      "    . Debugging...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration: 6 Months.\n",
      "üéØ Who Should Take: The 6-month Full Stack Online Certification Program with SkillSet Arena is designed for beginners wh...\n",
      "üéì Learning Mode: Mode: Instructor-led live online classes.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Full Stack Certification Program with SkillSet Arena\n",
      "‚úÖ Successfully scraped: Full Stack Certification Program with SkillSet Arena\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/generative-ai-and-prompt-engineering/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/generative-ai-and-prompt-engineering/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Generative AI Mastery: From Fundamentals to Advanced Applications\n",
      "üìù About Course: About course not available...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration:\n",
      "üéØ Who Should Take: Target audience information not available...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Generative AI Mastery: From Fundamentals to Advanced Applications\n",
      "‚úÖ Successfully scraped: Generative AI Mastery: From Fundamentals to Advanced Applications\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/google-looker-studio-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/google-looker-studio-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Google Looker Studio Program(Fundamentals to Advanced Reporting)\n",
      "üìù About Course: The Google Looker Studio Program is designed to help individuals master data visualization and repor...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Program Duration:\n",
      "üéØ Who Should Take: The Google Looker Studio Program is designed to help individuals master data visualization and repor...\n",
      "üéì Learning Mode: Advanced Data Modelling, Creating Calculated Fields and Custom Metrics, Blending Data from Multiple Sources, Understanding Data Relationships and Joins.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Google Looker Studio Program(Fundamentals to Advanced Reporting)\n",
      "‚úÖ Successfully scraped: Google Looker Studio Program(Fundamentals to Advanced Reporting)\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/google-sheet-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/google-sheet-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Google Sheets Program with SkillSet Arena Certification\n",
      "üìù About Course: The Google Sheets Certification Program equips participants with the skills to master Google‚Äôs power...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Program Duration:\n",
      "üéØ Who Should Take: The Google Sheets Certification Program equips participants with the skills to master Google‚Äôs power...\n",
      "üéì Learning Mode: Financial Analysis and Budgeting Using Financial Functions: NPV, IRR, PMT, Building Financial Models in Google Sheets, Budget Tracking and Forecasting.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Google Sheets Program with SkillSet Arena Certification\n",
      "‚úÖ Successfully scraped: Google Sheets Program with SkillSet Arena Certification\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/investment-banking/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/investment-banking/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Investment Banking Certification program\n",
      "üìù About Course: About course not available...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration\n",
      "üéØ Who Should Take: Target audience information not available...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Investment Banking Certification program\n",
      "‚úÖ Successfully scraped: Investment Banking Certification program\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/life-skill-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/life-skill-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Mastering Life Skills & Soft Skills for Personal and Professional Growth\n",
      "üìù About Course: The Life Skills Program is designed to equip individuals with essential tools for personal and profe...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration not available\n",
      "üéØ Who Should Take: The Life Skills Program is designed to equip individuals with essential tools for personal and profe...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Mastering Life Skills & Soft Skills for Personal and Professional Growth\n",
      "‚úÖ Successfully scraped: Mastering Life Skills & Soft Skills for Personal and Professional Growth\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/machine-learning-artificial-intelligence-11-online-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/machine-learning-artificial-intelligence-11-online-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "üìù About Course: Duration: 2 Months...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration: 2 Months\n",
      "üéØ Who Should Take: Duration: 2 Months...\n",
      "üéì Learning Mode: Model Codebase (GitHub Repository).\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "‚úÖ Successfully scraped: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/machine-learning-artificial-intelligence-21-online-certification-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/machine-learning-artificial-intelligence-21-online-certification-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "üìù About Course: Total Duration: 3 Months...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Total Duration: 3 Months\n",
      "üéØ Who Should Take: Total Duration: 3 Months...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "‚úÖ Successfully scraped: Machine Learning & Artificial Intelligence Short Term Program By SkillSet Arena\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/microsoft-excel-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/microsoft-excel-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Microsoft Excel Program with SkillSet Arena Certification\n",
      "üìù About Course: The Microsoft Excel Program is a comprehensive course designed to take users from beginner to advanc...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Upon successful completion of the final exam, participants will receive aCertificate of Excel Proficiencywhich can be shared on LinkedIn and added to resumes.\n",
      "‚è≥ Duration: Program Duration:\n",
      "üéØ Who Should Take: The Microsoft Excel Program is a comprehensive course designed to take users from beginner to advanc...\n",
      "üéì Learning Mode: Financial Functions and Analysis, Using Financial Functions: NPV, IRR, PMT, FV, Building Loan Amortization Schedules, Introduction to Financial Modelling.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Microsoft Excel Program with SkillSet Arena Certification\n",
      "‚úÖ Successfully scraped: Microsoft Excel Program with SkillSet Arena Certification\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/powerbi/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/powerbi/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Business Analytics Certification Program with Microsoft Power BI\n",
      "üìù About Course: Month 1: Foundations of Business Analytics & Power BI Basics...\n",
      "‚ö†Ô∏è Program Structure section not found\n",
      "üìö Syllabus extracted (22 characters)\n",
      "üìñ Syllabus preview: Syllabus not available...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Duration: Up to 6 months.\n",
      "üéØ Who Should Take: Month 1: Foundations of Business Analytics & Power BI Basics...\n",
      "üéì Learning Mode: Mode: Instructor-led live online course with weekly live Q&A sessions and assignments.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Business Analytics Certification Program with Microsoft Power BI\n",
      "‚úÖ Successfully scraped: Business Analytics Certification Program with Microsoft Power BI\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/pythoncourse/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/pythoncourse/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: Python Certification Program with AI\n",
      "üìù About Course: This program will focus on building foundational Python skills while introducing core AI concepts an...\n",
      "üìö Syllabus extracted (3236 characters)\n",
      "üìñ Syllabus preview: Month 1: Introduction to Python & Programming Basics:\n",
      "üîπ Module 1:\n",
      "    . What is Python? Installing Python (IDLE, Anaconda).\n",
      "    . Understanding IDEs (Jupyter Notebook, VS Code).\n",
      "    . Python syntax, variables, data types (int, float, str, bool).\n",
      "\n",
      "üîπ Module 2:\n",
      "    . Control structures: If statements, loops (for, while).\n",
      "    . Functions: Defining, arguments, return values.\n",
      "\n",
      "üîπ Module 3:\n",
      "    . Basic input/output.\n",
      "    . Error handling and debugging.\n",
      "\n",
      "üîπ Module 4:\n",
      "    . Project: Build a basic calculator...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Earn a certificate upon completing the course and final project fromSkillSet Arena.\n",
      "‚è≥ Duration: Duration: Upto 6 months.\n",
      "üéØ Who Should Take: This program will focus on building foundational Python skills while introducing core AI concepts an...\n",
      "üéì Learning Mode: Mode: Instructor led live classes with assessments and hands-on projects.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Python Certification Program with AI\n",
      "‚úÖ Successfully scraped: Python Certification Program with AI\n",
      "\n",
      "==================================================\n",
      "Scraping: https://skillsetarena.com/structured-query-language-program/\n",
      "==================================================\n",
      "üåê Accessing URL: https://skillsetarena.com/structured-query-language-program/\n",
      "üíæ Page source saved as page_source.html\n",
      "üìõ Course Name: SQL Certification Program From SkillSet Arena\n",
      "üìù About Course: The SQL Certification Program provides a comprehensive introduction to Structured Query Language (SQ...\n",
      "üìö Syllabus extracted (1565 characters)\n",
      "üìñ Syllabus preview: üîπ Module 1: Introduction to SQL:\n",
      "    . What is SQL, SQL Syntax and Structure, keywords: SELECT, FROM, WHERE etc.\n",
      "    . Types of SQL Commands: DDL (Data Definition Language),DML (Data Manipulation Language) & DCL (Data Control Language).\n",
      "\n",
      "üîπ Module 2: Database Fundamentals:\n",
      "    . What is a Database, Relational databases vs. non-relational databases, database Tables, Rows, and Columns, primary Keys and Foreign Keys, normalization Basics.\n",
      "\n",
      "üîπ Module 3: Basic Queries:\n",
      "    . SELECT Statement, filtering...\n",
      "‚ö†Ô∏è Detailed Curriculum section not found\n",
      "üìò Syllabus Curriculum extracted (24 characters)\n",
      "üìñ Syllabus Curriculum preview: Curriculum not available...\n",
      "üèÜ Certificate: Certificate information not available\n",
      "‚è≥ Duration: Program Duration:\n",
      "üéØ Who Should Take: The SQL Certification Program provides a comprehensive introduction to Structured Query Language (SQ...\n",
      "üéì Learning Mode: .is-small-text{font-size:.875em}.is-regular-text{font-size:1em}.is-large-text{font-size:2.25em}.is-larger-text{font-size:3em}.has-drop-cap:not(:focus):first-letter{float:left;font-size:8.4em;font-style:normal;font-weight:100;line-height:.68;margin:.05em .1em 0 0;text-transform:uppercase}body.rtl .has-drop-cap:not(:focus):first-letter{float:none;margin-left:.1em}p.has-drop-cap.has-background{overflow:hidden}:root :where(p.has-background){padding:1.25em 2.375em}:where(p.has-text-color:not(.has-link-color)) a{color:inherit}p.has-text-align-left[style*=\"writing-mode:vertical-lr\"],p.has-text-align-right[style*=\"writing-mode:vertical-rl\"]{rotate:180deg}\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: SQL Certification Program From SkillSet Arena\n",
      "‚úÖ Successfully scraped: SQL Certification Program From SkillSet Arena\n",
      "üéâ Process completed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    try:\n",
    "        syllabus_text = \"\"\n",
    "        \n",
    "        # Find the Program Structure section\n",
    "        program_structure = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Program Structure\" in text)\n",
    "        \n",
    "        if not program_structure:\n",
    "            print(\"‚ö†Ô∏è Program Structure section not found\")\n",
    "            return \"Syllabus not available\"\n",
    "        \n",
    "        # Find all elements after Program Structure\n",
    "        current_element = program_structure.find_next_sibling()\n",
    "        \n",
    "        while current_element:\n",
    "            # Check for month headings\n",
    "            if current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                month_text = current_element.get_text(strip=True)\n",
    "                if \"Month\" in month_text:\n",
    "                    syllabus_text += f\"\\n{month_text}\\n\"\n",
    "            \n",
    "            # Check for module details\n",
    "            elif current_element.name == \"details\" and \"wp-block-details\" in current_element.get(\"class\", []):\n",
    "                # Get module summary\n",
    "                summary = current_element.find(\"summary\")\n",
    "                if summary:\n",
    "                    syllabus_text += f\"üîπ {summary.get_text(strip=True)}\\n\"\n",
    "                \n",
    "                # Get the list items\n",
    "                lessons_list = current_element.find(\"ul\", class_=\"wp-block-list\")\n",
    "                if lessons_list:\n",
    "                    lessons = lessons_list.find_all(\"li\")\n",
    "                    for lesson in lessons:\n",
    "                        lesson_text = lesson.get_text(strip=True)\n",
    "                        if lesson_text:\n",
    "                            syllabus_text += f\"    . {lesson_text}\\n\"\n",
    "                \n",
    "                syllabus_text += \"\\n\"\n",
    "            \n",
    "            # Stop if we reach the next major section\n",
    "            elif current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                next_section_text = current_element.get_text(strip=True)\n",
    "                if \"Key Features\" in next_section_text or \"Tools & Technologies\" in next_section_text:\n",
    "                    break\n",
    "            \n",
    "            current_element = current_element.find_next_sibling()\n",
    "        \n",
    "        return syllabus_text.strip() if syllabus_text else \"Syllabus not available\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Syllabus extraction error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return \"Syllabus extraction failed\"\n",
    "\n",
    "# -------------------- SYLLABUS CURRICULUM EXTRACTION --------------------\n",
    "def extract_syllabus_curriculum(soup):\n",
    "    try:\n",
    "        curriculum_text = \"\"\n",
    "        \n",
    "        # Find the Detailed Curriculum section\n",
    "        detailed_curriculum = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Detailed Curriculum\" in text)\n",
    "        \n",
    "        if not detailed_curriculum:\n",
    "            print(\"‚ö†Ô∏è Detailed Curriculum section not found\")\n",
    "            return \"Curriculum not available\"\n",
    "        \n",
    "        # Find all elements after Detailed Curriculum\n",
    "        current_element = detailed_curriculum.find_next_sibling()\n",
    "        \n",
    "        while current_element:\n",
    "            # Check for month headings\n",
    "            if current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                month_text = current_element.get_text(strip=True)\n",
    "                if \"Month\" in month_text:\n",
    "                    curriculum_text += f\"\\n{month_text}\\n\"\n",
    "            \n",
    "            # Check for module details\n",
    "            elif current_element.name == \"details\" and \"wp-block-details\" in current_element.get(\"class\", []):\n",
    "                # Get module summary\n",
    "                summary = current_element.find(\"summary\")\n",
    "                if summary:\n",
    "                    curriculum_text += f\"üîπ {summary.get_text(strip=True)}\\n\"\n",
    "                \n",
    "                # Get the list items\n",
    "                lessons_list = current_element.find(\"ul\", class_=\"wp-block-list\")\n",
    "                if lessons_list:\n",
    "                    lessons = lessons_list.find_all(\"li\")\n",
    "                    for lesson in lessons:\n",
    "                        lesson_text = lesson.get_text(strip=True)\n",
    "                        if lesson_text:\n",
    "                            curriculum_text += f\"    . {lesson_text}\\n\"\n",
    "                \n",
    "                curriculum_text += \"\\n\"\n",
    "            \n",
    "            # Stop if we reach the next major section\n",
    "            elif current_element.name == \"p\" and current_element.get(\"class\") == [\"has-small-font-size\"]:\n",
    "                next_section_text = current_element.get_text(strip=True)\n",
    "                if \"Program Summary\" in next_section_text or \"Final Outcome\" in next_section_text or \"Learning Approach\" in next_section_text:\n",
    "                    break\n",
    "            \n",
    "            current_element = current_element.find_next_sibling()\n",
    "        \n",
    "        return curriculum_text.strip() if curriculum_text else \"Curriculum not available\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Syllabus curriculum extraction error: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return \"Syllabus curriculum extraction failed\"\n",
    "\n",
    "# -------------------- EXTRACT DURATION --------------------\n",
    "def extract_duration(soup):\n",
    "    try:\n",
    "        # Look for duration in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Duration\" in text:\n",
    "                return text\n",
    "        \n",
    "        # Alternative search\n",
    "        duration_elem = soup.find(string=re.compile(\"Duration\", re.IGNORECASE))\n",
    "        if duration_elem:\n",
    "            return duration_elem.strip()\n",
    "        \n",
    "        return \"Duration not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Duration extraction error: {str(e)}\")\n",
    "        return \"Duration not available\"\n",
    "\n",
    "# -------------------- EXTRACT CERTIFICATE INFO --------------------\n",
    "def extract_certificate(soup):\n",
    "    try:\n",
    "        # Look for certificate information\n",
    "        certificate_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in certificate_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"certificate\" in text.lower():\n",
    "                return text\n",
    "        \n",
    "        # Look in certificate of completion section\n",
    "        cert_section = soup.find(\"p\", class_=\"has-small-font-size\", string=lambda text: text and \"Certificate of Completion\" in text)\n",
    "        if cert_section:\n",
    "            cert_list = cert_section.find_next(\"ul\", class_=\"wp-block-list\")\n",
    "            if cert_list:\n",
    "                cert_item = cert_list.find(\"li\", class_=\"has-small-font-size\")\n",
    "                if cert_item:\n",
    "                    return cert_item.get_text(strip=True)\n",
    "        \n",
    "        return \"Certificate information not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Certificate extraction error: {str(e)}\")\n",
    "        return \"Certificate information not available\"\n",
    "\n",
    "# -------------------- EXTRACT WHO SHOULD TAKE --------------------\n",
    "def extract_who_should_take(soup):\n",
    "    try:\n",
    "        # Look for the course description paragraph that describes who should take the course\n",
    "        course_name_elem = soup.find(\"p\", class_=\"has-upper-heading-font-size\")\n",
    "        if course_name_elem:\n",
    "            # Find the next paragraph with the description (this is usually the target audience description)\n",
    "            next_elem = course_name_elem.find_next_sibling()\n",
    "            while next_elem:\n",
    "                if next_elem.name == \"p\" and \"has-small-font-size\" in next_elem.get(\"class\", []):\n",
    "                    description_text = next_elem.get_text(strip=True)\n",
    "                    if description_text and not description_text.startswith(\"Program Overview\"):\n",
    "                        # This is typically the description that explains who the course is for\n",
    "                        return description_text\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "        \n",
    "        # Fallback: Look for prerequisites in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Prerequisites\" in text or \"suitable for beginners\" in text.lower():\n",
    "                return text\n",
    "        \n",
    "        return \"Target audience information not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Who should take extraction error: {str(e)}\")\n",
    "        return \"Target audience information not available\"\n",
    "\n",
    "# -------------------- EXTRACT LEARNING MODE --------------------\n",
    "def extract_learning_mode(soup):\n",
    "    try:\n",
    "        # Look for mode in program overview\n",
    "        overview_items = soup.find_all(\"li\", class_=\"has-small-font-size\")\n",
    "        for item in overview_items:\n",
    "            text = item.get_text(strip=True)\n",
    "            if \"Mode:\" in text or \"Mode\" in text:\n",
    "                return text\n",
    "        \n",
    "        # Alternative search for learning mode\n",
    "        mode_elem = soup.find(string=re.compile(r\"Mode:|Instructor-led|live online\", re.IGNORECASE))\n",
    "        if mode_elem:\n",
    "            return mode_elem.strip()\n",
    "        \n",
    "        return \"Learning mode not available\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Learning mode extraction error: {str(e)}\")\n",
    "        return \"Learning mode not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver(headless=False)  # Set to False for debugging\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        # Scroll multiple times to ensure all content loads\n",
    "        for i in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Debug: Save page source for inspection\n",
    "        with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(\"üíæ Page source saved as page_source.html\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name\n",
    "        course_name_elem = soup.find(\"p\", class_=\"has-upper-heading-font-size\")\n",
    "        course_name = course_name_elem.get_text(strip=True) if course_name_elem else \"Course Name Not Found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. About Course - Get the first paragraph after course name\n",
    "        about_course = \"About course not available\"\n",
    "        if course_name_elem:\n",
    "            # Find the next paragraph with the description\n",
    "            next_elem = course_name_elem.find_next_sibling()\n",
    "            while next_elem:\n",
    "                if next_elem.name == \"p\" and \"has-small-font-size\" in next_elem.get(\"class\", []):\n",
    "                    about_text = next_elem.get_text(strip=True)\n",
    "                    if about_text and not about_text.startswith(\"Program Overview\"):\n",
    "                        about_course = about_text\n",
    "                        break\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        # 3. Syllabus\n",
    "        syllabus = extract_syllabus(soup)\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus)} characters)\")\n",
    "        \n",
    "        # Debug: Print first 500 chars of syllabus\n",
    "        print(f\"üìñ Syllabus preview: {syllabus[:500]}...\")\n",
    "\n",
    "        # 4. Syllabus Curriculum\n",
    "        syllabus_curriculum = extract_syllabus_curriculum(soup)\n",
    "        print(f\"üìò Syllabus Curriculum extracted ({len(syllabus_curriculum)} characters)\")\n",
    "        \n",
    "        # Debug: Print first 500 chars of syllabus curriculum\n",
    "        print(f\"üìñ Syllabus Curriculum preview: {syllabus_curriculum[:500]}...\")\n",
    "\n",
    "        # 5. Certificate\n",
    "        certificate_info = extract_certificate(soup)\n",
    "        print(f\"üèÜ Certificate: {certificate_info}\")\n",
    "\n",
    "        # 6. Duration\n",
    "        duration = extract_duration(soup)\n",
    "        print(f\"‚è≥ Duration: {duration}\")\n",
    "\n",
    "        # 7. Who Should Take\n",
    "        who_should_take = extract_who_should_take(soup)\n",
    "        print(f\"üéØ Who Should Take: {who_should_take[:100]}...\")\n",
    "\n",
    "        # 8. Learning Mode\n",
    "        learning_mode = extract_learning_mode(soup)\n",
    "        print(f\"üéì Learning Mode: {learning_mode}\")\n",
    "\n",
    "        return course_name, about_course, syllabus, syllabus_curriculum, certificate_info, duration, who_should_take, learning_mode\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "        return [\"Error\"] * 8\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \n",
    "        \"About Course\", \n",
    "        \"Syllabus\", \n",
    "        \"Syllabus Curriculum\", \n",
    "        \"Certificate\", \n",
    "        \"Duration\",\n",
    "        \"Who Should Take\",\n",
    "        \"Learning Mode\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Skip if course already exists\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        # Add new row with matching columns\n",
    "        new_row = pd.DataFrame([dict(zip(columns, [*data[:8], data[-1]]))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- READ URLS FROM EXCEL --------------------\n",
    "def read_urls_from_excel(file_path):\n",
    "    \"\"\"\n",
    "    Read course URLs from an Excel file.\n",
    "    Expected column name: 'Course URL' or 'URL'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Check for possible column names\n",
    "        url_columns = ['Course URL', 'URL', 'course_url', 'url', 'Link', 'link']\n",
    "        url_column = None\n",
    "        \n",
    "        for col in url_columns:\n",
    "            if col in df.columns:\n",
    "                url_column = col\n",
    "                break\n",
    "        \n",
    "        if url_column is None:\n",
    "            print(\"‚ùå No URL column found in the Excel file.\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return []\n",
    "        \n",
    "        # Get URLs and remove any NaN values\n",
    "        urls = df[url_column].dropna().tolist()\n",
    "        \n",
    "        print(f\"üìñ Found {len(urls)} URLs in the Excel file\")\n",
    "        return urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading Excel file: {e}\")\n",
    "        return []\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Input Excel file path\n",
    "    input_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\skill_course_links.xlsx\" # Change this to your input file path\n",
    "    output_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\course_smartset_all.xlsx\"\n",
    "    \n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    \n",
    "    # Read URLs from Excel file\n",
    "    course_urls = read_urls_from_excel(input_file_path)\n",
    "    \n",
    "    if not course_urls:\n",
    "        print(\"‚ùå No URLs found to scrape. Please check your input Excel file.\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\"üîó URLs to scrape: {course_urls}\")\n",
    "    \n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Scraping: {course_url}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel((*course_data, course_url), output_file_path)\n",
    "            print(f\"‚úÖ Successfully scraped: {course_data[0]}\")\n",
    "        else:\n",
    "            \n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"üéâ Process completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
