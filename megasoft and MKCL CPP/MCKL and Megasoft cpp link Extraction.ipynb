{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fcb5cb",
   "metadata": {},
   "source": [
    "# Course link Extract  MCKL link only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f28668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def get_all_course_links(base_url=\"https://klic.mkcl.org\"):\n",
    "    \"\"\"Get all course links from the MKCL KLiC website by handling pagination\"\"\"\n",
    "    all_courses = []\n",
    "    page = 1\n",
    "    max_pages = 50  # Safety limit to prevent infinite loops\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        # Construct URL for the current page\n",
    "        if page == 1:\n",
    "            url = f\"{base_url}/klic-courses\"\n",
    "        else:\n",
    "            url = f\"{base_url}/klic-courses/page:{page}\"\n",
    "        \n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch the page content\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all course cards\n",
    "            course_cards = soup.find_all('a', class_='card', href=True)\n",
    "            \n",
    "            if not course_cards:\n",
    "                print(\"No more courses found. Ending scraping.\")\n",
    "                break\n",
    "            \n",
    "            # Extract course information\n",
    "            for card in course_cards:\n",
    "                # Extract course name\n",
    "                title_element = card.find('div', class_='ct-title')\n",
    "                course_name = title_element.get_text(strip=True) if title_element else \"Unknown Course\"\n",
    "                \n",
    "                # Extract course link\n",
    "                course_link = card['href']\n",
    "                full_link = urljoin(base_url, course_link)\n",
    "                \n",
    "                all_courses.append({\n",
    "                    \"Course Name\": course_name,\n",
    "                    \"Course Link\": full_link\n",
    "                })\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            next_page_link = soup.find('a', rel='next')\n",
    "            if not next_page_link:\n",
    "                print(\"No next page found. Ending scraping.\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(1)  # Be polite with a delay between requests\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def main():\n",
    "    # Get all courses from the website\n",
    "    all_courses = get_all_course_links()\n",
    "    \n",
    "    # Create DataFrame and remove duplicates\n",
    "    df = pd.DataFrame(all_courses)\n",
    "    df = df.drop_duplicates(subset=[\"Course Link\"])\n",
    "    \n",
    "    # Save to Excel\n",
    "    output_path = r'C:\\Users\\taslim.siddiqui\\Downloads\\MKCL_KLiC_All_Courses.xlsx'\n",
    "    df.to_excel(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully extracted {len(df)} courses!\")\n",
    "    print(f\"üìÇ Saved to: {output_path}\")\n",
    "    \n",
    "    # Display sample of the results\n",
    "    print(\"\\nSample of extracted courses:\")\n",
    "    print(df.head(10).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43818b9",
   "metadata": {},
   "source": [
    "# Course link extraction only 14 courses for megasoft link only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01665b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_megasoft_courses(html_content):\n",
    "    \"\"\"Extract course names and links from Megasoft Technologies HTML content\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    courses = []\n",
    "    \n",
    "    # Find all course elements\n",
    "    course_elements = soup.find_all('div', class_='edublink-single-course')\n",
    "    \n",
    "    for course in course_elements:\n",
    "        # Extract course name\n",
    "        title_element = course.find('h6', class_='title')\n",
    "        if not title_element:\n",
    "            title_element = course.find(['h6', 'h5', 'h4', 'h3', 'h2'], class_=re.compile(r'title'))\n",
    "        \n",
    "        if title_element:\n",
    "            course_name = title_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract course link\n",
    "            link_element = title_element.find('a')\n",
    "            if not link_element:\n",
    "                link_element = course.find('a', class_='course-thumb')\n",
    "            \n",
    "            if link_element and link_element.has_attr('href'):\n",
    "                course_link = link_element['href']\n",
    "                \n",
    "                # Make sure link is absolute, not relative\n",
    "                if not course_link.startswith('http'):\n",
    "                    base_url = \"https://megasofttech.in\"\n",
    "                    course_link = base_url + (course_link if course_link.startswith('/') else '/' + course_link)\n",
    "                \n",
    "                courses.append({\n",
    "                    \"Course Name\": course_name,\n",
    "                    \"Course Link\": course_link\n",
    "                })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "def fetch_all_courses():\n",
    "    \"\"\"Fetch courses from all pages\"\"\"\n",
    "    base_url = \"https://megasofttech.in/courses/\"\n",
    "    all_courses = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}page/{page}/\"\n",
    "        \n",
    "        print(f\"üìÑ Fetching page {page}: {url}\")\n",
    "        \n",
    "        html_content = fetch_html_from_url(url)\n",
    "        if not html_content:\n",
    "            print(f\"‚ùå Failed to fetch page {page}\")\n",
    "            break\n",
    "        \n",
    "        # Check if this page has courses\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        course_elements = soup.find_all('div', class_='edublink-single-course')\n",
    "        \n",
    "        if not course_elements:\n",
    "            print(\"‚èπÔ∏è No more courses found\")\n",
    "            break\n",
    "        \n",
    "        # Extract courses from this page\n",
    "        page_courses = extract_megasoft_courses(html_content)\n",
    "        all_courses.extend(page_courses)\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(page_courses)} courses on page {page}\")\n",
    "        \n",
    "        # Check if there's a next page\n",
    "        next_page_link = soup.find('a', class_='page-numbers', href=True)\n",
    "        if not next_page_link or 'page/' not in next_page_link['href']:\n",
    "            break\n",
    "            \n",
    "        page += 1\n",
    "        \n",
    "        # Safety limit to prevent infinite loops\n",
    "        if page > 10:\n",
    "            print(\"‚ö†Ô∏è Safety limit reached (10 pages)\")\n",
    "            break\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def fetch_html_from_url(url):\n",
    "    \"\"\"Fetch HTML content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching website: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_html_from_file(file_path):\n",
    "    \"\"\"Read HTML content from a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Option 1: Fetch all courses from the website (with pagination)\n",
    "    all_courses = fetch_all_courses()\n",
    "    \n",
    "    # Option 2: Load HTML from a local file (uncomment below lines)\n",
    "    # file_path = \"All Courses - Megasoft Technologies.html\"\n",
    "    # html_content = read_html_from_file(file_path)\n",
    "    # if html_content:\n",
    "    #     all_courses = extract_megasoft_courses(html_content)\n",
    "    \n",
    "    if not all_courses:\n",
    "        print(\"‚ùå No courses found. Please check the source.\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_courses)\n",
    "    \n",
    "    # Remove duplicates (just in case)\n",
    "    df = df.drop_duplicates(subset=['Course Name', 'Course Link'])\n",
    "    \n",
    "    # Save to Excel\n",
    "    output_path = r'C:\\Users\\taslim.siddiqui\\Downloads\\Megasoft_Technologies_Coursespage2.xlsx'\n",
    "    \n",
    "    try:\n",
    "        df.to_excel(output_path, index=False)\n",
    "        print(f\"‚úÖ Successfully extracted {len(df)} courses!\")\n",
    "        print(f\"üìÇ Saved to: {output_path}\")\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\nExtracted courses:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving to Excel: {e}\")\n",
    "        # Try alternative path if the original fails\n",
    "        alternative_path = \"Megasoft_Technologies_Courses.xlsx\"\n",
    "        df.to_excel(alternative_path, index=False)\n",
    "        print(f\"üìÇ Saved to alternative location: {os.path.abspath(alternative_path)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
