{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e447a51a",
   "metadata": {},
   "source": [
    "# MCKL course link only for one course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a05795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üìñ Processing: https://klic.mkcl.org/klic-courses/c-sharp\n",
      "üåê Accessing URL: https://klic.mkcl.org/klic-courses/c-sharp\n",
      "üìõ Course Name: C Sharp (C#)\n",
      "üìù About Course: KLiC Certificate in C# course is designed to provide a comprehensive understanding of C# programming...\n",
      "üìö Syllabus extracted (190 lines)\n",
      "‚è≥ Duration: 120 hours\n",
      "üìä Course Level: Foundation\n",
      "üó£Ô∏è Language: English\n",
      "üéì Learning Mode: Learn at ALC or Learn at Home\n",
      "üí∞ Price: 6000/-\n",
      "üèÜ Certificate: KLiC Certificate in C# course is designed to provide a comprehensive understanding of C# programming. Covering fundamental to advanced topics, this course includes data types, control structures, object-oriented programming concepts, and the use of C# in web and mobile applications. With a blend of theoretical knowledge and practical skills, students will learn to construct, debug, and optimize C# programs, preparing them for real-world development challenges.\n",
      "üë• Eligibility:\n",
      "- Learner should preferably a std. 10th Pass student (Not Compulsory)\n",
      "- It is desirable that Learner should have done MS-CIT Course (Not Compulsory)\n",
      "- Classify the fundamental features and syntax of the C# programming language.\n",
      "- Describe the integrated development environments (IDEs) used for C# development.\n",
      "- Classify different data types and control structures in C#.\n",
      "- Diagnose common errors and debugging techniques in C# programs.\n",
      "- Predict the outcomes of C# code snippets and their execution.\n",
      "- Demonstrate object-oriented programming concepts in C#.\n",
      "- Identify the use of C# in developing web and mobile applications.\n",
      "üè¶ Fee Structure:\n",
      "6000/-\n",
      "- All other fees remain unchanged\n",
      "- Education loans are available through leading banks and NBFCs.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: C Sharp (C#)\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    # options.add_argument(\"--headless=new\")  # Uncomment for silent scraping\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- CLEAN TEXT --------------------\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.split()) if text else text\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Course Name\n",
    "        course_name = \"Not Found\"\n",
    "        for selector in [\"div.page-header h1\", \"h1.course-title\", \"h1.title\", \"h1\", \"title\"]:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                course_name = clean_text(element.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # About Course (FULL DIV with bullet formatting)\n",
    "        about_course = \"Not Found\"\n",
    "        about_div = soup.find(\"div\", class_=\"ci-text\")\n",
    "        if about_div:\n",
    "            parts = []\n",
    "            for tag in about_div.find_all([\"p\", \"li\"]):\n",
    "                text = clean_text(tag.get_text(\" \", strip=True))\n",
    "                if tag.name == \"li\":\n",
    "                    parts.append(f\"- {text}\")\n",
    "                else:\n",
    "                    parts.append(text)\n",
    "            about_course = \"\\n\".join(parts)\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        # ---------- Syllabus (accordion) ----------\n",
    "        syllabus = \"Not Found\"\n",
    "        accordion = soup.find(\"div\", class_=\"accordion\", attrs={\"class\": \"course-sc-syllabus\"})\n",
    "        if accordion:\n",
    "            modules = []\n",
    "            for card in accordion.find_all(\"div\", class_=\"card\"):\n",
    "                # module title\n",
    "                header = card.find(\"button\")\n",
    "                module_title = header.get_text(strip=True) if header else \"Untitled Module\"\n",
    "\n",
    "                # module content\n",
    "                content_div = card.find(\"div\", class_=\"course-sc-syllabus-content\")\n",
    "                lessons = []\n",
    "                if content_div:\n",
    "                    for li in content_div.find_all(\"li\"):\n",
    "                        lessons.append(\"- \" + li.get_text(strip=True))\n",
    "                    for p in content_div.find_all(\"p\"):\n",
    "                        lessons.append(\"- \" + p.get_text(\" \", strip=True))\n",
    "\n",
    "                if lessons:\n",
    "                    modules.append(f\"{module_title}\\n\" + \"\\n\".join(lessons))\n",
    "                else:\n",
    "                    modules.append(module_title)\n",
    "\n",
    "            if modules:\n",
    "                syllabus = \"\\n\\n\".join(modules)\n",
    "\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus.splitlines())} lines)\")\n",
    "\n",
    "        # -------------------- Helper to extract normalized info --------------------\n",
    "        def extract_field(keyword):\n",
    "            elems = soup.find_all(string=lambda t: t and keyword in t.lower())\n",
    "            for elem in elems:\n",
    "                parent = elem.parent\n",
    "                if parent:\n",
    "                    raw = clean_text(parent.get_text(\" \", strip=True))\n",
    "                    value = raw\n",
    "                    for k in [\"duration\", \"level\", \"language\", \"mode\", \"learning\"]:\n",
    "                        value = value.replace(k, \"\", 1)\n",
    "                        value = value.replace(k.capitalize(), \"\", 1)\n",
    "                    value = value.replace(\":\", \"\").replace(\"layers-outline\", \"\").strip()\n",
    "                    return value\n",
    "            return \"Not Found\"\n",
    "\n",
    "        duration = extract_field(\"duration\")\n",
    "        level = extract_field(\"level\")\n",
    "        language = extract_field(\"language\")\n",
    "        learning_mode = extract_field(\"mode\") if extract_field(\"mode\") != \"Not Found\" else extract_field(\"learning\")\n",
    "\n",
    "        print(f\"‚è≥ Duration: {duration}\")\n",
    "        print(f\"üìä Course Level: {level}\")\n",
    "        print(f\"üó£Ô∏è Language: {language}\")\n",
    "        print(f\"üéì Learning Mode: {learning_mode}\")\n",
    "\n",
    "        # Price\n",
    "        price = \"Not Found\"\n",
    "        try:\n",
    "            price_td = soup.find(\"td\", style=lambda s: s and \"text-align:center\" in s)\n",
    "            if price_td:\n",
    "                price = clean_text(price_td.get_text(\" \", strip=True))\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # Certificate\n",
    "        certificate = extract_field(\"certificate\")\n",
    "        print(f\"üèÜ Certificate: {certificate}\")\n",
    "\n",
    "        # -------------------- Eligibility (Combine both sources) --------------------\n",
    "        eligibility_parts = []\n",
    "        try:\n",
    "            for box in soup.find_all(\"div\", class_=\"cs-textbox\"):\n",
    "                title_div = box.find(\"div\", class_=\"cs-textbox-title\")\n",
    "                info_div = box.find(\"div\", class_=\"cs-textbox-info\")\n",
    "                if title_div and \"eligibility\" in title_div.get_text(strip=True).lower() and info_div:\n",
    "                    list_items = info_div.find_all(\"li\")\n",
    "                    if list_items:\n",
    "                        eligibility_parts.append(\n",
    "                            \"\\n\".join([f\"- {li.get_text(strip=True)}\" for li in list_items])\n",
    "                        )\n",
    "                    else:\n",
    "                        eligibility_parts.append(clean_text(info_div.get_text(\" \", strip=True)))\n",
    "                    break\n",
    "\n",
    "            section = soup.find(\"div\", class_=\"course-section-content\")\n",
    "            if section:\n",
    "                ul = section.find(\"ul\")\n",
    "                if ul:\n",
    "                    lis = ul.find_all(\"li\")\n",
    "                    if lis:\n",
    "                        eligibility_parts.append(\n",
    "                            \"\\n\".join([f\"- {li.get_text(strip=True)}\" for li in lis])\n",
    "                        )\n",
    "                    else:\n",
    "                        eligibility_parts.append(clean_text(section.get_text(\" \", strip=True)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Eligibility extraction issue: {e}\")\n",
    "\n",
    "        eligibility = \"\\n\".join([part for part in eligibility_parts if part]) or \"Not Found\"\n",
    "        print(f\"üë• Eligibility:\\n{eligibility}\")\n",
    "\n",
    "        # -------------------- Fee Structure --------------------\n",
    "        fee_structure = \"Not Found\"\n",
    "        try:\n",
    "            offer_price = price if price != \"Not Found\" else None\n",
    "            if offer_price:\n",
    "                fee_structure = (\n",
    "                    f\"{offer_price}\\n\"\n",
    "                    \"- All other fees remain unchanged\\n\"\n",
    "                    \"- Education loans are available through leading banks and NBFCs.\"\n",
    "                )\n",
    "            else:\n",
    "                fee_structure = (\n",
    "                    \"- All other fees remain unchanged\\n\"\n",
    "                    \"- Education loans are available through leading banks and NBFCs.\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fee structure extraction issue: {e}\")\n",
    "        print(f\"üè¶ Fee Structure:\\n{fee_structure}\")\n",
    "\n",
    "        return [\n",
    "            course_name, about_course, syllabus, duration, level, language,\n",
    "            learning_mode, price, certificate, eligibility, fee_structure, url\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 12\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Syllabus\", \"Duration\", \"Course Level\",\n",
    "        \"Language\", \"Learning Mode\", \"Price\", \"Certificate\", \"Eligibility\",\n",
    "        \"Fee Structure\", \"Course URL\"\n",
    "    ]\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        new_row = pd.DataFrame([dict(zip(columns, data))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "       \"https://klic.mkcl.org/klic-courses/c-sharp\"\n",
    "    ]\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\MKCL.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüìñ Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, file_path)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"‚úÖ Process completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284de6d",
   "metadata": {},
   "source": [
    "# MCKL course for input excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e95be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Found 244 URLs in the Excel file\n",
      "üöÄ Starting scraping process...\n",
      "\n",
      "üìñ Processing: https://klic.mkcl.org/klic-courses/advanced-excel\n",
      "üåê Accessing URL: https://klic.mkcl.org/klic-courses/advanced-excel\n",
      "üìõ Course Name: Advanced Excel\n",
      "üìù About Course: Excel skills are as important as the subject knowledge. Those who know Excel can find a better payin...\n",
      "üìö Syllabus extracted (102 lines)\n",
      "‚è≥ Duration: 120 hours\n",
      "üìä Course Level: Advanced\n",
      "üó£Ô∏è Language: English, Marathi, Hindi\n",
      "üéì Learning Mode: Learn at ALC or Learn at Home\n",
      "üí∞ Price: 6000/-\n",
      "üèÜ Certificate: Certificate of Completion\n",
      "üë• Eligibility:\n",
      "- Learner should preferably a std. 10th Pass student (Not Compulsory)\n",
      "- It is desirable that Learner should have done MS-CIT Course (Not Compulsory)\n",
      "- Accounting and Finance Students ‚Äì To gain practical skills in managing data, preparing reports, and performing financial analysis.\n",
      "- Business and Management Students ‚Äì Essential for understanding data-driven decision-making, budgeting, and inventory management.\n",
      "- Data Analysts and Aspiring Data Professionals ‚Äì Offers foundational tools like pivot tables, formulas, and scenario analysis for handling large datasets.\n",
      "- Retail Store Managers and Logistics Staff ‚Äì Useful for inventory tracking, sales analysis, and reporting.\n",
      "- Human Resources (HR) Professionals ‚Äì Beneficial for managing employee data, attendance records, and performance analysis.\n",
      "- Project Managers ‚Äì Enables tracking project progress, managing resources, and scenario planning for effective project outcomes.\n",
      "- Entrepreneurs and Small Business Owners ‚Äì Essential for budgeting, sales analysis, and performance reporting to optimize business decisions.\n",
      "üè¶ Fee Structure:\n",
      "6000/-\n",
      "- All other fees remain unchanged\n",
      "- Education loans are available through leading banks and NBFCs.\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Advanced Excel\n",
      "\n",
      "üìñ Processing: https://klic.mkcl.org/klic-courses/ai-ml-basics\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 265\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m course_url \u001b[38;5;129;01min\u001b[39;00m course_urls:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìñ Processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourse_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     course_data = \u001b[43mscrape_course_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcourse_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(item != \u001b[33m\"\u001b[39m\u001b[33mError\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m course_data):\n\u001b[32m    267\u001b[39m         save_to_excel(course_data, output_file_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mscrape_course_data\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mscrape_course_data\u001b[39m(url):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     driver = \u001b[43mget_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müåê Accessing URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mget_driver\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m options.add_argument(\u001b[33m\"\u001b[39m\u001b[33m--log-level=3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# options.add_argument(\"--headless=new\")  # Uncomment for silent scraping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m driver = \u001b[43muc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\undetected_chromedriver\\__init__.py:258\u001b[39m, in \u001b[36mChrome.__init__\u001b[39m\u001b[34m(self, options, user_data_dir, driver_executable_path, browser_executable_path, port, enable_cdp_events, desired_capabilities, advanced_elements, keep_alive, log_level, headless, version_main, patcher_force_close, suppress_welcome, use_subprocess, debug, no_sandbox, user_multi_procs, **kw)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.patcher = Patcher(\n\u001b[32m    252\u001b[39m     executable_path=driver_executable_path,\n\u001b[32m    253\u001b[39m     force=patcher_force_close,\n\u001b[32m    254\u001b[39m     version_main=version_main,\n\u001b[32m    255\u001b[39m     user_multi_procs=user_multi_procs,\n\u001b[32m    256\u001b[39m )\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# self.patcher.auto(user_multiprocess = user_multi_num_procs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# self.patcher = patcher\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\undetected_chromedriver\\patcher.py:179\u001b[39m, in \u001b[36mPatcher.auto\u001b[39m\u001b[34m(self, executable_path, force, version_main, _)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.version_full = release\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m.unzip_package(\u001b[38;5;28mself\u001b[39m.fetch_package())\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\undetected_chromedriver\\patcher.py:229\u001b[39m, in \u001b[36mPatcher.patch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mpatch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_exe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_binary_patched()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python311\\Lib\\site-packages\\undetected_chromedriver\\patcher.py:347\u001b[39m, in \u001b[36mPatcher.patch_exe\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    345\u001b[39m start = time.perf_counter()\n\u001b[32m    346\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mpatching driver executable \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.executable_path)\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecutable_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr+b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[32m    348\u001b[39m     content = fh.read()\n\u001b[32m    349\u001b[39m     \u001b[38;5;66;03m# match_injected_codeblock = re.search(rb\"{window.*;}\", content)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    # options.add_argument(\"--headless=new\")  # Uncomment for silent scraping\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- CLEAN TEXT --------------------\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.split()) if text else text\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Course Name\n",
    "        course_name = \"Not Found\"\n",
    "        for selector in [\"div.page-header h1\", \"h1.course-title\", \"h1.title\", \"h1\", \"title\"]:\n",
    "            element = soup.select_one(selector)\n",
    "            if element:\n",
    "                course_name = clean_text(element.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "       # About Course (FULL DIV with bullet formatting)\n",
    "        about_course = \"Not Found\"\n",
    "        about_div = soup.find(\"div\", class_=\"ci-text\")\n",
    "        if about_div:\n",
    "            parts = []\n",
    "            for tag in about_div.find_all([\"p\", \"li\"]):\n",
    "                text = clean_text(tag.get_text(\" \", strip=True))\n",
    "                if tag.name == \"li\":\n",
    "                    parts.append(f\"- {text}\")\n",
    "                else:\n",
    "                    parts.append(text)\n",
    "            about_course = \"\\n\".join(parts)\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        # ---------- Syllabus (accordion) ----------\n",
    "        syllabus = \"Not Found\"\n",
    "        accordion = soup.find(\"div\", class_=\"accordion\", attrs={\"class\": \"course-sc-syllabus\"})\n",
    "        if accordion:\n",
    "            modules = []\n",
    "            for card in accordion.find_all(\"div\", class_=\"card\"):\n",
    "                # module title\n",
    "                header = card.find(\"button\")\n",
    "                module_title = header.get_text(strip=True) if header else \"Untitled Module\"\n",
    "\n",
    "                # module content\n",
    "                content_div = card.find(\"div\", class_=\"course-sc-syllabus-content\")\n",
    "                lessons = []\n",
    "                if content_div:\n",
    "                    # collect list items or paragraph text\n",
    "                    for li in content_div.find_all(\"li\"):\n",
    "                        lessons.append(\"- \" + li.get_text(strip=True))\n",
    "                    for p in content_div.find_all(\"p\"):\n",
    "                        lessons.append(\"- \" + p.get_text(\" \", strip=True))\n",
    "\n",
    "                # join module\n",
    "                if lessons:\n",
    "                    modules.append(f\"{module_title}\\n\" + \"\\n\".join(lessons))\n",
    "                else:\n",
    "                    modules.append(module_title)\n",
    "\n",
    "            if modules:\n",
    "                syllabus = \"\\n\\n\".join(modules)\n",
    "\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus.splitlines())} lines)\")\n",
    "\n",
    "        # -------------------- Helper to extract normalized info --------------------\n",
    "        def extract_field(keyword):\n",
    "            elems = soup.find_all(string=lambda t: t and keyword in t.lower())\n",
    "            for elem in elems:\n",
    "                parent = elem.parent\n",
    "                if parent:\n",
    "                    raw = clean_text(parent.get_text(\" \", strip=True))\n",
    "                    # remove keyword and extra symbols\n",
    "                    value = raw\n",
    "                    for k in [\"duration\", \"level\", \"language\", \"mode\", \"learning\"]:\n",
    "                        value = value.replace(k, \"\", 1)\n",
    "                        value = value.replace(k.capitalize(), \"\", 1)\n",
    "                    value = value.replace(\":\", \"\").replace(\"layers-outline\", \"\").strip()\n",
    "                    return value\n",
    "            return \"Not Found\"\n",
    "\n",
    "        duration = extract_field(\"duration\")\n",
    "        level = extract_field(\"level\")\n",
    "        language = extract_field(\"language\")\n",
    "        learning_mode = extract_field(\"mode\") if extract_field(\"mode\") != \"Not Found\" else extract_field(\"learning\")\n",
    "\n",
    "        print(f\"‚è≥ Duration: {duration}\")\n",
    "        print(f\"üìä Course Level: {level}\")\n",
    "        print(f\"üó£Ô∏è Language: {language}\")\n",
    "        print(f\"üéì Learning Mode: {learning_mode}\")\n",
    "\n",
    "\n",
    "        # Price\n",
    "        price = \"Not Found\"\n",
    "        try:\n",
    "            price_td = soup.find(\"td\", style=lambda s: s and \"text-align:center\" in s)\n",
    "            if price_td:\n",
    "                price = clean_text(price_td.get_text(\" \", strip=True))\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # Certificate\n",
    "        certificate = extract_field(\"certificate\")\n",
    "        print(f\"üèÜ Certificate: {certificate}\")\n",
    "\n",
    "        # -------------------- Eligibility (Combine both sources) --------------------\n",
    "        eligibility_parts = []\n",
    "        try:\n",
    "            # --- From .cs-textbox ---\n",
    "            for box in soup.find_all(\"div\", class_=\"cs-textbox\"):\n",
    "                title_div = box.find(\"div\", class_=\"cs-textbox-title\")\n",
    "                info_div = box.find(\"div\", class_=\"cs-textbox-info\")\n",
    "                if title_div and \"eligibility\" in title_div.get_text(strip=True).lower() and info_div:\n",
    "                    list_items = info_div.find_all(\"li\")\n",
    "                    if list_items:\n",
    "                        eligibility_parts.append(\n",
    "                            \"\\n\".join([f\"- {li.get_text(strip=True)}\" for li in list_items])\n",
    "                        )\n",
    "                    else:\n",
    "                        eligibility_parts.append(clean_text(info_div.get_text(\" \", strip=True)))\n",
    "                    break\n",
    "\n",
    "            # --- From .course-section-content ---\n",
    "            section = soup.find(\"div\", class_=\"course-section-content\")\n",
    "            if section:\n",
    "                ul = section.find(\"ul\")\n",
    "                if ul:\n",
    "                    lis = ul.find_all(\"li\")\n",
    "                    if lis:\n",
    "                        eligibility_parts.append(\n",
    "                            \"\\n\".join([f\"- {li.get_text(strip=True)}\" for li in lis])\n",
    "                        )\n",
    "                    else:\n",
    "                        eligibility_parts.append(clean_text(section.get_text(\" \", strip=True)))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Eligibility extraction issue: {e}\")\n",
    "\n",
    "        # Merge all parts\n",
    "        eligibility = \"\\n\".join([part for part in eligibility_parts if part]) or \"Not Found\"\n",
    "        print(f\"üë• Eligibility:\\n{eligibility}\")\n",
    "\n",
    "\n",
    "         # -------------------- Fee Structure --------------------\n",
    "        fee_structure = \"Not Found\"\n",
    "        try:\n",
    "            offer_price = price if price != \"Not Found\" else None\n",
    "            if offer_price:\n",
    "                fee_structure = (\n",
    "                    f\"{offer_price}\\n\"\n",
    "                    \"- All other fees remain unchanged\\n\"\n",
    "                    \"- Education loans are available through leading banks and NBFCs.\"\n",
    "                )\n",
    "            else:\n",
    "                fee_structure = (\n",
    "                    \"- All other fees remain unchanged\\n\"\n",
    "                    \"- Education loans are available through leading banks and NBFCs.\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fee structure extraction issue: {e}\")\n",
    "        print(f\"üè¶ Fee Structure:\\n{fee_structure}\")\n",
    "\n",
    "\n",
    "        return [\n",
    "            course_name, about_course, syllabus, duration, level, language,\n",
    "            learning_mode, price, certificate, eligibility, url,fee_structure\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 11\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Syllabus\", \"Duration\", \"Course Level\",\n",
    "        \"Language\", \"Learning Mode\", \"Price\", \"Certificate\", \"Eligibility\", \"Course URL\",\"Fee Structure\"\n",
    "    ]\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        new_row = pd.DataFrame([dict(zip(columns, data))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- READ URLS FROM EXCEL --------------------\n",
    "def read_urls_from_excel(file_path, sheet_name=0, column_name=\"Course URL\"):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"‚ùå Column '{column_name}' not found in the Excel file.\")\n",
    "            return []\n",
    "        \n",
    "        urls = df[column_name].dropna().tolist()\n",
    "        print(f\"üìñ Found {len(urls)} URLs in the Excel file\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading Excel file: {e}\")\n",
    "        return []\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to input Excel file with URLs\n",
    "    input_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\MCKL course all input ccdc.xlsx\"  # Update this path\n",
    "    output_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\MKCL_all course link_output2new.xlsx\"\n",
    "    \n",
    "    # Read URLs from Excel\n",
    "    course_urls = read_urls_from_excel(input_file_path)\n",
    "    \n",
    "    if not course_urls:\n",
    "        print(\"‚ùå No URLs found. Please check your input file.\")\n",
    "    else:\n",
    "        print(\"üöÄ Starting scraping process...\")\n",
    "        \n",
    "        for course_url in course_urls:\n",
    "            print(f\"\\nüìñ Processing: {course_url}\")\n",
    "            course_data = scrape_course_data(course_url)\n",
    "            if all(item != \"Error\" for item in course_data):\n",
    "                save_to_excel(course_data, output_file_path)\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"‚úÖ Process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f8297",
   "metadata": {},
   "source": [
    "# Mega soft courses for one course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ac03be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/data-science-analytics-courses/data-engineering/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/data-science-analytics-courses/data-engineering/\n",
      "üìõ Course Name: Data Engineering\n",
      "üìù About Course: - Focused training on building and managing data pipelines and infrastructure.\n",
      "- Learn to design, co...\n",
      "üë• Who Should Take: - IT professionals transitioning to data-focused roles.\n",
      "- Software developers expanding into data en...\n",
      "üìö Syllabus extracted (26 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Engineering\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    # options.add_argument(\"--headless=new\")  # Uncomment for silent scraping\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- CLEAN TEXT --------------------\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.split()) if text else text\n",
    "\n",
    "# -------------------- EXTRACT COURSE NAME --------------------\n",
    "def extract_course_name(soup):\n",
    "    # Try multiple selectors for course name\n",
    "    selectors = [\n",
    "        \"div.edublink-course-title h1.entry-title\",\n",
    "        \"h1.course-title\", \n",
    "        \"h1.title\",\n",
    "        \"h1\",\n",
    "        \"title\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return clean_text(element.get_text(strip=True))\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT ABOUT COURSE --------------------\n",
    "def extract_about_course(soup):\n",
    "    about_text = []\n",
    "    \n",
    "    # Look for the specific course description section in the HTML structure\n",
    "    course_desc_heading = soup.find(\"h2\", string=lambda text: text and \"Course Description\" in text)\n",
    "    \n",
    "    if course_desc_heading:\n",
    "        # Find the parent container\n",
    "        parent_container = course_desc_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        if parent_container:\n",
    "            # Find the text container that follows the heading\n",
    "            text_container = parent_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            if text_container:\n",
    "                # Extract text from the widget container\n",
    "                widget_container = text_container.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                if widget_container:\n",
    "                    # Extract list items if present\n",
    "                    if widget_container.find(\"ul\"):\n",
    "                        for li in widget_container.find_all(\"li\"):\n",
    "                            about_text.append(f\"- {clean_text(li.get_text())}\")\n",
    "                    else:\n",
    "                        # Extract paragraph text\n",
    "                        about_text.append(clean_text(widget_container.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(about_text) if about_text else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT WHO_SHOULD_TAKE --------------------\n",
    "def extract_who_should_take(soup):\n",
    "    who_text = []\n",
    "    \n",
    "    # Look for \"Who Can Take This Course\" section\n",
    "    who_heading = soup.find(\"h2\", string=lambda text: text and \"Who Can Take This Course\" in text)\n",
    "    \n",
    "    if who_heading:\n",
    "        # Find the parent container\n",
    "        parent_container = who_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        if parent_container:\n",
    "            # Find the text container that follows the heading\n",
    "            text_container = parent_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            if text_container:\n",
    "                # Extract text from the widget container\n",
    "                widget_container = text_container.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                if widget_container:\n",
    "                    # Extract list items if present\n",
    "                    if widget_container.find(\"ul\"):\n",
    "                        for li in widget_container.find_all(\"li\"):\n",
    "                            who_text.append(f\"- {clean_text(li.get_text())}\")\n",
    "                    else:\n",
    "                        # Extract paragraph text\n",
    "                        who_text.append(clean_text(widget_container.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(who_text) if who_text else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT SYLLABUS --------------------\n",
    "def extract_syllabus(soup):\n",
    "    syllabus_sections = []\n",
    "    \n",
    "    # Look for \"Course Curriculum\" section\n",
    "    curriculum_heading = soup.find(\"h2\", string=lambda text: text and \"Course Curriculum\" in text)\n",
    "    \n",
    "    if curriculum_heading:\n",
    "        # Find the parent container of the curriculum heading\n",
    "        curriculum_container = curriculum_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        \n",
    "        if curriculum_container:\n",
    "            # Find all sibling containers that might contain syllabus modules\n",
    "            next_element = curriculum_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            \n",
    "            while next_element:\n",
    "                # Check if this is a syllabus module (has a heading and content)\n",
    "                module_title_elem = next_element.find([\"h3\", \"h4\"])\n",
    "                if module_title_elem:\n",
    "                    # Look for the text editor widget container\n",
    "                    text_widget = next_element.find(\"div\", class_=\"elementor-widget-text-editor\")\n",
    "                    \n",
    "                    if text_widget:\n",
    "                        module_content_elem = text_widget.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                        \n",
    "                        if module_content_elem:\n",
    "                            module_title = clean_text(module_title_elem.get_text())\n",
    "                            module_content = \"\"\n",
    "                            \n",
    "                            # Extract content (list items or text)\n",
    "                            if module_content_elem.find(\"ul\"):\n",
    "                                items = []\n",
    "                                for li in module_content_elem.find_all(\"li\"):\n",
    "                                    items.append(f\"- {clean_text(li.get_text())}\")\n",
    "                                module_content = \"\\n\".join(items)\n",
    "                            else:\n",
    "                                module_content = clean_text(module_content_elem.get_text())\n",
    "                            \n",
    "                            syllabus_sections.append(f\"{module_title}\\n{module_content}\")\n",
    "                \n",
    "                # Move to the next sibling\n",
    "                next_element = next_element.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "    \n",
    "    return \"\\n\\n\".join(syllabus_sections) if syllabus_sections else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT FACULTY --------------------\n",
    "def extract_faculty(soup):\n",
    "    faculty_info = []\n",
    "    \n",
    "    # Look for instructor information in the course header\n",
    "    instructor_elements = soup.select(\"ul.eb-course-header-meta-items li.instructor\")\n",
    "    for instructor in instructor_elements:\n",
    "        faculty_info.append(clean_text(instructor.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(faculty_info) if faculty_info else \"Not Found\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Scroll to load all content\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract only the required course information\n",
    "        course_name = extract_course_name(soup)\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        about_course = extract_about_course(soup)\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        who_should_take = extract_who_should_take(soup)\n",
    "        print(f\"üë• Who Should Take: {who_should_take[:100]}...\")\n",
    "\n",
    "        syllabus = extract_syllabus(soup)\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus.splitlines())} lines)\")\n",
    "\n",
    "        faculty = extract_faculty(soup)\n",
    "        print(f\"üë®‚Äçüè´ Faculty: {faculty}\")\n",
    "\n",
    "        return [\n",
    "            course_name, about_course, who_should_take, syllabus, faculty, url\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 6\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Who Should Take\", \"Syllabus\", \"Faculty\", \"Course URL\"\n",
    "    ]\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        new_row = pd.DataFrame([dict(zip(columns, data))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "        \"https://megasofttech.in/courses/data-science-analytics-courses/data-engineering/\"\n",
    "    ]\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\MKCL.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüìñ Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, file_path)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"‚úÖ Process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce015d38",
   "metadata": {},
   "source": [
    "# Megasoft courses for input excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524a4431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "üìñ Found 14 URLs to process\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/data-science-analytics-courses/data-engineering/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/data-science-analytics-courses/data-engineering/\n",
      "üìõ Course Name: Data Engineering\n",
      "üìù About Course: - Focused training on building and managing data pipelines and infrastructure.\n",
      "- Learn to design, co...\n",
      "üë• Who Should Take: - IT professionals transitioning to data-focused roles.\n",
      "- Software developers expanding into data en...\n",
      "üìö Syllabus extracted (26 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Engineering\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/data-science-analytics-courses/data-science-course/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/data-science-analytics-courses/data-science-course/\n",
      "üìõ Course Name: Data Science Course\n",
      "üìù About Course: - Comprehensive training in analytics, machine learning, and AI.\n",
      "- Designed for beginners and profes...\n",
      "üë• Who Should Take: - Beginners interested in Data Science.\n",
      "- IT professionals expanding their expertise.\n",
      "- Business ana...\n",
      "üìö Syllabus extracted (20 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Science Course\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/data-science-analytics-courses/data-analytics/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/data-science-analytics-courses/data-analytics/\n",
      "üìõ Course Name: Data Analytics\n",
      "üìù About Course: - Comprehensive training in data analytics tools, techniques, and frameworks.\n",
      "- Learn data manipulat...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (11 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Data Analytics\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/express-frame-work/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/express-frame-work/\n",
      "üìõ Course Name: Express Frame Work Development\n",
      "üìù About Course: - Comprehensive training in Express.js, a minimal and flexible Node.js framework.\n",
      "- Learn to build f...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (20 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Express Frame Work Development\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/mongodb-course/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/mongodb-course/\n",
      "üìõ Course Name: MongoDB Course\n",
      "üìù About Course: - Comprehensive training in MongoDB, a leading NoSQL database.\n",
      "- Learn to design, query, and manage ...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (79 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: MongoDB Course\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/node-js-development/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/node-js-development/\n",
      "üìõ Course Name: Nodejs Development\n",
      "üìù About Course: - Comprehensive training in Node.js for server-side development.\n",
      "- Learn asynchronous programming, e...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (47 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Nodejs Development\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/react-developer/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/react-developer/\n",
      "üìõ Course Name: React Developer\n",
      "üìù About Course: - Comprehensive training in React.js for building dynamic, single-page applications.\n",
      "- Learn to crea...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (74 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: React Developer\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/python-developement/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/python-developement/\n",
      "üìõ Course Name: Python Development\n",
      "üìù About Course: - Comprehensive training in Python programming for web, data, and software development.\n",
      "- Master Pyt...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (130 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Python Development\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/php-full-stack-developer/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/php-full-stack-developer/\n",
      "üìõ Course Name: PHP Full Stack Developer\n",
      "üìù About Course: - In-depth training in PHP programming for web development.\n",
      "- Master server-side scripting, handling...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (120 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: PHP Full Stack Developer\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/dot-net-developer/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/dot-net-developer/\n",
      "üìõ Course Name: DOT NET developer\n",
      "üìù About Course: - Comprehensive training in .NET framework and its core components.\n",
      "- Learn to develop robust web, d...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (110 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: DOT NET developer\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/software-development/full-stack-java-developer/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/software-development/full-stack-java-developer/\n",
      "üìõ Course Name: Full Stack Java Developer\n",
      "üìù About Course: This course is designed to provide a comprehensive understanding of full-stack development using Jav...\n",
      "üë• Who Should Take: Not Found...\n",
      "üìö Syllabus extracted (135 lines)\n",
      "üë®‚Äçüè´ Faculty: By megasoft\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Full Stack Java Developer\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/cloud-engineering/klic-aws-training/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/cloud-engineering/klic-aws-training/\n",
      "üìõ Course Name: AWS Training Course\n",
      "üìù About Course: Not Found...\n",
      "üë• Who Should Take: - IT professionals transitioning to cloud roles.\n",
      "- Software developers aiming to build cloud-native ...\n",
      "üìö Syllabus extracted (33 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: AWS Training Course\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/cloud-engineering/klic-devops-training/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/cloud-engineering/klic-devops-training/\n",
      "üìõ Course Name: DevOps Training Course\n",
      "üìù About Course: - Comprehensive training on DevOps practices, tools, and culture.\n",
      "- Learn to automate development, t...\n",
      "üë• Who Should Take: - IT professionals transitioning to DevOps roles.\n",
      "- Software developers looking to streamline deploy...\n",
      "üìö Syllabus extracted (1 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: DevOps Training Course\n",
      "\n",
      "üìñ Processing: https://megasofttech.in/courses/cloud-engineering/salesforce-training-course/\n",
      "üåê Accessing URL: https://megasofttech.in/courses/cloud-engineering/salesforce-training-course/\n",
      "üìõ Course Name: Salesforce Training Course\n",
      "üìù About Course: Not Found...\n",
      "üë• Who Should Take: - IT professionals transitioning to Salesforce development roles.\n",
      "- Developers aiming to enhance ski...\n",
      "üìö Syllabus extracted (32 lines)\n",
      "üë®‚Äçüè´ Faculty: By Pavan K\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Salesforce Training Course\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    # options.add_argument(\"--headless=new\")  # Uncomment for silent scraping\n",
    "    driver = uc.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- CLEAN TEXT --------------------\n",
    "def clean_text(text):\n",
    "    return \" \".join(text.split()) if text else text\n",
    "\n",
    "# -------------------- EXTRACT COURSE NAME --------------------\n",
    "def extract_course_name(soup):\n",
    "    # Try multiple selectors for course name\n",
    "    selectors = [\n",
    "        \"div.edublink-course-title h1.entry-title\",\n",
    "        \"h1.course-title\", \n",
    "        \"h1.title\",\n",
    "        \"h1\",\n",
    "        \"title\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            return clean_text(element.get_text(strip=True))\n",
    "    \n",
    "    return \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT ABOUT COURSE --------------------\n",
    "def extract_about_course(soup):\n",
    "    about_text = []\n",
    "    \n",
    "    # Look for the specific course description section in the HTML structure\n",
    "    course_desc_heading = soup.find(\"h2\", string=lambda text: text and \"Course Description\" in text)\n",
    "    \n",
    "    if course_desc_heading:\n",
    "        # Find the parent container\n",
    "        parent_container = course_desc_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        if parent_container:\n",
    "            # Find the text container that follows the heading\n",
    "            text_container = parent_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            if text_container:\n",
    "                # Extract text from the widget container\n",
    "                widget_container = text_container.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                if widget_container:\n",
    "                    # Extract list items if present\n",
    "                    if widget_container.find(\"ul\"):\n",
    "                        for li in widget_container.find_all(\"li\"):\n",
    "                            about_text.append(f\"- {clean_text(li.get_text())}\")\n",
    "                    else:\n",
    "                        # Extract paragraph text\n",
    "                        about_text.append(clean_text(widget_container.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(about_text) if about_text else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT WHO_SHOULD_TAKE --------------------\n",
    "def extract_who_should_take(soup):\n",
    "    who_text = []\n",
    "    \n",
    "    # Look for \"Who Can Take This Course\" section\n",
    "    who_heading = soup.find(\"h2\", string=lambda text: text and \"Who Can Take This Course\" in text)\n",
    "    \n",
    "    if who_heading:\n",
    "        # Find the parent container\n",
    "        parent_container = who_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        if parent_container:\n",
    "            # Find the text container that follows the heading\n",
    "            text_container = parent_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            if text_container:\n",
    "                # Extract text from the widget container\n",
    "                widget_container = text_container.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                if widget_container:\n",
    "                    # Extract list items if present\n",
    "                    if widget_container.find(\"ul\"):\n",
    "                        for li in widget_container.find_all(\"li\"):\n",
    "                            who_text.append(f\"- {clean_text(li.get_text())}\")\n",
    "                    else:\n",
    "                        # Extract paragraph text\n",
    "                        who_text.append(clean_text(widget_container.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(who_text) if who_text else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT SYLLABUS --------------------\n",
    "def extract_syllabus(soup):\n",
    "    syllabus_sections = []\n",
    "    \n",
    "    # Look for \"Course Curriculum\" section\n",
    "    curriculum_heading = soup.find(\"h2\", string=lambda text: text and \"Course Curriculum\" in text)\n",
    "    \n",
    "    if curriculum_heading:\n",
    "        # Find the parent container of the curriculum heading\n",
    "        curriculum_container = curriculum_heading.find_parent(\"div\", class_=\"elementor-element\")\n",
    "        \n",
    "        if curriculum_container:\n",
    "            # Find all sibling containers that might contain syllabus modules\n",
    "            next_element = curriculum_container.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "            \n",
    "            while next_element:\n",
    "                # Check if this is a syllabus module (has a heading and content)\n",
    "                module_title_elem = next_element.find([\"h3\", \"h4\"])\n",
    "                if module_title_elem:\n",
    "                    # Look for the text editor widget container\n",
    "                    text_widget = next_element.find(\"div\", class_=\"elementor-widget-text-editor\")\n",
    "                    \n",
    "                    if text_widget:\n",
    "                        module_content_elem = text_widget.find(\"div\", class_=\"elementor-widget-container\")\n",
    "                        \n",
    "                        if module_content_elem:\n",
    "                            module_title = clean_text(module_title_elem.get_text())\n",
    "                            module_content = \"\"\n",
    "                            \n",
    "                            # Extract content (list items or text)\n",
    "                            if module_content_elem.find(\"ul\"):\n",
    "                                items = []\n",
    "                                for li in module_content_elem.find_all(\"li\"):\n",
    "                                    items.append(f\"- {clean_text(li.get_text())}\")\n",
    "                                module_content = \"\\n\".join(items)\n",
    "                            else:\n",
    "                                module_content = clean_text(module_content_elem.get_text())\n",
    "                            \n",
    "                            syllabus_sections.append(f\"{module_title}\\n{module_content}\")\n",
    "                \n",
    "                # Move to the next sibling\n",
    "                next_element = next_element.find_next_sibling(\"div\", class_=\"elementor-element\")\n",
    "    \n",
    "    return \"\\n\\n\".join(syllabus_sections) if syllabus_sections else \"Not Found\"\n",
    "\n",
    "# -------------------- EXTRACT FACULTY --------------------\n",
    "def extract_faculty(soup):\n",
    "    faculty_info = []\n",
    "    \n",
    "    # Look for instructor information in the course header\n",
    "    instructor_elements = soup.select(\"ul.eb-course-header-meta-items li.instructor\")\n",
    "    for instructor in instructor_elements:\n",
    "        faculty_info.append(clean_text(instructor.get_text()))\n",
    "    \n",
    "    return \"\\n\".join(faculty_info) if faculty_info else \"Not Found\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Scroll to load all content\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract only the required course information\n",
    "        course_name = extract_course_name(soup)\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        about_course = extract_about_course(soup)\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        who_should_take = extract_who_should_take(soup)\n",
    "        print(f\"üë• Who Should Take: {who_should_take[:100]}...\")\n",
    "\n",
    "        syllabus = extract_syllabus(soup)\n",
    "        print(f\"üìö Syllabus extracted ({len(syllabus.splitlines())} lines)\")\n",
    "\n",
    "        faculty = extract_faculty(soup)\n",
    "        print(f\"üë®‚Äçüè´ Faculty: {faculty}\")\n",
    "\n",
    "        return [\n",
    "            course_name, about_course, who_should_take, syllabus, faculty, url\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 6\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Who Should Take\", \"Syllabus\", \"Faculty\", \"Course URL\"\n",
    "    ]\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        if data[-1] in df[\"Course URL\"].values:\n",
    "            print(f\"üîÑ Course already exists: {data[0]}\")\n",
    "            return\n",
    "\n",
    "        new_row = pd.DataFrame([dict(zip(columns, data))])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {data[0]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- READ URLS FROM EXCEL --------------------\n",
    "def read_urls_from_excel(file_path, sheet_name=0, url_column=\"Course URL\"):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        urls = df[url_column].dropna().tolist()\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading Excel file: {e}\")\n",
    "        return []\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Read input Excel file with course URLs\n",
    "    input_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\mega soft CCDC.xlsx\"  # Update this path\n",
    "    output_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\megasoft.xlsx\"\n",
    "    \n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    \n",
    "    # Read URLs from input Excel file\n",
    "    course_urls = read_urls_from_excel(input_file_path)\n",
    "    \n",
    "    if not course_urls:\n",
    "        print(\"‚ùå No URLs found in the input file\")\n",
    "    else:\n",
    "        print(f\"üìñ Found {len(course_urls)} URLs to process\")\n",
    "        \n",
    "        for course_url in course_urls:\n",
    "            print(f\"\\nüìñ Processing: {course_url}\")\n",
    "            course_data = scrape_course_data(course_url)\n",
    "            if all(item != \"Error\" for item in course_data):\n",
    "                save_to_excel(course_data, output_file_path)\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"‚úÖ Process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4b855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
