{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af01bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- FIXED SYLLABUS EXTRACTION FOR NESTED UL STRUCTURE --------------------\n",
    "def extract_syllabus_fixed(soup):\n",
    "    \"\"\"Extract syllabus handling the nested UL structure correctly\"\"\"\n",
    "    modules = []\n",
    "    \n",
    "    accordion = soup.find('div', id='accordionExamplecurriculum')\n",
    "    if not accordion:\n",
    "        return [{\"title\": \"Syllabus not available\", \"content\": \"Could not extract syllabus content\"}]\n",
    "    \n",
    "    cards = accordion.find_all('div', class_='card')\n",
    "    \n",
    "    for card in cards:\n",
    "        header = card.find('div', class_='card-header')\n",
    "        if header:\n",
    "            button = header.find('button')\n",
    "            if button:\n",
    "                module_title = clean_text(button.get_text())\n",
    "                \n",
    "                card_body = card.find('div', class_='card-body')\n",
    "                if card_body:\n",
    "                    if \"Training Curriculum\" in module_title:\n",
    "                        module_content = extract_curriculum_fixed(card_body)\n",
    "                    else:\n",
    "                        module_content = extract_module_fixed(card_body)\n",
    "                    \n",
    "                    if module_title and module_content:\n",
    "                        modules.append({\n",
    "                            \"title\": module_title,\n",
    "                            \"content\": module_content\n",
    "                        })\n",
    "    \n",
    "    return modules\n",
    "\n",
    "def extract_module_fixed(card_body):\n",
    "    \"\"\"Extract module content handling nested UL structure\"\"\"\n",
    "    content_parts = []\n",
    "    main_uls = card_body.find_all('ul')\n",
    "    \n",
    "    for main_ul in main_uls:\n",
    "        topics_content = extract_topics_from_main_ul(main_ul)\n",
    "        if topics_content:\n",
    "            content_parts.append(topics_content)\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "def extract_topics_from_main_ul(main_ul):\n",
    "    topics_content = []\n",
    "    for child in main_ul.children:\n",
    "        if hasattr(child, 'name'):\n",
    "            if child.name == 'p' and 'text-align-first-with-icon' in child.get('class', []):\n",
    "                topic_text = clean_text(child.get_text())\n",
    "                if topic_text:\n",
    "                    bullet_points = extract_all_bullet_points_from_topic(child)\n",
    "                    topics_content.append(topic_text)\n",
    "                    topics_content.extend(bullet_points)\n",
    "    return \"\\n\".join(topics_content)\n",
    "\n",
    "def extract_all_bullet_points_from_topic(topic_element):\n",
    "    bullet_points = []\n",
    "    current_element = topic_element.next_sibling\n",
    "    \n",
    "    while current_element:\n",
    "        if hasattr(current_element, 'name'):\n",
    "            if current_element.name == 'ul':\n",
    "                list_items = current_element.find_all('li', class_='text-design second')\n",
    "                for li in list_items:\n",
    "                    li_text = clean_text(li.get_text())\n",
    "                    if li_text:\n",
    "                        bullet_points.append(f\"  ‚Ä¢ {li_text}\")\n",
    "            elif current_element.name == 'p':\n",
    "                break\n",
    "        current_element = current_element.next_sibling\n",
    "    \n",
    "    return bullet_points\n",
    "\n",
    "def extract_curriculum_fixed(card_body):\n",
    "    content_parts = []\n",
    "    paragraphs = card_body.find_all('p', class_='text-align-first-with-icon')\n",
    "    for p in paragraphs:\n",
    "        p_text = clean_text(p.get_text())\n",
    "        if p_text and p_text not in content_parts:\n",
    "            content_parts.append(p_text)\n",
    "    \n",
    "    learning_items_added = False\n",
    "    for main_ul in card_body.find_all('ul'):\n",
    "        for child in main_ul.children:\n",
    "            if hasattr(child, 'name') and child.name == 'p':\n",
    "                if 'In this program you will learn' in child.get_text():\n",
    "                    content_parts.append(\"In this program you will learn:\")\n",
    "                    nested_uls = main_ul.find_all('ul')\n",
    "                    for nested_ul in nested_uls:\n",
    "                        list_items = nested_ul.find_all('li', class_='text-design second')\n",
    "                        for li in list_items:\n",
    "                            li_text = clean_text(li.get_text())\n",
    "                            if li_text:\n",
    "                                content_parts.append(f\"‚Ä¢ {li_text}\")\n",
    "                    learning_items_added = True\n",
    "                    break\n",
    "        if learning_items_added:\n",
    "            break\n",
    "    \n",
    "    return \"\\n\".join(content_parts)\n",
    "\n",
    "# -------------------- ENHANCED DEBUGGING --------------------\n",
    "def debug_html_structure_detailed(soup):\n",
    "    print(\"\\nüîç DETAILED HTML STRUCTURE ANALYSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    accordion = soup.find('div', id='accordionExamplecurriculum')\n",
    "    if accordion:\n",
    "        cards = accordion.find_all('div', class_='card')\n",
    "        for card in cards:\n",
    "            header = card.find('div', class_='card-header')\n",
    "            if header:\n",
    "                button = header.find('button')\n",
    "                if button and 'Module 1: ASP.NET and MVC' in button.get_text():\n",
    "                    print(f\"\\nüìñ ANALYZING: Module 1: ASP.NET and MVC\")\n",
    "                    card_body = card.find('div', class_='card-body')\n",
    "                    if card_body:\n",
    "                        main_uls = card_body.find_all('ul')\n",
    "                        print(f\"Main UL elements: {len(main_uls)}\")\n",
    "\n",
    "# -------------------- FEE STRUCTURE --------------------\n",
    "def get_fee_structure(price):\n",
    "    if price and price != \"Not available\":\n",
    "        fee_structure = (\n",
    "            f\"{price} \\n- All other fees remain unchanged\\n\"\n",
    "            \"- Education loans are available through leading banks and NBFCs.\"\n",
    "        )\n",
    "    else:\n",
    "        fee_structure = (\n",
    "            \"- All other fees remain unchanged\\n\"\n",
    "            \"- Education loans are available through leading banks and NBFCs.\"\n",
    "        )\n",
    "    return fee_structure\n",
    "\n",
    "# -------------------- CERTIFICATE --------------------\n",
    "def extract_certificate(soup):\n",
    "    cert_img = soup.find('img', alt=re.compile(r'certificate', re.IGNORECASE))\n",
    "    if cert_img:\n",
    "        return cert_img.get('src', '') or cert_img.get('data-src', 'Certificate image found but URL not available')\n",
    "    \n",
    "    cert_imgs = soup.find_all('img', src=re.compile(r'certificate', re.IGNORECASE))\n",
    "    if cert_imgs:\n",
    "        return cert_imgs[0].get('src', 'Certificate image URL found')\n",
    "    \n",
    "    return \"https://www.cromacampus.com/public/img/Certificate-new-file.webp\"\n",
    "\n",
    "# -------------------- TEST FUNCTION --------------------\n",
    "def test_single_course_final(url):\n",
    "    print(\"üß™ FINAL SYLLABUS EXTRACTION TEST\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    driver = get_driver(headless=True)  # switched to headless for batch runs\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        course_name = extract_course_name(soup)\n",
    "        about_course = extract_about_course(soup)\n",
    "        duration = extract_duration(soup)\n",
    "        learning_mode = extract_learning_mode(soup)\n",
    "        price = extract_price(soup)\n",
    "        certificate = extract_certificate(soup)\n",
    "        modules = extract_syllabus_fixed(soup)\n",
    "        syllabus_content = format_syllabus_for_excel(modules)\n",
    "        fee_structure = get_fee_structure(price)\n",
    "        \n",
    "        course_data = {\n",
    "            \"Course Name\": course_name,\n",
    "            \"About Course\": about_course,\n",
    "            \"Duration\": duration,\n",
    "            \"Price\": price,\n",
    "            \"Syllabus\": syllabus_content,\n",
    "            \"Certificate\": certificate,\n",
    "            \"Fee Structure\": fee_structure,\n",
    "            \"Learning Mode\": learning_mode,\n",
    "            \"Course URL\": url\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Extraction completed\")\n",
    "        return course_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå TEST FAILED: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- MAIN FUNCTION (EXCEL MULTIPLE URLS) --------------------\n",
    "def main_test_from_excel():\n",
    "    input_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\Croma campus all course.xlsx\"   # input file\n",
    "    output_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\multi_course_output_croma campus.xlsx\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read input Excel file: {e}\")\n",
    "        return\n",
    "\n",
    "    if \"Course URL\" not in df.columns:\n",
    "        print(\"‚ùå Excel file must contain a column named 'Course URL'\")\n",
    "        return\n",
    "\n",
    "    all_results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        url = str(row[\"Course URL\"]).strip()\n",
    "        if not url or not url.startswith((\"http://\", \"https://\")):\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid URL at row {idx+2}: {url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüöÄ Processing {idx+1}/{len(df)}: {url}\")\n",
    "        result = test_single_course_final(url)\n",
    "        if result:\n",
    "            all_results.append(result)\n",
    "\n",
    "    if all_results:\n",
    "        try:\n",
    "            final_df = pd.DataFrame(all_results)\n",
    "            final_df.to_excel(output_file, index=False)\n",
    "            print(f\"\\n‚úÖ All courses extracted successfully!\")\n",
    "            print(f\"üìÅ Output saved to: {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save final Excel file: {e}\")\n",
    "    else:\n",
    "        print(\"‚ùå No results extracted\")\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_course_name(soup):\n",
    "    course_name_tag = soup.find(\"div\", class_=\"banner-heading\")\n",
    "    if course_name_tag:\n",
    "        h1_tag = course_name_tag.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            return clean_text(h1_tag.get_text())\n",
    "    h1_tag = soup.find(\"h1\")\n",
    "    if h1_tag:\n",
    "        return clean_text(h1_tag.get_text())\n",
    "    return \"Course name not found\"\n",
    "\n",
    "def extract_about_course(soup):\n",
    "    about_points = []\n",
    "    uls = soup.find_all('ul')\n",
    "    for ul in uls:\n",
    "        lis = ul.find_all('li', style=\"font-size:13px\")\n",
    "        for li in lis:\n",
    "            text = clean_text(li.get_text())\n",
    "            if text and len(text) > 50:\n",
    "                about_points.append(text)\n",
    "    if about_points:\n",
    "        return \"\\n\".join([f\"- {point}\" for point in about_points])\n",
    "    return \"About course not found\"\n",
    "\n",
    "def extract_duration(soup):\n",
    "    duration_div = soup.find('div', class_='first-section-duation')\n",
    "    if duration_div:\n",
    "        p_tags = duration_div.find_all('p')\n",
    "        if len(p_tags) >= 1:\n",
    "            return clean_text(p_tags[0].get_text())\n",
    "    return \"Duration not found\"\n",
    "\n",
    "def extract_learning_mode(soup):\n",
    "    mode_div = soup.find('div', class_='first-section-duation border-for-right')\n",
    "    if mode_div:\n",
    "        p_tags = mode_div.find_all('p')\n",
    "        if len(p_tags) >= 1:\n",
    "            return clean_text(p_tags[0].get_text())\n",
    "    return \"Learning mode not found\"\n",
    "\n",
    "def extract_price(soup):\n",
    "    price_span = soup.find('span', class_='disc-amt')\n",
    "    if price_span:\n",
    "        price_text = clean_text(price_span.get_text())\n",
    "        price_match = re.search(r'[‚Çπ\\d.,]+', price_text)\n",
    "        if price_match:\n",
    "            return price_match.group()\n",
    "    return \"Price not found\"\n",
    "\n",
    "def save_single_course_to_excel(course_data, output_file_path):\n",
    "    try:\n",
    "        df = pd.DataFrame([course_data])\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        print(f\"üíæ Course data saved to: {output_file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving to Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "def format_syllabus_for_excel(modules):\n",
    "    if not modules or (len(modules) == 1 and \"not available\" in modules[0][\"title\"].lower()):\n",
    "        return \"Syllabus not available\"\n",
    "    \n",
    "    syllabus_text = \"\"\n",
    "    for i, module in enumerate(modules, 1):\n",
    "        title = module['title']\n",
    "        content = module['content']\n",
    "        clean_title = re.sub(r'^(Module\\s+\\d+:\\s*)+', '', title)\n",
    "        clean_title = re.sub(r'Module\\s+\\d+\\s*:\\s*', '', clean_title)\n",
    "        \n",
    "        if \"Training Curriculum\" in title:\n",
    "            syllabus_text += f\"{clean_title}\\n\"\n",
    "            syllabus_text += \"=\" * 60 + \"\\n\"\n",
    "            syllabus_text += f\"{content}\\n\\n\"\n",
    "        else:\n",
    "            module_match = re.search(r'Module\\s+(\\d+)', title)\n",
    "            if module_match:\n",
    "                module_num = module_match.group(1)\n",
    "                syllabus_text += f\"Module {module_num}: {clean_title}\\n\"\n",
    "            else:\n",
    "                syllabus_text += f\"Module {i-1}: {clean_title}\\n\"\n",
    "            syllabus_text += \"=\" * 60 + \"\\n\"\n",
    "            syllabus_text += f\"{content}\\n\\n\"\n",
    "        syllabus_text += \"-\" * 60 + \"\\n\\n\"\n",
    "    \n",
    "    return syllabus_text.strip()\n",
    "\n",
    "# -------------------- ENTRY --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # main_test()  # single test run\n",
    "    main_test_from_excel()  # run for all links in Excel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382235c",
   "metadata": {},
   "source": [
    "course link https://www.cromacampus.com/master-program/professional-in-data-analytics-with-powerbi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Processing 1/2: https://www.cromacampus.com/courses/ethical-hacking-training-in-noida\n",
      "\n",
      "üöÄ Processing 2/2: https://www.cromacampus.com/courses/cicd-jenkins-certification-training/\n",
      "\n",
      "‚úÖ All courses extracted successfully!\n",
      "üìÅ Output saved to: C:\\Users\\taslim.siddiqui\\Downloads\\test_croma campus.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_driver(headless=True):\n",
    "    \"\"\"Initialize Chrome driver\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_course_name(soup):\n",
    "    \"\"\"Extract course name from the page\"\"\"\n",
    "    curriculum_heading = soup.find('div', class_='curriculum-heading')\n",
    "    if curriculum_heading:\n",
    "        h3 = curriculum_heading.find('h3')\n",
    "        if h3:\n",
    "            return clean_text(h3.get_text())\n",
    "    \n",
    "    selectors = [\n",
    "        \"div.banner-heading h1\",\n",
    "        \"h1\",\n",
    "        \"title\"\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            text = clean_text(element.get_text())\n",
    "            if text:\n",
    "                return text\n",
    "    \n",
    "    return \"Course name not found\"\n",
    "\n",
    "def extract_syllabus_content(soup):\n",
    "    \"\"\"Extract all syllabus content with improved parsing\"\"\"\n",
    "    syllabus_parts = []\n",
    "    accordion = (soup.find('div', id='accordionExamplecurriculum') or \n",
    "                 soup.find('div', class_='curriculum-accordian') or\n",
    "                 soup.find('div', class_='accordion'))\n",
    "    \n",
    "    if not accordion:\n",
    "        print(\"‚ùå No curriculum accordion found\")\n",
    "        return extract_fallback_syllabus(soup)\n",
    "    \n",
    "    print(f\"‚úÖ Found curriculum accordion\")\n",
    "    cards = (accordion.find_all('div', class_='card') or \n",
    "             accordion.find_all('div', class_='accordion-item') or\n",
    "             accordion.find_all('div', recursive=False))\n",
    "    \n",
    "    print(f\"üìö Found {len(cards)} cards/modules\")\n",
    "    \n",
    "    for i, card in enumerate(cards, 1):\n",
    "        header = (card.find('div', class_='card-header') or \n",
    "                  card.find('div', class_='accordion-header') or\n",
    "                  card.find(['h2', 'h3', 'h4']))\n",
    "        \n",
    "        if header:\n",
    "            button = header.find('button')\n",
    "            if button:\n",
    "                module_title = clean_text(button.get_text())\n",
    "            else:\n",
    "                module_title = clean_text(header.get_text())\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if not module_title or 'download curriculum' in module_title.lower():\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing module: {module_title}\")\n",
    "        \n",
    "        card_body = (card.find('div', class_='card-body') or \n",
    "                     card.find('div', class_='accordion-body') or\n",
    "                     card.find('div', class_='collapse') or\n",
    "                     card.find('div', class_='show1'))\n",
    "        \n",
    "        if card_body:\n",
    "            module_content = extract_module_content_improved(card_body)\n",
    "            if module_content:\n",
    "                syllabus_parts.append(f\"{module_title}\")\n",
    "                syllabus_parts.append(\"=\" * 60)\n",
    "                syllabus_parts.append(module_content)\n",
    "                syllabus_parts.append(\"\")  \n",
    "    \n",
    "    return \"\\n\".join(syllabus_parts) if syllabus_parts else extract_fallback_syllabus(soup)\n",
    "\n",
    "def extract_module_content_improved(card_body):\n",
    "    \"\"\"Improved content extraction with multiple strategies\"\"\"\n",
    "    content_parts = []\n",
    "    for unwanted in card_body.find_all(['div', 'button'], class_=re.compile(r'download|syllabus', re.I)):\n",
    "        unwanted.decompose()\n",
    "    \n",
    "    main_uls = card_body.find_all('ul')\n",
    "    for main_ul in main_uls:\n",
    "        for child in main_ul.children:\n",
    "            if hasattr(child, 'name'):\n",
    "                if child.name == 'p':\n",
    "                    topic_classes = child.get('class', [])\n",
    "                    if any('text-align-first-with-icon' in cls for cls in topic_classes) or 'first' in str(topic_classes):\n",
    "                        topic_text = clean_text(child.get_text())\n",
    "                        if topic_text and len(topic_text) > 3:\n",
    "                            content_parts.append(f\"üìö {topic_text}\")\n",
    "                            bullet_points = extract_bullet_points_improved(child)\n",
    "                            for bullet in bullet_points:\n",
    "                                content_parts.append(f\"    ‚Ä¢ {bullet}\")\n",
    "                            if bullet_points:\n",
    "                                content_parts.append(\"\")\n",
    "    \n",
    "    if not content_parts:\n",
    "        all_text = card_body.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n",
    "        for line in lines:\n",
    "            if line and not any(x in line.lower() for x in ['download', 'curriculum', 'get full']):\n",
    "                if len(line) > 10:\n",
    "                    content_parts.append(f\"‚Ä¢ {line}\")\n",
    "    \n",
    "    return \"\\n\".join(content_parts) if content_parts else \"No content available for this module\"\n",
    "\n",
    "def extract_bullet_points_improved(topic_element):\n",
    "    bullet_points = []\n",
    "    current_element = topic_element.next_sibling\n",
    "    while current_element:\n",
    "        if hasattr(current_element, 'name'):\n",
    "            if current_element.name == 'ul':\n",
    "                list_items = current_element.find_all('li')\n",
    "                for li in list_items:\n",
    "                    li_text = clean_text(li.get_text())\n",
    "                    if li_text and len(li_text) > 3:\n",
    "                        bullet_points.append(li_text)\n",
    "            elif current_element.name == 'p':\n",
    "                topic_classes = current_element.get('class', [])\n",
    "                if any('text-align-first-with-icon' in cls for cls in topic_classes) or 'first' in str(topic_classes):\n",
    "                    break\n",
    "        current_element = current_element.next_sibling\n",
    "    return bullet_points\n",
    "\n",
    "def extract_fallback_syllabus(soup):\n",
    "    print(\"üîÑ Using fallback syllabus extraction\")\n",
    "    syllabus_parts = []\n",
    "    possible_sections = soup.find_all(['div', 'section'], class_=re.compile(r'curriculum|syllabus|module|course-content', re.I))\n",
    "    for section in possible_sections:\n",
    "        text_content = section.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in text_content.split('\\n') if line.strip()]\n",
    "        for line in lines:\n",
    "            if (len(line) > 20 and not any(x in line.lower() for x in ['download', 'enroll', 'contact', 'fee', 'price'])):\n",
    "                syllabus_parts.append(f\"‚Ä¢ {line}\")\n",
    "    return \"\\n\".join(syllabus_parts) if syllabus_parts else \"Syllabus content not available on page\"\n",
    "\n",
    "def extract_single_course_data(url):\n",
    "    print(f\"üåê Extracting data from: {url}\")\n",
    "    driver = get_driver(headless=False)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(5)\n",
    "        if \"404\" in driver.title or \"not found\" in driver.title.lower():\n",
    "            print(\"‚ùå Page not found (404)\")\n",
    "            return None\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        course_name = extract_course_name(soup)\n",
    "        print(f\"‚úÖ Course Name: {course_name}\")\n",
    "        syllabus_content = extract_syllabus_content(soup)\n",
    "        course_data = {\n",
    "            \"Course Name\": course_name,\n",
    "            \"Course Syllabus\": syllabus_content,\n",
    "            \"Course URL\": url\n",
    "        }\n",
    "        print(\"‚úÖ Extraction completed successfully!\")\n",
    "        return course_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def save_to_excel(course_data_list, filename=\"multiple_courses_output.xlsx\"):\n",
    "    \"\"\"Save multiple course data to Excel\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(course_data_list)\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"üíæ All course data saved to: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving to Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_course_info(course_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã EXTRACTED COURSE INFORMATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üè∑Ô∏è  COURSE NAME: {course_data['Course Name']}\")\n",
    "    print(f\"üîó URL: {course_data['Course URL']}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìñ COURSE SYLLABUS\")\n",
    "    print(\"=\"*80)\n",
    "    print(course_data['Course Syllabus'][:1000] + \"...\" if len(course_data['Course Syllabus']) > 1000 else course_data['Course Syllabus'])\n",
    "\n",
    "# -------------------- MAIN EXECUTION --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ STARTING MULTIPLE COURSE EXTRACTION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # üìò Input Excel file path\n",
    "    input_excel = r\"C:\\Users\\taslim.siddiqui\\Downloads\\Croma campus all course.xlsx\"  # must contain 'Course Link' column\n",
    "    df = pd.read_excel(input_excel)\n",
    "\n",
    "    if 'Course Link' not in df.columns:\n",
    "        print(\"‚ùå 'Course Link' column not found in Excel file!\")\n",
    "    else:\n",
    "        all_data = []\n",
    "        for idx, url in enumerate(df['Course Link'].dropna(), 1):\n",
    "            print(f\"\\nüîπ [{idx}] Processing URL: {url}\")\n",
    "            course_data = extract_single_course_data(url)\n",
    "            if course_data:\n",
    "                all_data.append(course_data)\n",
    "                display_course_info(course_data)\n",
    "\n",
    "        # üíæ Save all extracted data\n",
    "        output_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\multiple_courses_output_pending.xlsx\"\n",
    "        save_to_excel(all_data, output_file)\n",
    "        print(f\"\\n‚úÖ Process completed for {len(all_data)} courses! Check Excel file: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
