{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236ec73a",
   "metadata": {},
   "source": [
    "Test for one course  price , syllabus , about course , course faculty , language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b368a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìó Course Name: DevOps Labs: 14 Real DevOps Projects\n",
      "üåê Language: English (US)\n",
      "üë®‚Äçüè´ Course Faculty: Brian Su\n",
      "üí∞ Price: ‚Çπ995.00\n",
      "\n",
      "üìù About Course:\n",
      "DevOps Labs: 14 Real DevOps Projectsis your hands-on path to mastering the most in-demand DevOps skills. Whether you're an aspiring DevOps engineer, a developer aiming to understand infrastructure, or someone preparing for technical job interviews, this course equips you with practical experience across real-world scenarios.\n",
      "Containerization with DockerBuild, run, and manage containerized applications.\n",
      "Orchestration using KubernetesDeploy and scale apps using Kubernetes clusters.\n",
      "Version Control with Git, GitHub & GitLabCollaborate, manage source code, and automate pipelines.\n",
      "CI/CD PipelinesImplement Continuous Integration and Continuous Delivery workflows from scratch.\n",
      "Infrastructure as CodeLearn to define, manage, and provision infrastructure efficiently.\n",
      "Job Interview ReadinessTackle interview questions with confidence using project-based insights.\n",
      "14 real-world, end-to-end DevOps projects\n",
      "Step-by-step video labs & documentation\n",
      "Integration of Git, Docker, Kubernetes, GitHub Actions, GitLab CI/CD\n",
      "Code snippets, architecture diagrams & job interview tips\n",
      "Lifetime access with updates for modern DevOps tools\n",
      "\n",
      "üìò Section 1 : Introduction\n",
      "- Introduction\n",
      "- The list of all the DevOps labs\n",
      "- Course features\n",
      "\n",
      "üìò Section 2 : Prerequisites - Install DevOps tools in Windows\n",
      "- Install DevOps tools in Windows\n",
      "- Demo - How to Install Docker in Windows\n",
      "- Demo - How to install VirtualBox in Windows\n",
      "- Demo - How to install Vagrant in Windows\n",
      "- Demo - How to run Vagrant in Windows\n",
      "- Demo - Install Chocolatey in Windows\n",
      "- Demo - Install Helm in Windows\n",
      "- Demo - Install minikube in Windows\n",
      "- Demo - Start minikube in Windows\n",
      "- Demo - Install Kubectl in Windows\n",
      "\n",
      "üìò Section 3 : Prerequisites - Install DevOps tools in Mac\n",
      "- Install DevOps tools in Mac\n",
      "\n",
      "üìò Section 4 : Prerequisites - Install DevOps tools in in Ubuntu\n",
      "- Install DevOps tools in Ubuntu\n",
      "\n",
      "üìò Section 5 : Lab #01 Deploy an Go API app to Azure With ACR and AKS\n",
      "- Deploy an Go API app to Azure With ACR and AKS (1 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (2 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (3 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (4 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (5 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (6 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (7 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (8 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (9 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (10 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (11 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (12 of 13)\n",
      "- Deploy an Go API app to Azure With ACR and AKS (13 of 13)\n",
      "\n",
      "üìò Section 6 : Lab #02 Deploy Prometheus/Grafana on Minikube and Monitor\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (1 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (2 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (3 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (4 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (5 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (6 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (7 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (8 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (9 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (10 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (11 of 12)\n",
      "- Deploy Prometheus/Grafana on Minikube and Monitor The Health (12 of 12)\n",
      "\n",
      "üìò Section 7 : Lab #03 Nexus Repository, Jenkins Pipeline and Tomcat\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (1 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (2 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (3 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (4 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (5 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (6 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (7 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (8 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (9 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (10 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (11 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (12 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (13 of 14)\n",
      "- Nexus Repository, Jenkins Pipeline and Tomcat (14 of 14)\n",
      "\n",
      "üìò Section 8 : Lab #04 Backup Vault in Minio with a cronjob\n",
      "- Backup Vault in Minio (1 of 14)\n",
      "- Backup Vault in Minio (2 of 14)\n",
      "- Backup Vault in Minio (3 of 14)\n",
      "- Backup Vault in Minio (4 of 14)\n",
      "- Backup Vault in Minio (5 of 14)\n",
      "- Backup Vault in Minio (6 of 14)\n",
      "- Backup Vault in Minio (7 of 14)\n",
      "- Backup Vault in Minio (8 of 14)\n",
      "- Backup Vault in Minio (9 of 14)\n",
      "- Backup Vault in Minio (10 of 14)\n",
      "- Backup Vault in Minio (11 of 14)\n",
      "- Backup Vault in Minio (12 of 14)\n",
      "- Backup Vault in Minio (13 of 14)\n",
      "- Backup Vault in Minio (14 of 14)\n",
      "\n",
      "üìò Section 9 : Lab #05 Vault Jenkins Pipeline\n",
      "- Vault Jenkins Pipeline (1 of 9)\n",
      "- Vault Jenkins Pipeline (2 of 9)\n",
      "- Vault Jenkins Pipeline (3 of 9)\n",
      "- Vault Jenkins Pipeline (4 of 9)\n",
      "- Vault Jenkins Pipeline (5 of 9)\n",
      "- Vault Jenkins Pipeline (6 of 9)\n",
      "- Vault Jenkins Pipeline (7 of 9)\n",
      "- Vault Jenkins Pipeline (8 of 9)\n",
      "- Vault Jenkins Pipeline (9 of 9)\n",
      "\n",
      "üìò Section 10 : Lab #06 Helm Deployment in Kubernetes\n",
      "- Helm Deployment in Kubernetes (1 of 4)\n",
      "- Helm Deployment in Kubernetes (2 of 4)\n",
      "- Helm Deployment in Kubernetes (3 of 4)\n",
      "- Helm Deployment in Kubernetes (4 of 4)\n",
      "\n",
      "üìò Section 11 : Lab #07 Create Read Only Kubeconfig File\n",
      "- Create Read Only Kubeconfig File (1 of 5)\n",
      "- Create Read Only Kubeconfig File (2 of 5)\n",
      "- Create Read Only Kubeconfig File (3 of 5)\n",
      "- Create Read Only Kubeconfig File (4 of 5)\n",
      "- Create Read Only Kubeconfig File (5 of 5)\n",
      "\n",
      "üìò Section 12 : Lab #08 Deploy and Use Vault As Agent Sidecar Injector\n",
      "- Deploy and Use Vault As Agent Sidecar Injector (1 of 5)\n",
      "- Deploy and Use Vault As Agent Sidecar Injector (2 of 5)\n",
      "- Deploy and Use Vault As Agent Sidecar Injector (3 of 5)\n",
      "- Deploy and Use Vault As Agent Sidecar Injector (4 of 5)\n",
      "- Deploy and Use Vault As Agent Sidecar Injector (5 of 5)\n",
      "\n",
      "üìò Section 13 : Lab #09 Managing SSH Access with Vault\n",
      "- Managing SSH Access with Vault (1 of 8)\n",
      "- Managing SSH Access with Vault (2 of 8)\n",
      "- Managing SSH Access with Vault (3 of 8)\n",
      "- Managing SSH Access with Vault (4 of 8)\n",
      "- Managing SSH Access with Vault (5 of 8)\n",
      "- Managing SSH Access with Vault (6 of 8)\n",
      "- Managing SSH Access with Vault (7 of 8)\n",
      "- Managing SSH Access with Vault (8 of 8)\n",
      "\n",
      "üìò Section 14 : Lab #10 Jenkins CICD Pipeline\n",
      "- Jenkins CICD Pipeline (1 of 7)\n",
      "- Jenkins CICD Pipeline (2 of 7)\n",
      "- Jenkins CICD Pipeline (3 of 7)\n",
      "- Jenkins CICD Pipeline (4 of 7)\n",
      "- Jenkins CICD Pipeline (5 of 7)\n",
      "- Jenkins CICD Pipeline (6 of 7)\n",
      "- Jenkins CICD Pipeline (7 of 7)\n",
      "\n",
      "üìò Section 15 : Lab #11 Install Jenkins Using Ansible\n",
      "- Install Jenkins Using Ansible (1 of 5)\n",
      "- Install Jenkins Using Ansible (2 of 5)\n",
      "- Install Jenkins Using Ansible (3 of 5)\n",
      "- Install Jenkins Using Ansible (4 of 5)\n",
      "- Install Jenkins Using Ansible (5 of 5)\n",
      "\n",
      "üìò Section 16 : Lab #12 GitLab CICD Pipeline\n",
      "- GitLab CICD Pipeline (1 of 11)\n",
      "- GitLab CICD Pipeline (2 of 11)\n",
      "- GitLab CICD Pipeline (3 of 11)\n",
      "- GitLab CICD Pipeline (4 of 11)\n",
      "- GitLab CICD Pipeline (5 of 11)\n",
      "- GitLab CICD Pipeline (6 of 11)\n",
      "- GitLab CICD Pipeline (7 of 11)\n",
      "- GitLab CICD Pipeline (8 of 11)\n",
      "- GitLab CICD Pipeline (9 of 11)\n",
      "- GitLab CICD Pipeline (10 of 11)\n",
      "- GitLab CICD Pipeline (11 of 11)\n",
      "\n",
      "üìò Section 17 : Lab #13 Develop a Java app in Kubernetes for Monitoring ConfigMap Modifications\n",
      "- Java app in Kubernetes for Monitoring ConfigMap Modifications (1 of 5)\n",
      "- Java app in Kubernetes for Monitoring ConfigMap Modifications (2 of 5)\n",
      "- Java app in Kubernetes for Monitoring ConfigMap Modifications (3 of 5)\n",
      "- Java app in Kubernetes for Monitoring ConfigMap Modifications (4 of 5)\n",
      "- Java app in Kubernetes for Monitoring ConfigMap Modifications (5 of 5)\n",
      "\n",
      "üìò Section 18 : Lab #14 Deploy Docker with Terraform Script\n",
      "- Deploy Docker with Terraform Script (1 of 6)\n",
      "- Deploy Docker with Terraform Script (2 of 6)\n",
      "- Deploy Docker with Terraform Script (3 of 6)\n",
      "- Deploy Docker with Terraform Script (4 of 6)\n",
      "- Deploy Docker with Terraform Script (5 of 6)\n",
      "- Deploy Docker with Terraform Script (6 of 6)\n",
      "\n",
      "üìò Section 19 : Congratulations!\n",
      "- Way to Go!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# === Input: One Course URL ===\n",
    "course_link = \"https://www.eduonix.com/devops-labs-14-real-devops-projects\"\n",
    "\n",
    "# === Setup Headless Chrome ===\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# === Utility: Check if text is a question ===\n",
    "def is_question(text):\n",
    "    question_starters = ['why', 'how', 'what', 'when', 'can', 'do', 'does', 'is', 'are', 'which']\n",
    "    text = text.strip().lower()\n",
    "    return text.endswith('?') or any(text.startswith(q) for q in question_starters)\n",
    "\n",
    "# === Load and Parse Page ===\n",
    "driver.get(course_link)\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# === Extract Course Name ===\n",
    "course_name_tag = soup.find('h1', class_='productTopHeading')\n",
    "course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Unknown Course Name\"\n",
    "\n",
    "# === Extract Language ===\n",
    "language = \"Unknown\"\n",
    "lang_spans = soup.find_all('span', class_='divInfo')\n",
    "for span in lang_spans:\n",
    "    if 'fa-globe' in str(span):\n",
    "        language = span.get_text(strip=True)\n",
    "        break\n",
    "\n",
    "# === Extract Instructor ===\n",
    "faculty = \"Unknown\"\n",
    "instructor_block = soup.find('b', string=\"Instructor:\")\n",
    "if instructor_block and instructor_block.next_sibling:\n",
    "    faculty_tag = instructor_block.find_next('a')\n",
    "    if faculty_tag:\n",
    "        faculty = faculty_tag.get_text(strip=True)\n",
    "\n",
    "# === Extract Price ===\n",
    "price_tag = soup.find('span', id='mnumber')\n",
    "price = price_tag.get_text(strip=True) if price_tag else \"Unknown Price\"\n",
    "\n",
    "# === Extract About Course ===\n",
    "about_course = \"Not Available\"\n",
    "about_div = soup.find('div', class_='viewMoreDescription')\n",
    "if about_div:\n",
    "    about_course = '\\n'.join(p.get_text(strip=True) for p in about_div.find_all('p') if p.get_text(strip=True))\n",
    "\n",
    "# === Extract Syllabus ===\n",
    "# === Extract All Course Titles ===\n",
    "course_titles = []\n",
    "for btn in soup.find_all('button', class_='edHead'):\n",
    "    title_tag = btn.find('span', class_='edCardTitle')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        if title:\n",
    "            course_titles.append(title)\n",
    "\n",
    "# === Extract All Syllabus Content ===\n",
    "syllabus_blocks = soup.find_all(['div', 'span', 'h4'])\n",
    "\n",
    "syllabus_content = \"\"\n",
    "current_course_index = -1\n",
    "\n",
    "for block in syllabus_blocks:\n",
    "    if block.name == 'span' and 'edCardTitle' in block.get('class', []):\n",
    "        current_course_index += 1\n",
    "        syllabus_content += f\"\\nüéì {course_titles[current_course_index]}\\n\"\n",
    "\n",
    "    elif block.name == 'h4' and 'curriculumName' in block.get('class', []):\n",
    "        module_title_elem = block\n",
    "        module_num_elem = block.find_previous('div', class_='moduleHeading')\n",
    "        module_title = module_title_elem.get_text(strip=True)\n",
    "        module_num = module_num_elem.get_text(strip=True) if module_num_elem else ''\n",
    "        full_module_title = f\"{module_num} {module_title}\".strip()\n",
    "        syllabus_content += f\"\\nüî∏ {full_module_title}\\n\"\n",
    "\n",
    "    elif block.name == 'span' and 'secInfo' in block.get('class', []):\n",
    "        b_tag = block.find('b')\n",
    "        if b_tag:\n",
    "            title = b_tag.get_text(strip=True)\n",
    "            if 'frequently asked' in title.lower() or 'faq' in title.lower() or is_question(title):\n",
    "                continue\n",
    "            syllabus_content += f\"\\nüìò {title.strip()}\\n\"\n",
    "\n",
    "    elif block.name == 'div' and 'syllbusMain' in block.get('class', []):\n",
    "        desc_div = block.find('div', class_='syllbusDesc')\n",
    "        if desc_div:\n",
    "            for btn in desc_div.find_all('button'):\n",
    "                btn.decompose()\n",
    "            lesson_text = desc_div.get_text(strip=True)\n",
    "            if lesson_text:\n",
    "                syllabus_content += f\"- {lesson_text.strip()}\\n\"\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# === Final Output ===\n",
    "if not syllabus_content.strip():\n",
    "    syllabus_content = \"No Syllabus Found\"\n",
    "\n",
    "# === Print Results ===\n",
    "print(f\"üìó Course Name: {course_name}\")\n",
    "print(f\"üåê Language: {language}\")\n",
    "print(f\"üë®‚Äçüè´ Course Faculty: {faculty}\")\n",
    "print(f\"üí∞ Price: {price}\")\n",
    "print(f\"\\nüìù About Course:\\n{about_course}\\n\")\n",
    "print(syllabus_content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd12630",
   "metadata": {},
   "source": [
    "EXTRACTED  FOR ALL COURSES WEB SCRAPPER  # IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a89b7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping (1/9): https://www.eduonix.com/gpt-tools-for-marketing-edegree\n",
      "Scraping (2/9): https://www.eduonix.com/management-skills-team-leadership-skills-masterclass-2019\n",
      "Scraping (3/9): https://www.eduonix.com/gpt-tools-for-marketing-edegree\n",
      "Scraping (4/9): https://www.eduonix.com/nail-node-js-amp-javascript-job-interviews \n",
      "Scraping (5/9): https://www.eduonix.com/complete-java-9-masterclass-beginner-to-expert\n",
      "Scraping (6/9): https://www.eduonix.com/lifetime-ethical-hacking-and-cyber-security-membership\n",
      "Scraping (7/9): https://www.eduonix.com/lifetime-data-science-machine-learning-membership\n",
      "Scraping (8/9): https://www.eduonix.com/mastering-modern-workplace-essentials\n",
      "Scraping (9/9): https://www.eduonix.com/lifetime-web-development-membership\n",
      "‚úÖ All courses saved to: C:\\Users\\taslim.siddiqui\\Downloads\\eduonix_scraped_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# === Input Excel file ===\n",
    "input_excel_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\eduonix_link2.xlsx\"\n",
    "df_links = pd.read_excel(input_excel_path)\n",
    "course_links = df_links['Course Link'].dropna().tolist()\n",
    "\n",
    "# === Setup Chrome ===\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# === Utility ===\n",
    "def is_question(text):\n",
    "    question_starters = ['why', 'how', 'what', 'when', 'can', 'do', 'does', 'is', 'are', 'which']\n",
    "    text = text.strip().lower()\n",
    "    return text.endswith('?') or any(text.startswith(q) for q in question_starters)\n",
    "\n",
    "# === Scrape each course ===\n",
    "all_data = []\n",
    "\n",
    "for i, link in enumerate(course_links):\n",
    "    print(f\"Scraping ({i+1}/{len(course_links)}): {link}\")\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # --- Course Name ---\n",
    "        course_name_tag = soup.find('h1', class_='productTopHeading')\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Unknown Course Name\"\n",
    "\n",
    "        # --- Language ---\n",
    "        language = \"Not found\"\n",
    "        lang_spans = soup.find_all('span', class_='divInfo')\n",
    "        for span in lang_spans:\n",
    "            if 'fa-globe' in str(span):\n",
    "                language = span.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "        # --- Course Faculty ---\n",
    "        faculty = \"Not found\"\n",
    "        instructor_tag = soup.find('a', href=lambda x: x and '/i/' in x)\n",
    "        if instructor_tag:\n",
    "            faculty = instructor_tag.get_text(strip=True)\n",
    "\n",
    "        # --- Price ---\n",
    "        price = \"Not found\"\n",
    "        price_tag = soup.find('span', id='mnumber')\n",
    "        if price_tag:\n",
    "            price_text = price_tag.get_text(strip=True)\n",
    "            price = price_text.replace('\\xa0', ' ').replace('‚Çπ', ' ').strip()\n",
    "        else:\n",
    "            strike_tag = soup.find('strike', class_='original_price')\n",
    "            if strike_tag:\n",
    "                price_text = strike_tag.get_text(strip=True)\n",
    "                price = price_text.replace('\\xa0', ' ').replace('‚Çπ', ' ').strip()\n",
    "            else:\n",
    "                cut_price_tag = soup.find('div', class_='cutPrice')\n",
    "                if cut_price_tag:\n",
    "                    price_text = cut_price_tag.get_text(strip=True)\n",
    "                    price = price_text.replace('\\xa0', ' ').replace('‚Çπ', '').strip()\n",
    "\n",
    "        # --- Certificate ---\n",
    "        certificate = \"No\"\n",
    "        certificate_div = soup.find('div', class_='courseCertificate')\n",
    "        if certificate_div:\n",
    "            certificate = \"Yes\"\n",
    "\n",
    "        # --- About Course ---\n",
    "        about_course = \"Not Available\"\n",
    "        about_div = soup.find('div', class_='viewMoreDescription')\n",
    "        if about_div:\n",
    "            about_course = '\\n'.join(\n",
    "                p.get_text(strip=True) for p in about_div.find_all('p') if p.get_text(strip=True)\n",
    "            )\n",
    "\n",
    "        # --- Course Syllabus ---\n",
    "        course_titles = []\n",
    "        for btn in soup.find_all('button', class_='edHead'):\n",
    "            title_tag = btn.find('span', class_='edCardTitle')\n",
    "            if title_tag:\n",
    "                title = title_tag.get_text(strip=True)\n",
    "                if title:\n",
    "                    course_titles.append(title)\n",
    "\n",
    "        syllabus_blocks = soup.find_all(['div', 'span', 'h4'])\n",
    "\n",
    "        syllabus_content = \"\"\n",
    "        current_course_index = -1\n",
    "\n",
    "        for block in syllabus_blocks:\n",
    "            if block.name == 'span' and 'edCardTitle' in block.get('class', []):\n",
    "                current_course_index += 1\n",
    "                if current_course_index < len(course_titles):\n",
    "                    syllabus_content += f\"\\nüéì {course_titles[current_course_index]}\\n\"\n",
    "\n",
    "            elif block.name == 'h4' and 'curriculumName' in block.get('class', []):\n",
    "                module_title_elem = block\n",
    "                module_num_elem = block.find_previous('div', class_='moduleHeading')\n",
    "                module_title = module_title_elem.get_text(strip=True)\n",
    "                module_num = module_num_elem.get_text(strip=True) if module_num_elem else ''\n",
    "                full_module_title = f\"{module_num} {module_title}\".strip()\n",
    "                syllabus_content += f\"\\nüî∏ {full_module_title}\\n\"\n",
    "\n",
    "            elif block.name == 'span' and 'secInfo' in block.get('class', []):\n",
    "                b_tag = block.find('b')\n",
    "                if b_tag:\n",
    "                    title = b_tag.get_text(strip=True)\n",
    "                    if 'frequently asked' in title.lower() or 'faq' in title.lower() or is_question(title):\n",
    "                        continue\n",
    "                    syllabus_content += f\"\\nüìò {title.strip()}\\n\"\n",
    "\n",
    "            elif block.name == 'div' and 'syllbusMain' in block.get('class', []):\n",
    "                desc_div = block.find('div', class_='syllbusDesc')\n",
    "                if desc_div:\n",
    "                    for btn in desc_div.find_all('button'):\n",
    "                        btn.decompose()\n",
    "                    lesson_text = desc_div.get_text(strip=True)\n",
    "                    if lesson_text:\n",
    "                        syllabus_content += f\"- {lesson_text.strip()}\\n\"\n",
    "\n",
    "        # === Save course data ===\n",
    "        course_data = {\n",
    "            'Course Name': course_name,\n",
    "            'Course Link': link,\n",
    "            'Language': language,\n",
    "            'Course Faculty': faculty,\n",
    "            'Price': price,\n",
    "            'Certificate': certificate,\n",
    "            'About Course': about_course,\n",
    "            'Course Syllabus': syllabus_content.strip()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {link}: {e}\")\n",
    "        course_data = {\n",
    "            'Course Name': '',\n",
    "            'Course Link': link,\n",
    "            'Language': '',\n",
    "            'Course Faculty': '',\n",
    "            'Price': '',\n",
    "            'Certificate': '',\n",
    "            'About Course': '',\n",
    "            'Course Syllabus': ''\n",
    "        }\n",
    "\n",
    "    all_data.append(course_data)\n",
    "    time.sleep(1)\n",
    "\n",
    "# === Save to Excel ===\n",
    "driver.quit()\n",
    "df_out = pd.DataFrame(all_data)\n",
    "output_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\eduonix_scraped_output.xlsx\"\n",
    "df_out.to_excel(output_path, index=False)\n",
    "print(\"‚úÖ All courses saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1c005",
   "metadata": {},
   "source": [
    "Extracted course syllbaus for one link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "course_link = \"https://www.eduonix.com/lifetime-learning-access?coupon_code=LF199IND\"\n",
    "# Set up headless browser\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Helper to detect question-style section headers (FAQs\n",
    "def is_question(text):\n",
    "    question_starters = ['why', 'how', 'what', 'when', 'can', 'do', 'does', 'is', 'are', 'which']\n",
    "    text = text.strip().lower()\n",
    "    return text.endswith('?') or any(text.startswith(q) for q in question_starters)\n",
    "\n",
    "# Load the page\n",
    "driver.get(course_link)\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# === Extract All Course Titles ===\n",
    "course_titles = []\n",
    "for btn in soup.find_all('button', class_='edHead'):\n",
    "    title_tag = btn.find('span', class_='edCardTitle')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        if title:\n",
    "            course_titles.append(title)\n",
    "\n",
    "# === Extract All Syllabus Content ===\n",
    "syllabus_blocks = soup.find_all(['div', 'span', 'h4'])\n",
    "\n",
    "syllabus_content = \"\"\n",
    "current_course_index = -1\n",
    "\n",
    "for block in syllabus_blocks:\n",
    "    if block.name == 'span' and 'edCardTitle' in block.get('class', []):\n",
    "        current_course_index += 1\n",
    "        syllabus_content += f\"\\nüéì {course_titles[current_course_index]}\\n\"\n",
    "\n",
    "    elif block.name == 'h4' and 'curriculumName' in block.get('class', []):\n",
    "        module_title_elem = block\n",
    "        module_num_elem = block.find_previous('div', class_='moduleHeading')\n",
    "        module_title = module_title_elem.get_text(strip=True)\n",
    "        module_num = module_num_elem.get_text(strip=True) if module_num_elem else ''\n",
    "        full_module_title = f\"{module_num} {module_title}\".strip()\n",
    "        syllabus_content += f\"\\nüî∏ {full_module_title}\\n\"\n",
    "\n",
    "    elif block.name == 'span' and 'secInfo' in block.get('class', []):\n",
    "        b_tag = block.find('b')\n",
    "        if b_tag:\n",
    "            title = b_tag.get_text(strip=True)\n",
    "            if 'frequently asked' in title.lower() or 'faq' in title.lower() or is_question(title):\n",
    "                continue\n",
    "            syllabus_content += f\"\\nüìò {title.strip()}\\n\"\n",
    "\n",
    "    elif block.name == 'div' and 'syllbusMain' in block.get('class', []):\n",
    "        desc_div = block.find('div', class_='syllbusDesc')\n",
    "        if desc_div:\n",
    "            for btn in desc_div.find_all('button'):\n",
    "                btn.decompose()\n",
    "            lesson_text = desc_div.get_text(strip=True)\n",
    "            if lesson_text:\n",
    "                syllabus_content += f\"- {lesson_text.strip()}\\n\"\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Save combined syllabus + link\n",
    "all_data = [{\n",
    "    'Course Link': course_link,\n",
    "    'Syllabus': syllabus_content.strip()\n",
    "}]\n",
    "\n",
    "# Write to Excel\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_excel(\"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\eduonix_syllabus_output.xlsx\", index=False)\n",
    "\n",
    "# Optional: print result to console\n",
    "for entry in all_data:\n",
    "    print(\"Course Link:\", entry['Course Link'])\n",
    "    print(entry['Syllabus'])\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03405888",
   "metadata": {},
   "source": [
    "Extracted course syllabus only  for all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bf8b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Scraping (1/3): https://www.eduonix.com/lifetime-ethical-hacking-and-cyber-security-membership\n",
      "‚è≥ Scraping (2/3): https://www.eduonix.com/lifetime-data-science-machine-learning-membership\n",
      "‚è≥ Scraping (3/3): https://www.eduonix.com/lifetime-learning-access?coupon_code=LF199IND\n",
      "\n",
      "‚úÖ Scraping complete. Output saved to: C:\\Users\\taslim.siddiqui\\Downloads\\eduonix_syllabus_output.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load Excel with course links\n",
    "input_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\eduonix_link1.xlsx\"\n",
    "df_links = pd.read_excel(input_path)\n",
    "\n",
    "# Validate column\n",
    "if 'Course Link' not in df_links.columns:\n",
    "    raise ValueError(\"Excel must contain a 'Course Link' column.\")\n",
    "\n",
    "# Headless browser setup\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Detect FAQ-like or question-style content\n",
    "def is_question(text):\n",
    "    question_starters = ['why', 'how', 'what', 'when', 'can', 'do', 'does', 'is', 'are', 'which']\n",
    "    text = text.strip().lower()\n",
    "    return text.endswith('?') or any(text.startswith(q) for q in question_starters)\n",
    "\n",
    "# Loop over all links\n",
    "all_data = []\n",
    "\n",
    "for idx, row in df_links.iterrows():\n",
    "    course_link = row['Course Link']\n",
    "    print(f\"‚è≥ Scraping ({idx + 1}/{len(df_links)}): {course_link}\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    try:\n",
    "        driver.get(course_link)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        syllabus_content = \"\"\n",
    "        course_titles = [span.get_text(strip=True) for span in soup.find_all('span', class_='edCardTitle')]\n",
    "\n",
    "        current_course_index = -1\n",
    "        blocks = soup.find_all(['div', 'span', 'h4'])\n",
    "\n",
    "        for block in blocks:\n",
    "            # New Course Detected\n",
    "            if block.name == 'span' and 'edCardTitle' in block.get('class', []):\n",
    "                current_course_index += 1\n",
    "                if current_course_index < len(course_titles):\n",
    "                    syllabus_content += f\"\\nüéì {course_titles[current_course_index]}\\n\"\n",
    "\n",
    "            # Module title\n",
    "            elif block.name == 'h4' and 'curriculumName' in block.get('class', []):\n",
    "                module_title = block.get_text(strip=True)\n",
    "                module_num_elem = block.find_previous('div', class_='moduleHeading')\n",
    "                module_num = module_num_elem.get_text(strip=True) if module_num_elem else ''\n",
    "                full_module_title = f\"{module_num} {module_title}\".strip()\n",
    "                syllabus_content += f\"\\nüî∏ {full_module_title}\\n\"\n",
    "\n",
    "            # Section title\n",
    "            elif block.name == 'span' and 'secInfo' in block.get('class', []):\n",
    "                b_tag = block.find('b')\n",
    "                if b_tag:\n",
    "                    title = b_tag.get_text(strip=True)\n",
    "                    if 'frequently asked' in title.lower() or 'faq' in title.lower() or is_question(title):\n",
    "                        continue\n",
    "                    syllabus_content += f\"\\nüìò {title}\\n\"\n",
    "\n",
    "            # Lesson content\n",
    "            elif block.name == 'div' and 'syllbusMain' in block.get('class', []):\n",
    "                desc_div = block.find('div', class_='syllbusDesc')\n",
    "                if desc_div:\n",
    "                    for btn in desc_div.find_all('button'):\n",
    "                        btn.decompose()\n",
    "                    lesson_text = desc_div.get_text(strip=True)\n",
    "                    if lesson_text:\n",
    "                        syllabus_content += f\"- {lesson_text}\\n\"\n",
    "\n",
    "        all_data.append({\n",
    "            'Course Link': course_link,\n",
    "            'Syllabus': syllabus_content.strip()\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {course_link}: {e}\")\n",
    "        all_data.append({'Course Link': course_link, 'Syllabus': ''})\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Save output to Excel\n",
    "output_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\eduonix_syllabus_output.xlsx\"\n",
    "pd.DataFrame(all_data).to_excel(output_path, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Scraping complete. Output saved to:\", output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
