{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_driver():\n",
    "    \"\"\"Initialize Chrome driver\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    # options.add_argument(\"--headless\")  # Remove for debugging\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "def find_all_courses_comprehensive():\n",
    "    \"\"\"\n",
    "    Find ALL courses from the entire website by exploring every possible page\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EXTRACTING ALL COURSES FROM ENTIRE WEBSITE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    driver = None\n",
    "    all_courses = []\n",
    "    base_url = \"https://www.smartonlinecourse.co.in\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize driver\n",
    "        print(\"üöÄ Initializing Chrome browser...\")\n",
    "        driver = get_driver()\n",
    "        \n",
    "        # STEP 1: Start from main store and find all navigation links\n",
    "        print(\"\\nüìå STEP 1: Exploring main navigation...\")\n",
    "        main_store_url = f\"{base_url}/s/store\"\n",
    "        print(f\"üåê Visiting: {main_store_url}\")\n",
    "        driver.get(main_store_url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Take screenshot for debugging\n",
    "        driver.save_screenshot(\"main_store.png\")\n",
    "        print(\"üì∏ Screenshot saved: main_store.png\")\n",
    "        \n",
    "        # Get all links on the main page\n",
    "        all_links = get_all_links_on_page(driver, base_url)\n",
    "        print(f\"   Found {len(all_links)} total links on main page\")\n",
    "        \n",
    "        # STEP 2: Visit all category pages\n",
    "        print(\"\\nüìå STEP 2: Visiting all category pages...\")\n",
    "        \n",
    "        # Extract category links\n",
    "        category_links = []\n",
    "        for link_info in all_links:\n",
    "            url = link_info['url']\n",
    "            text = link_info['text'].lower()\n",
    "            \n",
    "            # Look for category indicators\n",
    "            category_keywords = ['risk', 'insurance', 'skill', 'mock', 'banking', \n",
    "                                'legal', 'technology', 'life', 'magazine', 'productivity']\n",
    "            if any(keyword in text for keyword in category_keywords) and len(text) > 2:\n",
    "                category_links.append(url)\n",
    "        \n",
    "        # Visit each category link\n",
    "        category_links = list(set(category_links))  # Remove duplicates\n",
    "        print(f\"   Found {len(category_links)} category pages to explore\")\n",
    "        \n",
    "        for i, category_url in enumerate(category_links, 1):\n",
    "            print(f\"\\n   {i}/{len(category_links)}: Visiting category: {category_url}\")\n",
    "            \n",
    "            try:\n",
    "                driver.get(category_url)\n",
    "                time.sleep(4)\n",
    "                \n",
    "                # Scroll to load all content\n",
    "                scroll_to_load_all(driver)\n",
    "                \n",
    "                # Extract courses from this category\n",
    "                courses = extract_courses_from_current_page(driver, base_url)\n",
    "                print(f\"     Found {len(courses)} courses on this page\")\n",
    "                \n",
    "                all_courses.extend(courses)\n",
    "                \n",
    "                # Try to find and follow pagination\n",
    "                more_courses = handle_pagination(driver, base_url)\n",
    "                all_courses.extend(more_courses)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"     Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # STEP 3: Try to find course listing pages\n",
    "        print(\"\\nüìå STEP 3: Searching for course listing pages...\")\n",
    "        \n",
    "        # Common course listing patterns\n",
    "        listing_patterns = [\n",
    "            \"courses\", \"all-courses\", \"course-list\", \"online-courses\", \n",
    "            \"programs\", \"learn\", \"training\", \"certificate\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in listing_patterns:\n",
    "            listing_url = f\"{base_url}/{pattern}\"\n",
    "            print(f\"\\n   Checking: {listing_url}\")\n",
    "            \n",
    "            try:\n",
    "                driver.get(listing_url)\n",
    "                time.sleep(4)\n",
    "                \n",
    "                # Check if page exists and has content\n",
    "                if \"404\" not in driver.title.lower() and \"not found\" not in driver.page_source.lower():\n",
    "                    scroll_to_load_all(driver)\n",
    "                    \n",
    "                    courses = extract_courses_from_current_page(driver, base_url)\n",
    "                    print(f\"     Found {len(courses)} courses\")\n",
    "                    \n",
    "                    all_courses.extend(courses)\n",
    "                    \n",
    "                    # Handle pagination\n",
    "                    more_courses = handle_pagination(driver, base_url)\n",
    "                    all_courses.extend(more_courses)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"     Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # STEP 4: Try to discover via sitemap or robots.txt\n",
    "        print(\"\\nüìå STEP 4: Checking for sitemap...\")\n",
    "        \n",
    "        sitemap_urls = [\n",
    "            f\"{base_url}/sitemap.xml\",\n",
    "            f\"{base_url}/sitemap_index.xml\",\n",
    "            f\"{base_url}/wp-sitemap.xml\",\n",
    "        ]\n",
    "        \n",
    "        for sitemap_url in sitemap_urls:\n",
    "            try:\n",
    "                print(f\"\\n   Checking sitemap: {sitemap_url}\")\n",
    "                driver.get(sitemap_url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Parse XML content\n",
    "                page_source = driver.page_source\n",
    "                if 'xml' in page_source:\n",
    "                    soup = BeautifulSoup(page_source, 'xml')\n",
    "                    urls = soup.find_all('loc')\n",
    "                    \n",
    "                    course_urls = []\n",
    "                    for url_tag in urls:\n",
    "                        url = url_tag.get_text()\n",
    "                        if '/courses/' in url and re.search(r'-[a-f0-9]{24}$', url):\n",
    "                            course_urls.append(url)\n",
    "                    \n",
    "                    print(f\"     Found {len(course_urls)} course URLs in sitemap\")\n",
    "                    \n",
    "                    for course_url in course_urls:\n",
    "                        course_name = extract_course_name_from_url(course_url)\n",
    "                        all_courses.append({\n",
    "                            'url': course_url,\n",
    "                            'course_name': course_name\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"     Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # STEP 5: Remove duplicates and process results\n",
    "        print(\"\\nüìå STEP 5: Processing results...\")\n",
    "        \n",
    "        unique_courses = remove_duplicates(all_courses)\n",
    "        print(f\"‚úÖ Total unique courses found: {len(unique_courses)}\")\n",
    "        \n",
    "        # STEP 6: Categorize and save\n",
    "        if unique_courses:\n",
    "            categorized_courses = categorize_courses(unique_courses)\n",
    "            save_all_courses_to_excel(categorized_courses)\n",
    "            return categorized_courses\n",
    "        else:\n",
    "            print(\"\\n‚ùå No courses found\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüí• Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "def get_all_links_on_page(driver, base_url):\n",
    "    \"\"\"Get all links on current page\"\"\"\n",
    "    links = []\n",
    "    \n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Find all anchor tags\n",
    "    anchor_tags = soup.find_all('a', href=True)\n",
    "    \n",
    "    for tag in anchor_tags:\n",
    "        href = tag.get('href', '')\n",
    "        text = tag.get_text(strip=True)\n",
    "        \n",
    "        if href:\n",
    "            # Make full URL\n",
    "            if href.startswith('/'):\n",
    "                full_url = urljoin(base_url, href)\n",
    "            elif href.startswith('http'):\n",
    "                full_url = href\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Clean URL\n",
    "            clean_url = full_url.split('?')[0].split('#')[0].rstrip('/')\n",
    "            \n",
    "            links.append({\n",
    "                'url': clean_url,\n",
    "                'text': text\n",
    "            })\n",
    "    \n",
    "    return links\n",
    "\n",
    "def scroll_to_load_all(driver):\n",
    "    \"\"\"Scroll to load all lazy-loaded content\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "    \n",
    "    while scroll_attempts < 5:\n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Check for \"Load More\" buttons\n",
    "        try:\n",
    "            buttons = driver.find_elements(By.XPATH, \n",
    "                \"//button[contains(., 'Load') or contains(., 'More') or contains(., 'Show')]\")\n",
    "            for button in buttons:\n",
    "                try:\n",
    "                    if button.is_displayed():\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n",
    "                        time.sleep(1)\n",
    "                        button.click()\n",
    "                        print(\"       Clicked 'Load More' button\")\n",
    "                        time.sleep(3)\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Calculate new scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0\n",
    "            last_height = new_height\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "def extract_courses_from_current_page(driver, base_url):\n",
    "    \"\"\"Extract all course links from current page\"\"\"\n",
    "    courses = []\n",
    "    \n",
    "    # Get page source\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # METHOD 1: Search for course URL patterns\n",
    "    patterns = [\n",
    "        r'https?://[^\"\\']+/courses/[^\"\\']+?-[a-f0-9]{24}[^\"\\']*',\n",
    "        r'/courses/[^\"\\']+?-[a-f0-9]{24}',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, page_source, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                match = match[0]\n",
    "            \n",
    "            # Construct full URL\n",
    "            if match.startswith('/'):\n",
    "                full_url = urljoin(base_url, match)\n",
    "            elif match.startswith('http'):\n",
    "                full_url = match\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Clean URL\n",
    "            clean_url = full_url.split('?')[0].split('#')[0].rstrip('/')\n",
    "            \n",
    "            # Extract course name\n",
    "            course_name = extract_course_name_from_url(clean_url)\n",
    "            \n",
    "            # Add to list\n",
    "            courses.append({\n",
    "                'url': clean_url,\n",
    "                'course_name': course_name\n",
    "            })\n",
    "    \n",
    "    # METHOD 2: Also look for links in anchor tags\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    anchor_tags = soup.find_all('a', href=True)\n",
    "    \n",
    "    for tag in anchor_tags:\n",
    "        href = tag.get('href', '')\n",
    "        text = tag.get_text(strip=True)\n",
    "        \n",
    "        if href and '/courses/' in href:\n",
    "            # Check if it has the ID pattern\n",
    "            if re.search(r'-[a-f0-9]{24}$', href):\n",
    "                # Make full URL\n",
    "                if href.startswith('/'):\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                elif href.startswith('http'):\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                clean_url = full_url.split('?')[0].split('#')[0].rstrip('/')\n",
    "                \n",
    "                # Use link text if available, otherwise extract from URL\n",
    "                if text and len(text) > 5:\n",
    "                    course_name = text\n",
    "                else:\n",
    "                    course_name = extract_course_name_from_url(clean_url)\n",
    "                \n",
    "                courses.append({\n",
    "                    'url': clean_url,\n",
    "                    'course_name': course_name\n",
    "                })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "def handle_pagination(driver, base_url):\n",
    "    \"\"\"Handle pagination if exists\"\"\"\n",
    "    more_courses = []\n",
    "    \n",
    "    try:\n",
    "        # Look for pagination links\n",
    "        pagination_links = driver.find_elements(By.XPATH, \n",
    "            \"//a[contains(@href, 'page=') or contains(@href, 'p=') or contains(text(), '2') or contains(text(), 'Next')]\")\n",
    "        \n",
    "        if pagination_links:\n",
    "            print(\"     Found pagination, checking next pages...\")\n",
    "            \n",
    "            # Try to visit next few pages\n",
    "            for i in range(2, 6):  # Check pages 2-5\n",
    "                try:\n",
    "                    # Construct page URL\n",
    "                    current_url = driver.current_url\n",
    "                    if '?' in current_url:\n",
    "                        page_url = f\"{current_url}&page={i}\"\n",
    "                    else:\n",
    "                        page_url = f\"{current_url}?page={i}\"\n",
    "                    \n",
    "                    print(f\"       Checking page {i}: {page_url}\")\n",
    "                    driver.get(page_url)\n",
    "                    time.sleep(4)\n",
    "                    \n",
    "                    # Scroll to load\n",
    "                    scroll_to_load_all(driver)\n",
    "                    \n",
    "                    # Extract courses\n",
    "                    courses = extract_courses_from_current_page(driver, base_url)\n",
    "                    more_courses.extend(courses)\n",
    "                    \n",
    "                    print(f\"         Found {len(courses)} courses on page {i}\")\n",
    "                    \n",
    "                except:\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return more_courses\n",
    "\n",
    "def extract_course_name_from_url(url):\n",
    "    \"\"\"Extract course name from URL\"\"\"\n",
    "    try:\n",
    "        # Pattern: /courses/[course-name]-[24-char-id]\n",
    "        match = re.search(r'/courses/([a-zA-Z0-9\\-]+)-[a-f0-9]{24}', url)\n",
    "        if match:\n",
    "            name_part = match.group(1)\n",
    "            # Convert kebab-case to Title Case\n",
    "            name_part = name_part.replace('-', ' ')\n",
    "            name_part = ' '.join(word.capitalize() for word in name_part.split())\n",
    "            return name_part\n",
    "        \n",
    "        # Alternative pattern\n",
    "        match = re.search(r'/courses/(.+)', url)\n",
    "        if match:\n",
    "            name_part = match.group(1)\n",
    "            # Remove ID if present\n",
    "            name_part = re.sub(r'-[a-f0-9]{24}$', '', name_part)\n",
    "            name_part = name_part.replace('-', ' ').replace('%20', ' ')\n",
    "            name_part = ' '.join(word.capitalize() for word in name_part.split())\n",
    "            return name_part\n",
    "        \n",
    "        return url.split('/')[-1].replace('-', ' ').title()\n",
    "    except:\n",
    "        return \"Course\"\n",
    "\n",
    "def remove_duplicates(courses):\n",
    "    \"\"\"Remove duplicate courses\"\"\"\n",
    "    unique_courses = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    for course in courses:\n",
    "        clean_url = course['url'].split('?')[0].split('#')[0].rstrip('/')\n",
    "        if clean_url not in seen_urls:\n",
    "            seen_urls.add(clean_url)\n",
    "            unique_courses.append({\n",
    "                'url': clean_url,\n",
    "                'course_name': course['course_name']\n",
    "            })\n",
    "    \n",
    "    return unique_courses\n",
    "\n",
    "def categorize_courses(courses):\n",
    "    \"\"\"Categorize courses based on keywords\"\"\"\n",
    "    categories = {\n",
    "        'Risk Management': ['risk management', 'risk'],\n",
    "        'Insurance': ['insurance', 'insure', 'policy'],\n",
    "        'Legal and Regulatory Compliance': ['legal', 'law', 'regulation', 'compliance'],\n",
    "        'Life Insurance': ['life insurance'],\n",
    "        'Banking': ['banking', 'bank', 'finance'],\n",
    "        'MAGAZINE': ['magazine'],\n",
    "        'Mock Test': ['mock test', 'exam', 'quiz'],\n",
    "        'Skill Building and Productivity': ['skill', 'productivity', 'soft skills'],\n",
    "        'Technology': ['technology', 'tech', 'cyber', 'digital', 'ai', 'artificial intelligence'],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    categorized = []\n",
    "    \n",
    "    for course in courses:\n",
    "        search_text = (course['course_name'] + ' ' + course['url']).lower()\n",
    "        \n",
    "        assigned_category = 'Other'\n",
    "        for category, keywords in categories.items():\n",
    "            if category == 'Other':\n",
    "                continue\n",
    "            \n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in search_text:\n",
    "                    assigned_category = category\n",
    "                    break\n",
    "            \n",
    "            if assigned_category != 'Other':\n",
    "                break\n",
    "        \n",
    "        course['category'] = assigned_category\n",
    "        categorized.append(course)\n",
    "    \n",
    "    return categorized\n",
    "\n",
    "def save_all_courses_to_excel(courses):\n",
    "    \"\"\"Save all courses to Excel\"\"\"\n",
    "    # Prepare data\n",
    "    data = []\n",
    "    for i, course in enumerate(courses, 1):\n",
    "        data.append({\n",
    "            'S.No': i,\n",
    "            'Course Name': course['course_name'],\n",
    "            'URL': course['url'],\n",
    "            'Category': course['category'],\n",
    "            'Scraped_Date': datetime.now().strftime('%d-%m-%Y %H:%M')\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    output_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\all_courses_complete.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='All Courses', index=False)\n",
    "        \n",
    "        # Auto-adjust columns\n",
    "        worksheet = writer.sheets['All Courses']\n",
    "        for column in worksheet.columns:\n",
    "            max_length = 0\n",
    "            column_letter = column[0].column_letter\n",
    "            for cell in column:\n",
    "                try:\n",
    "                    if cell.value:\n",
    "                        max_length = max(max_length, len(str(cell.value)))\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = min(max_length + 2, 50)\n",
    "            worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        \n",
    "        # Bold header\n",
    "        for cell in worksheet[1]:\n",
    "            cell.font = cell.font.copy(bold=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved {len(df)} courses to: {output_path}\")\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    print(\"\\nüìä COMPREHENSIVE CATEGORY DISTRIBUTION:\")\n",
    "    category_counts = df['Category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"   {category}: {count} courses\")\n",
    "    \n",
    "    # Show sample from each category\n",
    "    print(\"\\nüìã SAMPLE FROM EACH CATEGORY:\")\n",
    "    for category in category_counts.index:\n",
    "        category_courses = df[df['Category'] == category].head(3)\n",
    "        print(f\"\\n   {category} (showing {len(category_courses)} of {category_counts[category]}):\")\n",
    "        for idx, row in category_courses.iterrows():\n",
    "            print(f\"     ‚Ä¢ {row['Course Name']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Advanced discovery using API/JSON endpoints\n",
    "def discover_courses_via_api():\n",
    "    \"\"\"Try to discover courses via API endpoints\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED DISCOVERY VIA API ENDPOINTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    base_url = \"https://www.smartonlinecourse.co.in\"\n",
    "    all_courses = []\n",
    "    \n",
    "    # Common API endpoints\n",
    "    api_endpoints = [\n",
    "        f\"{base_url}/api/courses\",\n",
    "        f\"{base_url}/api/v1/courses\",\n",
    "        f\"{base_url}/wp-json/wp/v2/courses\",\n",
    "        f\"{base_url}/graphql\",\n",
    "        f\"{base_url}/courses.json\",\n",
    "        f\"{base_url}/data/courses.json\",\n",
    "    ]\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    for endpoint in api_endpoints:\n",
    "        try:\n",
    "            print(f\"\\nüîç Checking API: {endpoint}\")\n",
    "            response = requests.get(endpoint, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    courses = extract_courses_from_json(data, base_url)\n",
    "                    print(f\"   Found {len(courses)} courses via API\")\n",
    "                    all_courses.extend(courses)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Try to parse as text/HTML\n",
    "                    courses = extract_courses_from_text(response.text, base_url)\n",
    "                    print(f\"   Found {len(courses)} courses in response text\")\n",
    "                    all_courses.extend(courses)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def extract_courses_from_json(data, base_url):\n",
    "    \"\"\"Extract courses from JSON data\"\"\"\n",
    "    courses = []\n",
    "    \n",
    "    def traverse(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            # Check for course data\n",
    "            if 'url' in obj and '/courses/' in str(obj['url']):\n",
    "                url = obj['url']\n",
    "                if isinstance(url, str) and re.search(r'-[a-f0-9]{24}$', url):\n",
    "                    course_name = obj.get('name') or obj.get('title') or obj.get('course_name', '')\n",
    "                    if not course_name:\n",
    "                        course_name = extract_course_name_from_url(url)\n",
    "                    \n",
    "                    courses.append({\n",
    "                        'url': url,\n",
    "                        'course_name': course_name\n",
    "                    })\n",
    "            \n",
    "            # Recursively traverse\n",
    "            for value in obj.values():\n",
    "                traverse(value)\n",
    "                \n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                traverse(item)\n",
    "    \n",
    "    traverse(data)\n",
    "    return courses\n",
    "\n",
    "def extract_courses_from_text(text, base_url):\n",
    "    \"\"\"Extract courses from text/HTML\"\"\"\n",
    "    courses = []\n",
    "    \n",
    "    # Search for course URL patterns\n",
    "    patterns = [\n",
    "        r'https?://[^\"\\']+/courses/[^\"\\']+?-[a-f0-9]{24}[^\"\\']*',\n",
    "        r'/courses/[^\"\\']+?-[a-f0-9]{24}',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                match = match[0]\n",
    "            \n",
    "            # Construct full URL\n",
    "            if match.startswith('/'):\n",
    "                full_url = urljoin(base_url, match)\n",
    "            elif match.startswith('http'):\n",
    "                full_url = match\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            clean_url = full_url.split('?')[0].split('#')[0].rstrip('/')\n",
    "            course_name = extract_course_name_from_url(clean_url)\n",
    "            \n",
    "            courses.append({\n",
    "                'url': clean_url,\n",
    "                'course_name': course_name\n",
    "            })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"SMART ONLINE COURSE - COMPLETE WEBSITE COURSE EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Finding ALL courses in format:\")\n",
    "    print(\"https://www.smartonlinecourse.co.in/courses/[Course-Name]-[24-char-ID]\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Method 1: Comprehensive Selenium exploration\n",
    "    print(\"\\nüéØ METHOD 1: Comprehensive website exploration...\")\n",
    "    courses = find_all_courses_comprehensive()\n",
    "    \n",
    "    # Method 2: API discovery\n",
    "    if len(courses) < 50:\n",
    "        print(\"\\nüéØ METHOD 2: API endpoint discovery...\")\n",
    "        api_courses = discover_courses_via_api()\n",
    "        \n",
    "        # Merge results\n",
    "        existing_urls = set(course['url'] for course in courses)\n",
    "        for course in api_courses:\n",
    "            if course['url'] not in existing_urls:\n",
    "                courses.append(course)\n",
    "        \n",
    "        print(f\"\\n‚ûï Added {len(api_courses)} courses from API discovery\")\n",
    "        \n",
    "        # Save final results\n",
    "        if courses:\n",
    "            categorized = categorize_courses(courses)\n",
    "            save_all_courses_to_excel(categorized)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if courses:\n",
    "        print(f\"üéâ EXTRACTION COMPLETE!\")\n",
    "        print(f\"üìä Total courses extracted: {len(courses)}\")\n",
    "        print(f\"üìÅ Output saved to: C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\all_courses_complete.xlsx\")\n",
    "    else:\n",
    "        print(\"‚ùå No courses found\")\n",
    "        print(\"\\nüí° The website might:\")\n",
    "        print(\"   1. Require login to see all courses\")\n",
    "        print(\"   2. Use complex JavaScript loading\")\n",
    "        print(\"   3. Have courses behind different URLs\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da50776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
