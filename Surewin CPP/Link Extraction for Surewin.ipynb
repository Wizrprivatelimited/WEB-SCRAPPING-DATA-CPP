{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7279b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1: Find all course/product links on SureWin India\n",
    "# ----------------------------------------------------\n",
    "def find_all_course_pages_surewin():\n",
    "    base_url = \"https://surewinindia.com\"\n",
    "    all_courses = set()\n",
    "\n",
    "    print(\"üîç Scanning SureWin India website for course pages...\")\n",
    "\n",
    "    # Test if website is accessible first\n",
    "    try:\n",
    "        test_response = requests.get(base_url, timeout=10)\n",
    "        if test_response.status_code != 200:\n",
    "            print(f\"‚ùå Website not accessible. Status code: {test_response.status_code}\")\n",
    "            return []\n",
    "        print(\"‚úÖ Website is accessible\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot connect to website: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Main category pages to start crawling\n",
    "    main_pages = [\n",
    "        base_url,\n",
    "        f\"{base_url}/shop/\",\n",
    "        f\"{base_url}/courses/\",\n",
    "        f\"{base_url}/products/\",\n",
    "        f\"{base_url}/online-courses/\",\n",
    "        f\"{base_url}/all-courses/\",\n",
    "        f\"{base_url}/programs/\",\n",
    "        f\"{base_url}/trainings/\",\n",
    "        f\"{base_url}/certification-courses/\",\n",
    "        f\"{base_url}/diploma-courses/\",\n",
    "        f\"{base_url}/professional-courses/\",\n",
    "        f\"{base_url}/it-courses/\",\n",
    "        f\"{base_url}/technical-courses/\",\n",
    "        f\"{base_url}/management-courses/\",\n",
    "        f\"{base_url}/skill-development-courses/\",\n",
    "        f\"{base_url}/office-management-courses/\",\n",
    "        f\"{base_url}/teacher-training-courses/\",\n",
    "        f\"{base_url}/job-oriented-courses/\",\n",
    "        f\"{base_url}/safety-management-courses/\",\n",
    "        f\"{base_url}/advance-diploma/\",\n",
    "        f\"{base_url}/vocation-courses/\",\n",
    "        f\"{base_url}/yoga-courses/\",\n",
    "        f\"{base_url}/certificate-courses/\",\n",
    "        f\"{base_url}/industrial-courses/\",\n",
    "        f\"{base_url}/hobby-courses/\",\n",
    "        f\"{base_url}/edp-courses/\",\n",
    "        f\"{base_url}/language-courses/\",\n",
    "    ]\n",
    "\n",
    "    visited_pages = set()\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "\n",
    "    def scan_page(url):\n",
    "        if url in visited_pages:\n",
    "            return\n",
    "        visited_pages.add(url)\n",
    "\n",
    "        try:\n",
    "            print(f\"üîç Scanning: {url}\")\n",
    "            response = session.get(url, timeout=15)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                all_links = soup.find_all('a', href=True)\n",
    "                print(f\"   Found {len(all_links)} total links on page\")\n",
    "\n",
    "                course_links = []\n",
    "                for link in all_links:\n",
    "                    href = link.get('href', '').strip()\n",
    "\n",
    "                    is_course_link = (\n",
    "                        ('/product/' in href or '/course/' in href or '/program/' in href)\n",
    "                        and 'surewinindia.com' in href\n",
    "                    )\n",
    "\n",
    "                    if is_course_link:\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        clean_url = full_url.split('?')[0].split('#')[0]\n",
    "\n",
    "                        if clean_url not in all_courses:\n",
    "                            all_courses.add(clean_url)\n",
    "                            course_links.append(clean_url)\n",
    "                            print(f\"   ‚úÖ Found course: {clean_url}\")\n",
    "\n",
    "                print(f\"   üìä Total course links found on this page: {len(course_links)}\")\n",
    "\n",
    "                # Scan pagination links\n",
    "                pagination_selectors = ['.page-numbers a', '.pagination a', 'a.next', 'a.prev', '.nav-links a', 'a.page-numbers']\n",
    "                for selector in pagination_selectors:\n",
    "                    for page_link in soup.select(selector):\n",
    "                        page_href = page_link.get('href', '')\n",
    "                        if page_href and 'surewinindia.com' in page_href:\n",
    "                            scan_page(page_href)\n",
    "\n",
    "                # Scan category links\n",
    "                category_selectors = ['a[href*=\"category\"]', 'a[href*=\"shop\"]', 'a[href*=\"courses\"]']\n",
    "                for selector in category_selectors:\n",
    "                    for cat_link in soup.select(selector):\n",
    "                        cat_href = cat_link.get('href', '')\n",
    "                        if cat_href and 'surewinindia.com' in cat_href:\n",
    "                            scan_page(cat_href)\n",
    "\n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed to load: HTTP {response.status_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error scanning {url}: {e}\")\n",
    "\n",
    "    successful_scans = 0\n",
    "    for page in main_pages:\n",
    "        try:\n",
    "            scan_page(page)\n",
    "            successful_scans += 1\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error with main page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nüìä Scan Summary: {successful_scans}/{len(main_pages)} pages scanned successfully\")\n",
    "    print(f\"üìä Total course URLs found: {len(all_courses)}\")\n",
    "\n",
    "    return list(all_courses)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2: Extract details from each course page\n",
    "# ----------------------------------------------------\n",
    "def get_course_details_surewin(course_urls, output_file):\n",
    "    if not course_urls:\n",
    "        print(\"‚ùå No course URLs to process\")\n",
    "        return []\n",
    "\n",
    "    # Load existing progress if file exists\n",
    "    existing_courses = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_excel(output_file)\n",
    "            existing_courses = existing_df.to_dict('records')\n",
    "            print(f\"üìÅ Loaded {len(existing_courses)} existing courses from {output_file}\")\n",
    "        except:\n",
    "            print(\"üìÅ Starting fresh - no existing file\")\n",
    "\n",
    "    processed_urls = set(course['course_link'] for course in existing_courses)\n",
    "    urls_to_process = [url for url in course_urls if url not in processed_urls]\n",
    "\n",
    "    print(f\"üîÑ Processing {len(urls_to_process)} new URLs\")\n",
    "\n",
    "    # Save backup of all input URLs\n",
    "    with open('surewin_course_urls_backup.txt', 'w') as f:\n",
    "        for url in course_urls:\n",
    "            f.write(url + '\\n')\n",
    "    print(\"üíæ URLs backup saved to 'surewin_course_urls_backup.txt'\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "\n",
    "    all_courses = existing_courses.copy()\n",
    "\n",
    "    for i, url in enumerate(urls_to_process, 1):\n",
    "        max_retries = 3\n",
    "        course_data = None\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"üìñ Fetching: {url}\")\n",
    "                response = session.get(url, timeout=20)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    # Try multiple selectors for title\n",
    "                    title_selectors = [\n",
    "                        'h1.product_title', '.product_title',\n",
    "                        'h1.entry-title', '.entry-title',\n",
    "                        'h1.title', 'h1',\n",
    "                        '.product-title', '.course-title',\n",
    "                        'title'\n",
    "                    ]\n",
    "\n",
    "                    course_name = None\n",
    "                    for selector in title_selectors:\n",
    "                        el = soup.select_one(selector)\n",
    "                        if el:\n",
    "                            course_name = el.get_text(strip=True)\n",
    "                            if course_name and len(course_name) > 3:\n",
    "                                break\n",
    "\n",
    "                    if not course_name:\n",
    "                        course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                        course_name = re.sub(r'\\s+', ' ', course_name).strip()\n",
    "\n",
    "                    course_data = {\n",
    "                        'course_name': course_name,\n",
    "                        'course_link': url\n",
    "                    }\n",
    "\n",
    "                    print(f\"‚úÖ {i}/{len(urls_to_process)}: {course_name}\")\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  HTTP {response.status_code} for {url}\")\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è∞ Timeout (attempt {attempt + 1}/{max_retries}) for {url}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error (attempt {attempt + 1}): {e}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "\n",
    "        if course_data:\n",
    "            all_courses.append(course_data)\n",
    "\n",
    "        # Auto-save every 3\n",
    "        if i % 3 == 0:\n",
    "            try:\n",
    "                pd.DataFrame(all_courses).to_excel(output_file, index=False)\n",
    "                print(f\"üíæ AUTO-SAVED: {len(all_courses)} courses to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error auto-saving: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        pd.DataFrame(all_courses).to_excel(output_file, index=False)\n",
    "        print(f\"üíæ FINAL SAVE: {len(all_courses)} courses to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in final save: {e}\")\n",
    "\n",
    "    return all_courses\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3: Main runner\n",
    "# ----------------------------------------------------\n",
    "def main():\n",
    "    print(\"üöÄ Scraping SureWin India Website\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    start_time = time.time()\n",
    "    save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\surewin_courses.xlsx\"\n",
    "\n",
    "    try:\n",
    "        print(\"Step 1: Finding course pages...\")\n",
    "        course_urls = find_all_course_pages_surewin()\n",
    "\n",
    "        if not course_urls:\n",
    "            print(\"\\n‚ùå No course pages found.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n‚úÖ Step 1 Complete: Found {len(course_urls)} course URLs\")\n",
    "\n",
    "        print(\"\\nStep 2: Extracting course details...\")\n",
    "        courses = get_course_details_surewin(course_urls, save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Process interrupted by user\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error: {e}\")\n",
    "        return\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if courses:\n",
    "        unique_courses = []\n",
    "        seen_urls = set()\n",
    "\n",
    "        for course in courses:\n",
    "            clean_url = course['course_link'].rstrip('/')\n",
    "            if clean_url not in seen_urls:\n",
    "                seen_urls.add(clean_url)\n",
    "                unique_courses.append(course)\n",
    "\n",
    "        try:\n",
    "            df_final = pd.DataFrame(unique_courses)\n",
    "            final_save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\surewin_courses_final.xlsx\"\n",
    "            df_final.to_excel(final_save_path, index=False)\n",
    "            print(f\"üíæ FINAL: {len(unique_courses)} courses to {final_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error saving final file: {e}\")\n",
    "\n",
    "        print(f\"\\nüéØ FINAL STATISTICS:\")\n",
    "        print(f\"Total course URLs found: {len(course_urls)}\")\n",
    "        print(f\"Unique courses extracted: {len(unique_courses)}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"\\nüìã SAMPLE COURSES (first 10):\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, course in enumerate(unique_courses[:10], 1):\n",
    "            print(f\"{i}. {course['course_name']}\")\n",
    "            print(f\"   {course['course_link']}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå No courses were processed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "def find_all_product_pages_surewin():\n",
    "    base_url = \"https://surewinindia.com\"\n",
    "    all_products = set()\n",
    "    \n",
    "    print(\"üîç Scanning SureWin India website for product pages...\")\n",
    "    \n",
    "    # Main pages to scan on SureWin website\n",
    "    main_pages = [\n",
    "        base_url,\n",
    "        f\"{base_url}/shop/\",\n",
    "        f\"{base_url}/courses/\",\n",
    "        f\"{base_url}/products/\",\n",
    "        f\"{base_url}/online-courses/\",\n",
    "        f\"{base_url}/all-courses/\",\n",
    "        f\"{base_url}/programs/\",\n",
    "        f\"{base_url}/trainings/\",\n",
    "        f\"{base_url}/certification-courses/\",\n",
    "        f\"{base_url}/diploma-courses/\",\n",
    "        f\"{base_url}/professional-courses/\",\n",
    "        f\"{base_url}/it-courses/\",\n",
    "        f\"{base_url}/technical-courses/\",\n",
    "        f\"{base_url}/management-courses/\",\n",
    "        f\"{base_url}/skill-development-courses/\",\n",
    "        f\"{base_url}/office-management-courses/\",\n",
    "        f\"{base_url}/teacher-training-courses/\",\n",
    "        f\"{base_url}/job-oriented-courses/\",\n",
    "        f\"{base_url}/safety-management-courses/\",\n",
    "        f\"{base_url}/advance-diploma/\",\n",
    "        f\"{base_url}/vocation-courses/\",\n",
    "        f\"{base_url}/yoga-courses/\",\n",
    "        f\"{base_url}/certificate-courses/\",\n",
    "        f\"{base_url}/industrial-courses/\",\n",
    "        f\"{base_url}/hobby-courses/\",\n",
    "        f\"{base_url}/edp-courses/\",\n",
    "        f\"{base_url}/language-courses/\",\n",
    "    ]\n",
    "    \n",
    "    # Common course categories for SureWin\n",
    "    categories = [\n",
    "        \"computer\", \"management\", \"skill\", \"office\", \"teacher\", \"job\", \n",
    "        \"safety\", \"yoga\", \"certificate\", \"diploma\", \"professional\", \n",
    "        \"it\", \"technical\", \"industrial\", \"hobby\", \"language\", \"web\",\n",
    "        \"programming\", \"accounting\", \"finance\", \"beauty\", \"health\",\n",
    "        \"training\", \"education\", \"development\", \"graphic\", \"hardware\",\n",
    "        \"networking\", \"software\", \"tally\"\n",
    "    ]\n",
    "    \n",
    "    for category in categories:\n",
    "        main_pages.extend([\n",
    "            f\"{base_url}/product-category/{category}/\",\n",
    "            f\"{base_url}/course-category/{category}/\",\n",
    "            f\"{base_url}/category/{category}/\"\n",
    "        ])\n",
    "    \n",
    "    visited_pages = set()\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    def scan_page(url):\n",
    "        if url in visited_pages:\n",
    "            return\n",
    "        visited_pages.add(url)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Scanning: {url}\")\n",
    "            response = session.get(url, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find all product links on SureWin\n",
    "                links = soup.find_all('a', href=True)\n",
    "                for link in links:\n",
    "                    href = link.get('href', '').strip()\n",
    "                    if '/product/' in href and 'surewinindia.com' in href:\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        # Clean URL - remove parameters and fragments\n",
    "                        clean_url = full_url.split('?')[0].split('#')[0]\n",
    "                        if clean_url not in all_products:\n",
    "                            all_products.add(clean_url)\n",
    "                            print(f\"Found product: {clean_url}\")\n",
    "                \n",
    "                # Look for pagination\n",
    "                pagination_links = soup.select('.page-numbers a, .pagination a, a.next, a.prev, .nav-links a')\n",
    "                for page_link in pagination_links:\n",
    "                    page_href = page_link.get('href', '')\n",
    "                    if page_href and 'surewinindia.com' in page_href and '/page/' in page_href:\n",
    "                        scan_page(page_href)\n",
    "                \n",
    "                # Look for category links\n",
    "                category_links = soup.select('a[href*=\"product-category\"], a[href*=\"course-category\"]')\n",
    "                for cat_link in category_links:\n",
    "                    cat_href = cat_link.get('href', '')\n",
    "                    if cat_href and 'surewinindia.com' in cat_href:\n",
    "                        scan_page(cat_href)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error scanning {url}: {e}\")\n",
    "    \n",
    "    # Scan all main pages\n",
    "    for page in main_pages:\n",
    "        try:\n",
    "            scan_page(page)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error with main page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return list(all_products)\n",
    "\n",
    "def get_course_details_surewin(product_urls, output_file='surewin_courses_auto.xlsx'):\n",
    "    \"\"\"Get course details from SureWin product pages with auto-save to Excel\"\"\"\n",
    "    \n",
    "    # Load existing progress if file exists\n",
    "    existing_courses = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_excel(output_file)\n",
    "            existing_courses = existing_df.to_dict('records')\n",
    "            print(f\"üìÅ Loaded {len(existing_courses)} existing courses from {output_file}\")\n",
    "        except:\n",
    "            print(\"üìÅ Starting fresh - no existing file\")\n",
    "    \n",
    "    # Create set of already processed URLs\n",
    "    processed_urls = set(course['course_link'] for course in existing_courses)\n",
    "    \n",
    "    # Filter out already processed URLs\n",
    "    urls_to_process = [url for url in product_urls if url not in processed_urls]\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(urls_to_process)} new URLs (skipping {len(processed_urls)} already processed)\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    all_courses = existing_courses.copy()\n",
    "    \n",
    "    for i, url in enumerate(urls_to_process, 1):\n",
    "        max_retries = 2\n",
    "        course_data = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = session.get(url, timeout=25)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    # Try to get course title from SureWin\n",
    "                    title_selectors = [\n",
    "                        'h1.product_title',\n",
    "                        '.product_title', \n",
    "                        'h1.entry-title',\n",
    "                        '.entry-title',\n",
    "                        'h1.title',\n",
    "                        'h1',\n",
    "                        '.product-title',\n",
    "                        '.course-title'\n",
    "                    ]\n",
    "                    \n",
    "                    course_name = None\n",
    "                    for selector in title_selectors:\n",
    "                        title_element = soup.select_one(selector)\n",
    "                        if title_element:\n",
    "                            course_name = title_element.get_text(strip=True)\n",
    "                            if course_name and len(course_name) > 3:\n",
    "                                break\n",
    "                    \n",
    "                    # If no title found, use the last part of URL\n",
    "                    if not course_name:\n",
    "                        course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                        course_name = re.sub(r'\\s+', ' ', course_name).strip()\n",
    "                    \n",
    "                    course_data = {\n",
    "                        'course_name': course_name,\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ {i}/{len(urls_to_process)}: {course_name}\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  HTTP {response.status_code} for {url}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è∞ Timeout (attempt {attempt + 1}/{max_retries}) for {url}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed after {max_retries} attempts: {url}\")\n",
    "                    course_data = {\n",
    "                        'course_name': url.split('/')[-2].replace('-', ' ').title() + \" (Timeout)\",\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error (attempt {attempt + 1}): {url} - {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed after {max_retries} attempts: {url}\")\n",
    "                    course_data = {\n",
    "                        'course_name': url.split('/')[-2].replace('-', ' ').title() + \" (Error)\",\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    break\n",
    "        \n",
    "        # Add course data if available\n",
    "        if course_data:\n",
    "            all_courses.append(course_data)\n",
    "        \n",
    "        # AUTO-SAVE to Excel every 5 courses\n",
    "        if i % 5 == 0:\n",
    "            try:\n",
    "                df = pd.DataFrame(all_courses)\n",
    "                df.to_excel(output_file, index=False)\n",
    "                print(f\"üíæ AUTO-SAVED: {len(all_courses)} courses to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error auto-saving: {e}\")\n",
    "        \n",
    "        # Progress tracking\n",
    "        if i % 10 == 0:\n",
    "            print(f\"üìä Progress: {i}/{len(urls_to_process)} new URLs processed | Total: {len(all_courses)} courses\")\n",
    "            \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Final save to Excel\n",
    "    try:\n",
    "        df = pd.DataFrame(all_courses)\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"üíæ FINAL SAVE: {len(all_courses)} courses to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in final save: {e}\")\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Scraping ALL course links from SureWin India website (surewinindia.com)\")\n",
    "    print(\"Target: https://surewinindia.com/product/... pattern\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Save to Excel file in Downloads folder\n",
    "    save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\surewin_courses.xlsx\"\n",
    "    output_file = save_path\n",
    "    \n",
    "    try:\n",
    "        # Find all product pages on SureWin\n",
    "        product_urls = find_all_product_pages_surewin()\n",
    "        \n",
    "        if not product_urls:\n",
    "            print(\"‚ùå No product pages found on SureWin website.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(product_urls)} product page URLs on SureWin\")\n",
    "        \n",
    "        # Save URLs backup\n",
    "        with open('surewin_product_urls_backup.txt', 'w') as f:\n",
    "            for url in product_urls:\n",
    "                f.write(url + '\\n')\n",
    "        print(\"üíæ URLs backup saved to 'surewin_product_urls_backup.txt'\")\n",
    "        \n",
    "        # Get course details with automatic saving to Excel\n",
    "        courses = get_course_details_surewin(product_urls, output_file)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Process interrupted by user\")\n",
    "        if 'courses' in locals():\n",
    "            try:\n",
    "                df = pd.DataFrame(courses)\n",
    "                df.to_excel(output_file, index=False)\n",
    "                print(f\"üíæ Saved {len(courses)} courses before interruption to {output_file}\")\n",
    "            except:\n",
    "                pass\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error: {e}\")\n",
    "        return\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if courses:\n",
    "        # Remove duplicates from final list\n",
    "        unique_courses = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        for course in courses:\n",
    "            clean_url = course['course_link'].rstrip('/')\n",
    "            if clean_url not in seen_urls:\n",
    "                seen_urls.add(clean_url)\n",
    "                unique_courses.append(course)\n",
    "        \n",
    "        # Save final deduplicated version to Excel\n",
    "        try:\n",
    "            df_final = pd.DataFrame(unique_courses)\n",
    "            final_save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\surewin_course_finals.xlsx\"\n",
    "            df_final.to_excel(final_save_path, index=False)\n",
    "            print(f\"üíæ FINAL DEDUPLICATED: {len(unique_courses)} courses to {final_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error saving final file: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Final Statistics for SureWin India:\")\n",
    "        print(f\"Total product URLs found: {len(product_urls)}\")\n",
    "        print(f\"Unique courses extracted: {len(unique_courses)}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Display sample\n",
    "        print(\"\\nüìã Sample of SureWin courses:\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, course in enumerate(unique_courses[:15], 1):\n",
    "            print(f\"{i}. {course['course_name']}\")\n",
    "            print(f\"   {course['course_link']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Final confirmation\n",
    "        print(f\"\\nData saved to {save_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No courses were processed from SureWin\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "def find_all_course_pages_iisdt():\n",
    "    base_url = \"https://surewinindia.com/\"\n",
    "    all_courses = set()\n",
    "    \n",
    "    print(\"üîç Scanning IISDT website for course pages...\")\n",
    "    \n",
    "    # Main pages to scan on IISDT website\n",
    "    main_pages = [\n",
    "        base_url,\n",
    "        f\"{base_url}/shop/\",\n",
    "        f\"{base_url}/courses/\",\n",
    "        f\"{base_url}/products/\",\n",
    "        f\"{base_url}/online-courses/\",\n",
    "        f\"{base_url}/all-courses/\",\n",
    "        f\"{base_url}/programs/\",\n",
    "        f\"{base_url}/trainings/\",\n",
    "    ]\n",
    "    \n",
    "    # Common course categories for IISDT\n",
    "    categories = [\n",
    "        \"human-rights\", \"law\", \"legal\", \"diploma\", \"certificate\", \n",
    "        \"management\", \"development\", \"social-work\", \"education\",\n",
    "        \"professional\", \"training\", \"course\", \"program\", \"sanitizer\",\n",
    "        \"manufacturing\", \"technology\", \"health\", \"safety\", \"bakery\",\n",
    "        \"hobby\", \"industrial\", \"folklore\",\"technical\",\"language\",\"hobby\"\n",
    "    ]\n",
    "    \n",
    "    for category in categories:\n",
    "        main_pages.extend([\n",
    "            f\"{base_url}/product-category/{category}/\",\n",
    "            f\"{base_url}/course-category/{category}/\",\n",
    "            f\"{base_url}/category/{category}/\"\\\n",
    "            \n",
    "            \n",
    "        ])\n",
    "    \n",
    "    visited_pages = set()\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    def scan_page(url):\n",
    "        if url in visited_pages:\n",
    "            return\n",
    "        visited_pages.add(url)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Scanning: {url}\")\n",
    "            response = session.get(url, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find ALL links first to see what's available\n",
    "                all_links = soup.find_all('a', href=True)\n",
    "                print(f\"   Found {len(all_links)} total links on page\")\n",
    "                \n",
    "                # Look for course/product links - IMPROVED DETECTION\n",
    "                course_links = []\n",
    "                for link in all_links:\n",
    "                    href = link.get('href', '').strip()\n",
    "                    text = link.get_text(strip=True)\n",
    "                    \n",
    "                    # IMPROVED: Check for course indicators in URL and text\n",
    "                    is_course_link = (\n",
    "                        ('/product/' in href or '/course/' in href or '/program/' in href) and \n",
    "                        'iisdt.com' in href\n",
    "                    )\n",
    "                    \n",
    "                    # Also check if link text suggests it's a course\n",
    "                    if not is_course_link and text:\n",
    "                        course_keywords = ['course', 'program', 'training', 'certificate', 'diploma', 'technology', 'development']\n",
    "                        if any(keyword in text.lower() for keyword in course_keywords) and len(text) > 5:\n",
    "                            if 'iisdt.com' in href and any(pattern in href for pattern in ['/course/', '/product/', '/program/']):\n",
    "                                is_course_link = True\n",
    "                    \n",
    "                    if is_course_link:\n",
    "                        full_url = urljoin(base_url, href)\n",
    "                        clean_url = full_url.split('?')[0].split('#')[0]\n",
    "                        \n",
    "                        if clean_url not in all_courses:\n",
    "                            all_courses.add(clean_url)\n",
    "                            course_links.append(clean_url)\n",
    "                            print(f\"   ‚úÖ Found course: {clean_url}\")\n",
    "                \n",
    "                print(f\"   üìä Total course links found on this page: {len(course_links)}\")\n",
    "                \n",
    "                # Look for pagination\n",
    "                pagination_selectors = ['.page-numbers a', '.pagination a', 'a.next', 'a.prev', '.nav-links a', 'a.page-numbers']\n",
    "                for selector in pagination_selectors:\n",
    "                    pagination_links = soup.select(selector)\n",
    "                    for page_link in pagination_links:\n",
    "                        page_href = page_link.get('href', '')\n",
    "                        if page_href and 'iisdt.com' in page_href:\n",
    "                            scan_page(page_href)\n",
    "                \n",
    "                # Look for category links\n",
    "                category_selectors = ['a[href*=\"category\"]', 'a[href*=\"shop\"]', 'a[href*=\"courses\"]']\n",
    "                for selector in category_selectors:\n",
    "                    category_links = soup.select(selector)\n",
    "                    for cat_link in category_links:\n",
    "                        cat_href = cat_link.get('href', '')\n",
    "                        if cat_href and 'iisdt.com' in cat_href:\n",
    "                            scan_page(cat_href)\n",
    "                        \n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed to load: HTTP {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error scanning {url}: {e}\")\n",
    "    \n",
    "    # Scan all main pages\n",
    "    successful_scans = 0\n",
    "    for page in main_pages:\n",
    "        try:\n",
    "            scan_page(page)\n",
    "            successful_scans += 1\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error with main page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüìä Scan Summary: {successful_scans}/{len(main_pages)} pages scanned successfully\")\n",
    "    print(f\"üìä Total course URLs found: {len(all_courses)}\")\n",
    "    \n",
    "    return list(all_courses)\n",
    "\n",
    "def get_course_details_iisdt(course_urls, output_file):\n",
    "    \"\"\"Get course details from IISDT pages with improved title extraction\"\"\"\n",
    "    \n",
    "    if not course_urls:\n",
    "        print(\"‚ùå No course URLs to process\")\n",
    "        return []\n",
    "    \n",
    "    # Load existing progress if file exists\n",
    "    existing_courses = []\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            existing_df = pd.read_excel(output_file)\n",
    "            existing_courses = existing_df.to_dict('records')\n",
    "            print(f\"üìÅ Loaded {len(existing_courses)} existing courses from {output_file}\")\n",
    "        except:\n",
    "            print(\"üìÅ Starting fresh - no existing file\")\n",
    "    \n",
    "    # Create set of already processed URLs\n",
    "    processed_urls = set(course['course_link'] for course in existing_courses)\n",
    "    \n",
    "    # Filter out already processed URLs\n",
    "    urls_to_process = [url for url in course_urls if url not in processed_urls]\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(urls_to_process)} new URLs\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    all_courses = existing_courses.copy()\n",
    "    \n",
    "    for i, url in enumerate(urls_to_process, 1):\n",
    "        max_retries = 2\n",
    "        course_data = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"üìñ Fetching: {url}\")\n",
    "                response = session.get(url, timeout=20)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    # IMPROVED title extraction for IISDT\n",
    "                    title_selectors = [\n",
    "                        'h1',  # Most common\n",
    "                        '.title-page h1',  # From the HTML you provided\n",
    "                        'h1.product_title',\n",
    "                        '.product_title', \n",
    "                        'h1.entry-title',\n",
    "                        '.entry-title',\n",
    "                        '.product-title',\n",
    "                        '.course-title',\n",
    "                        'title'  # Fallback to page title\n",
    "                    ]\n",
    "                    \n",
    "                    course_name = None\n",
    "                    for selector in title_selectors:\n",
    "                        title_element = soup.select_one(selector)\n",
    "                        if title_element:\n",
    "                            course_name = title_element.get_text(strip=True)\n",
    "                            # Clean the course name\n",
    "                            if course_name and len(course_name) > 3:\n",
    "                                # Remove website name if present\n",
    "                                course_name = re.sub(r'\\s*-\\s*IISDT\\s*$', '', course_name, flags=re.IGNORECASE)\n",
    "                                course_name = course_name.strip()\n",
    "                                break\n",
    "                    \n",
    "                    # If no title found, use the last part of URL and clean it\n",
    "                    if not course_name:\n",
    "                        course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                        course_name = re.sub(r'\\s+', ' ', course_name).strip()\n",
    "                        # Remove numbers at the end like \"-1\", \"-2\"\n",
    "                        course_name = re.sub(r'\\s+\\d+$', '', course_name)\n",
    "                    \n",
    "                    course_data = {\n",
    "                        'course_name': course_name,\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ {i}/{len(urls_to_process)}: {course_name}\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  HTTP {response.status_code} for {url}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è∞ Timeout (attempt {attempt + 1}/{max_retries}) for {url}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed after {max_retries} attempts: {url}\")\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                    course_name = re.sub(r'\\s+\\d+$', '', course_name)\n",
    "                    course_data = {\n",
    "                        'course_name': course_name + \" (Timeout)\",\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed after {max_retries} attempts: {url}\")\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                    course_name = re.sub(r'\\s+\\d+$', '', course_name)\n",
    "                    course_data = {\n",
    "                        'course_name': course_name + \" (Error)\",\n",
    "                        'course_link': url\n",
    "                    }\n",
    "                    break\n",
    "        \n",
    "        # Add course data if available\n",
    "        if course_data:\n",
    "            all_courses.append(course_data)\n",
    "        \n",
    "        # AUTO-SAVE to Excel every 3 courses\n",
    "        if i % 3 == 0:\n",
    "            try:\n",
    "                df = pd.DataFrame(all_courses)\n",
    "                df.to_excel(output_file, index=False)\n",
    "                print(f\"üíæ AUTO-SAVED: {len(all_courses)} courses to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error auto-saving: {e}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Final save\n",
    "    try:\n",
    "        df = pd.DataFrame(all_courses)\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"üíæ FINAL SAVE: {len(all_courses)} courses to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in final save: {e}\")\n",
    "    \n",
    "    return all_courses\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Scraping IISDT Website - Improved Course Detection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    save_path = r\"C:\\Users\\siddi\\Downloads\\iisdt_courses.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Find all course pages\n",
    "        print(\"Step 1: Finding course pages...\")\n",
    "        course_urls = find_all_course_pages_iisdt()\n",
    "        \n",
    "        if not course_urls:\n",
    "            print(\"\\n‚ùå No course pages found.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚úÖ Step 1 Complete: Found {len(course_urls)} course URLs\")\n",
    "        \n",
    "        # Save URLs backup\n",
    "        with open('iisdt_course_urls_backup.txt', 'w') as f:\n",
    "            for url in course_urls:\n",
    "                f.write(url + '\\n')\n",
    "        print(\"üíæ URLs backup saved to 'iisdt_course_urls_backup.txt'\")\n",
    "        \n",
    "        # Get course details\n",
    "        print(\"\\nStep 2: Extracting course details...\")\n",
    "        courses = get_course_details_iisdt(course_urls, save_path)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Process interrupted by user\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error: {e}\")\n",
    "        return\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if courses:\n",
    "        # Remove duplicates\n",
    "        unique_courses = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        for course in courses:\n",
    "            clean_url = course['course_link'].rstrip('/')\n",
    "            if clean_url not in seen_urls:\n",
    "                seen_urls.add(clean_url)\n",
    "                unique_courses.append(course)\n",
    "        \n",
    "        # Save final version\n",
    "        try:\n",
    "            df_final = pd.DataFrame(unique_courses)\n",
    "            final_save_path = r\"C:\\Users\\siddi\\Downloads\\iisdt_courses_final.xlsx\"\n",
    "            df_final.to_excel(final_save_path, index=False)\n",
    "            print(f\"üíæ FINAL: {len(unique_courses)} courses to {final_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error saving final file: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL STATISTICS:\")\n",
    "        print(f\"Total course URLs found: {len(course_urls)}\")\n",
    "        print(f\"Unique courses extracted: {len(unique_courses)}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Display sample\n",
    "        print(f\"\\nüìã SAMPLE COURSES (showing first 15):\")\n",
    "        print(\"=\" * 80)\n",
    "        for i, course in enumerate(unique_courses[:15], 1):\n",
    "            print(f\"{i}. {course['course_name']}\")\n",
    "            print(f\"   {course['course_link']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        print(f\"\\nData saved to {save_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No courses were processed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
