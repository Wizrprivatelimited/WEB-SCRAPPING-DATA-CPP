{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6ea99a",
   "metadata": {},
   "source": [
    "Do for One link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6d5166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üîç Processing: https://surewinindia.com/product/certificate-in-yoga-and-naturopathy/\n",
      "üåê Accessing URL: https://surewinindia.com/product/certificate-in-yoga-and-naturopathy/\n",
      "üìõ Course Name: Certificate in Yoga and Naturopathy\n",
      "üìù About Course: A Certificate in Yoga and Naturopathy opens doors to a rewarding career in the w...\n",
      "‚úÖ Eligibility: Completion of10+2 (higher Secondary)or equivalent.\n",
      "‚è±Ô∏è Duration: Three Months.\n",
      "üí∞ Price: Rs.7,000.00\n",
      "üë• Who Should Take It: Not available\n",
      "üìö Syllabus (Old): - Introduction to Yoga and Naturopathy:History and philosophy of yoga, Principle...\n",
      "üìö Syllabus (New): Introduction to Yoga and Naturopathy:History and philosophy of yoga, Principles ...\n",
      "üìú Certificate: https://iisdt.in/wp-content/uploads/2025/06/Sample-Diploma.pdf\n",
      "üö™ Browser closed\n",
      "üíæ Saved data for: Certificate in Yoga and Naturopathy\n",
      "\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")  # modern headless\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specified words from text\"\"\"\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- BULLET FORMATTING --------------------\n",
    "def format_bullets(text):\n",
    "    \"\"\"\n",
    "    Ensure there is a newline between each bullet point.\n",
    "    Example: \"- Topic 1... .- Topic 2...\" ‚Üí \"- Topic 1...\\n\\n- Topic 2...\"\n",
    "    \"\"\"\n",
    "    if not text or text == \"Not available\":\n",
    "        return text\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\.\\s*-\\s*', r'.\\n\\n- ', text)\n",
    "    return text\n",
    "\n",
    "# -------------------- SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    \"\"\"Extract syllabus content up to Job Opportunities or Career\"\"\"\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    for heading in headings:\n",
    "        heading_text = heading.get_text(strip=True).lower()\n",
    "        if 'syllabus' in heading_text:\n",
    "            syllabus_content = []\n",
    "            elements_to_process = []\n",
    "            current_elem = heading\n",
    "            while current_elem:\n",
    "                current_elem = current_elem.find_next_sibling()\n",
    "                if not current_elem:\n",
    "                    break\n",
    "                if current_elem.name in ['h1','h2','h3','h4','h5','h6']:\n",
    "                    heading_text = current_elem.get_text(strip=True).lower()\n",
    "                    if 'job opportunit' in heading_text or 'career' in heading_text:\n",
    "                        break\n",
    "                elements_to_process.append(current_elem)\n",
    "\n",
    "            for elem in elements_to_process:\n",
    "                elem_text = elem.get_text(strip=True)\n",
    "                if 'job opportunities' in elem_text.lower() and len(elem_text) < 100:\n",
    "                    break\n",
    "                if elem.name in ['p', 'ul', 'ol', 'div']:\n",
    "                    if elem.name in ['ul','ol']:\n",
    "                        for item in elem.find_all('li'):\n",
    "                            item_text = clean_text(item.get_text(strip=True))\n",
    "                            if item_text and 'job opportunit' not in item_text.lower():\n",
    "                                syllabus_content.append(f\"- {item_text}\")\n",
    "                    else:\n",
    "                        paragraph_text = clean_text(elem.get_text(strip=True))\n",
    "                        if paragraph_text and 'job opportunit' not in paragraph_text.lower():\n",
    "                            syllabus_content.append(paragraph_text)\n",
    "            if syllabus_content:\n",
    "                return format_bullets(\"\\n\".join(syllabus_content))\n",
    "    return \"Not available\"\n",
    "\n",
    "def extract_old_syllabus(soup):\n",
    "    \"\"\"Old method for syllabus extraction\"\"\"\n",
    "    syllabus_content = []\n",
    "    stop_markers = [\n",
    "        \"After completing\", \"Graduates\",\n",
    "        \"After successful\", \"After \",\n",
    "        \"Job Opportunities\", \"job opportunities\"\n",
    "    ]\n",
    "\n",
    "    syllabus_section = soup.find(string=re.compile(\"Syllabus\", re.IGNORECASE))\n",
    "    if syllabus_section:\n",
    "        next_elem = syllabus_section.find_next()\n",
    "        while next_elem and next_elem.name in [\"p\", \"ul\", \"ol\"]:\n",
    "            text_content = next_elem.get_text(strip=True)\n",
    "\n",
    "            if any(marker.lower() in text_content.lower() for marker in stop_markers):\n",
    "                break\n",
    "\n",
    "            if next_elem.name in [\"ul\", \"ol\"]:\n",
    "                for item in next_elem.find_all(\"li\"):\n",
    "                    item_text = clean_text(item.get_text(strip=True))\n",
    "                    if not any(marker.lower() in item_text.lower() for marker in stop_markers):\n",
    "                        syllabus_content.append(f\"- {item_text}\")\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                if text_content:\n",
    "                    syllabus_content.append(f\"- {clean_text(text_content)}\")\n",
    "\n",
    "            next_elem = next_elem.find_next_sibling()\n",
    "\n",
    "    return format_bullets(\"\\n\".join(syllabus_content)) if syllabus_content else \"Not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name\n",
    "        course_name_tag = soup.find(\"h1\", class_=\"product_title entry-title\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. About Course\n",
    "        about_course = \"About course not found\"\n",
    "        about_sections = soup.find_all(\"p\")\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"objective\" in strong_tag.get_text(strip=True).lower():\n",
    "                about_course = clean_text(section.get_text(\" \", strip=True))\n",
    "                break\n",
    "        print(f\"üìù About Course: {about_course[:80]}...\")\n",
    "\n",
    "        # 3. Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"eligibility\" in strong_tag.get_text(strip=True).lower():\n",
    "                eligibility = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚úÖ Eligibility: {eligibility}\")\n",
    "\n",
    "        # 4. Duration\n",
    "        duration = \"Not available\"\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"duration\" in strong_tag.get_text(strip=True).lower():\n",
    "                duration = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚è±Ô∏è Duration: {duration}\")\n",
    "\n",
    "        # 5. Price\n",
    "        price = \"Not available\"\n",
    "        price_tag = soup.find(\"bdi\")\n",
    "        if price_tag:\n",
    "            price = price_tag.get_text(strip=True)\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # 6. Who Should Take It\n",
    "        who_content = []\n",
    "        who_strong = soup.find(\"strong\", string=re.compile(r\"Who Should Enroll|Who Should Take It\", re.IGNORECASE))\n",
    "        if who_strong:\n",
    "            next_ul = who_strong.find_next(\"ul\")\n",
    "            if next_ul:\n",
    "                for li in next_ul.find_all(\"li\"):\n",
    "                    who_content.append(clean_text(li.get_text(strip=True)))\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "        print(f\"üë• Who Should Take It: {who_should_take}\")\n",
    "\n",
    "        # 7A. Old syllabus extraction\n",
    "        syllabus_old = extract_old_syllabus(soup)\n",
    "        # 7B. New syllabus extraction\n",
    "        syllabus_new = extract_syllabus(soup)\n",
    "\n",
    "        print(f\"üìö Syllabus (Old): {syllabus_old[:80]}...\")\n",
    "        print(f\"üìö Syllabus (New): {syllabus_new[:80]}...\")\n",
    "\n",
    "        # 8. Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        cert_tag = soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "        if cert_tag and cert_tag.has_attr(\"href\"):\n",
    "            cert_link = cert_tag[\"href\"]\n",
    "        else:\n",
    "            cert_img = soup.find(\"img\", src=re.compile(r\"certificate\", re.IGNORECASE))\n",
    "            if cert_img and cert_img.has_attr(\"src\"):\n",
    "                cert_link = cert_img[\"src\"]\n",
    "        print(f\"üìú Certificate: {cert_link}\")\n",
    "\n",
    "        return course_name, about_course, eligibility, duration, price, who_should_take, syllabus_old, syllabus_new, cert_link\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path, url):\n",
    "    columns = [\n",
    "        \"Course Name\",\n",
    "        \"About Course\",\n",
    "        \"Eligibility\",\n",
    "        \"Duration\",\n",
    "        \"Price\",\n",
    "        \"Who Should Take It\",\n",
    "        \"Syllabus (Old)\",\n",
    "        \"Syllabus (New)\",\n",
    "        \"Certificate\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "    (course_name, about_course, eligibility, duration, price,\n",
    "     who_should_take, syllabus_old, syllabus_new, cert_link) = data\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # ‚úÖ Avoid duplicate entries\n",
    "        if url not in df[\"Course URL\"].values:\n",
    "            row = {\n",
    "                \"Course Name\": course_name,\n",
    "                \"About Course\": about_course,\n",
    "                \"Eligibility\": eligibility,\n",
    "                \"Duration\": duration,\n",
    "                \"Price\": price,\n",
    "                \"Who Should Take It\": who_should_take,\n",
    "                \"Syllabus (Old)\": syllabus_old,\n",
    "                \"Syllabus (New)\": syllabus_new,\n",
    "                \"Certificate\": cert_link,\n",
    "                \"Course URL\": url\n",
    "            }\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "            df.to_excel(file_path, index=False)\n",
    "            print(f\"üíæ Saved data for: {course_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped duplicate: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "        \"https://surewinindia.com/product/certificate-in-yoga-and-naturopathy/\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\course.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüîç Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, file_path, course_url)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Process completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328b76d",
   "metadata": {},
   "source": [
    "Do for Entire Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab071e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üîç Processing: https://surewinindia.com/product/professional-course-in-travel-tour-management/\n",
      "üåê Accessing URL: https://surewinindia.com/product/professional-course-in-travel-tour-management/\n",
      "üìõ Course Name: Course name not found\n",
      "üìù About Course: A Professional Course in Travel & Tour Management is an ideal program for indivi...\n",
      "‚úÖ Eligibility: Completion of10th Grade (high School)or equivalent.\n",
      "‚è±Ô∏è Duration: Three Months.\n",
      "üí∞ Price: Rs.7,000.00\n",
      "üë• Who Should Take It: Not available\n",
      "üìö Syllabus (Old): - Introduction to Travel and Tourism:Concept and Scope of Travel & Tourism, Hist...\n",
      "üìö Syllabus (New): Introduction to Travel and Tourism:Concept and Scope of Travel & Tourism, Histor...\n",
      "üìú Certificate: https://iisdt.in/wp-content/uploads/2025/06/Sample-Diploma.pdf\n",
      "üö™ Browser closed\n",
      "\n",
      "üìÑ Course Data:\n",
      "Course Name: Course name not found\n",
      "\n",
      "About Course: A Professional Course in Travel & Tour Management is an ideal program for individuals passionate about travel, culture, and hospitality. It provides a strong foundation in the principles and practices of the tourism industry, preparing graduates for rewarding careers in one of the world‚Äôs most vibrant sectors. With opportunities for growth and exploration, this diploma is a gateway to a fulfilling and adventurous profession.\n",
      "\n",
      "Eligibility: Completion of10th Grade (high School)or equivalent.\n",
      "\n",
      "Duration: Three Months.\n",
      "\n",
      "Price: Rs.7,000.00\n",
      "\n",
      "Who Should Take It: Not available\n",
      "\n",
      "Syllabus (Old): - Introduction to Travel and Tourism:Concept and Scope of Travel & Tourism, History and Evolution of Tourism, Types of Tourism (Leisure, Adventure, Medical, Eco-Tourism), Role of Tourism in Economic Development, Tourism Organizations (UNWTO, IATA, WTO, ICAO), Tourism Planning and Development, Sustainable and Responsible Tourism, Emerging Trends in Global Tourism, Impact of Technology on Travel Industry, Career Opportunities in Tourism Sector.\n",
      "\n",
      "- Travel Agency and Tour Operations Management:Functions of a Travel Agency, Types of Travel Agencies (Retail, Wholesale, Online), Role and Responsibilities of a Tour Operator, Setting Up and Managing a Travel Agency, Understanding IATA Accreditation and Licensing, Travel Documentation (Passports, Visas, Permits), Tour Packaging and Itinerary Planning, Supplier Relations and Contracting, Customer Service in Travel Agencies, Legal and Ethical Issues in Travel Business.\n",
      "\n",
      "- Air Ticketing and Airline Operations:Introduction to Airline Industry, Role of IATA in Air Travel, Types of Airline Ticketing (Scheduled, Charter, Low-Cost), Reservation Systems and GDS (Amadeus, Galileo, Sabre), Fare Calculation and Ticketing Procedures, Understanding Airline Codes and Classes, Baggage Policies and Travel Regulations, Airport Operations and Security Procedures, Airline Alliances and Loyalty Programs, Case Studies on Airline Industry Trends.\n",
      "\n",
      "- Tour Packaging and Itinerary Planning:Components of a Tour Package, Types of Tour Packages (Group, FIT, Adventure, Luxury), Steps in Tour Planning and Design, Budgeting and Costing for Tour Packages, Destination Research and Selection, Creating Customized Itineraries, Role of Tour Guides and Escorts, Legal Aspects of Tour Operations, Marketing and Selling Tour Packages, Case Studies on Successful Tour Packages.\n",
      "\n",
      "- Hospitality and Accommodation Management:Types of Accommodation (Hotels, Resorts, Hostels, Homestays), Hotel Star Classification and Ratings, Hotel Operations and Management, Role of Travel Agencies in Hotel Bookings, Customer Service in Hospitality Industry, Revenue Management in Hotels, Emerging Trends in Hospitality Industry, Sustainable Practices in Hospitality, Role of OTAs (Online Travel Agencies) in Hotel Bookings, Case Studies on Hospitality Industry Best Practices.\n",
      "\n",
      "- Tourism Marketing and Promotion:Principles of Tourism Marketing, Understanding Consumer Behavior in Tourism, Digital Marketing Strategies for Travel Businesses, Role of Social Media in Tourism Promotion, Branding and Destination Marketing, Influencer Marketing in Travel Industry, Customer Relationship Management (CRM) in Tourism, Pricing Strategies for Tourism Products, Public Relations in Tourism Industry, Case Studies on Successful Tourism Marketing Campaigns.\n",
      "\n",
      "Syllabus (New): Introduction to Travel and Tourism:Concept and Scope of Travel & Tourism, History and Evolution of Tourism, Types of Tourism (Leisure, Adventure, Medical, Eco-Tourism), Role of Tourism in Economic Development, Tourism Organizations (UNWTO, IATA, WTO, ICAO), Tourism Planning and Development, Sustainable and Responsible Tourism, Emerging Trends in Global Tourism, Impact of Technology on Travel Industry, Career Opportunities in Tourism Sector.\n",
      "Travel Agency and Tour Operations Management:Functions of a Travel Agency, Types of Travel Agencies (Retail, Wholesale, Online), Role and Responsibilities of a Tour Operator, Setting Up and Managing a Travel Agency, Understanding IATA Accreditation and Licensing, Travel Documentation (Passports, Visas, Permits), Tour Packaging and Itinerary Planning, Supplier Relations and Contracting, Customer Service in Travel Agencies, Legal and Ethical Issues in Travel Business.\n",
      "Air Ticketing and Airline Operations:Introduction to Airline Industry, Role of IATA in Air Travel, Types of Airline Ticketing (Scheduled, Charter, Low-Cost), Reservation Systems and GDS (Amadeus, Galileo, Sabre), Fare Calculation and Ticketing Procedures, Understanding Airline Codes and Classes, Baggage Policies and Travel Regulations, Airport Operations and Security Procedures, Airline Alliances and Loyalty Programs, Case Studies on Airline Industry Trends.\n",
      "Tour Packaging and Itinerary Planning:Components of a Tour Package, Types of Tour Packages (Group, FIT, Adventure, Luxury), Steps in Tour Planning and Design, Budgeting and Costing for Tour Packages, Destination Research and Selection, Creating Customized Itineraries, Role of Tour Guides and Escorts, Legal Aspects of Tour Operations, Marketing and Selling Tour Packages, Case Studies on Successful Tour Packages.\n",
      "Hospitality and Accommodation Management:Types of Accommodation (Hotels, Resorts, Hostels, Homestays), Hotel Star Classification and Ratings, Hotel Operations and Management, Role of Travel Agencies in Hotel Bookings, Customer Service in Hospitality Industry, Revenue Management in Hotels, Emerging Trends in Hospitality Industry, Sustainable Practices in Hospitality, Role of OTAs (Online Travel Agencies) in Hotel Bookings, Case Studies on Hospitality Industry Best Practices.\n",
      "Tourism Marketing and Promotion:Principles of Tourism Marketing, Understanding Consumer Behavior in Tourism, Digital Marketing Strategies for Travel Businesses, Role of Social Media in Tourism Promotion, Branding and Destination Marketing, Influencer Marketing in Travel Industry, Customer Relationship Management (CRM) in Tourism, Pricing Strategies for Tourism Products, Public Relations in Tourism Industry, Case Studies on Successful Tourism Marketing Campaigns.\n",
      "\n",
      "üíæ Saved data for: Course name not found\n",
      "\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")  # modern headless\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specified words from text\"\"\"\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- BULLET FORMATTING --------------------\n",
    "def format_bullets(text):\n",
    "    \"\"\"\n",
    "    Ensure there is a newline between each bullet point.\n",
    "    Example: \"- Topic 1... .- Topic 2...\" ‚Üí \"- Topic 1...\\n\\n- Topic 2...\"\n",
    "    \"\"\"\n",
    "    if not text or text == \"Not available\":\n",
    "        return text\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\.\\s*-\\s*', r'.\\n\\n- ', text)\n",
    "    return text\n",
    "\n",
    "# -------------------- SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    \"\"\"Extract syllabus content up to Job Opportunities or Career\"\"\"\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    for heading in headings:\n",
    "        heading_text = heading.get_text(strip=True).lower()\n",
    "        if 'syllabus' in heading_text:\n",
    "            syllabus_content = []\n",
    "            elements_to_process = []\n",
    "            current_elem = heading\n",
    "            while current_elem:\n",
    "                current_elem = current_elem.find_next_sibling()\n",
    "                if not current_elem:\n",
    "                    break\n",
    "                if current_elem.name in ['h1','h2','h3','h4','h5','h6']:\n",
    "                    heading_text = current_elem.get_text(strip=True).lower()\n",
    "                    if 'job opportunit' in heading_text or 'career' in heading_text:\n",
    "                        break\n",
    "                elements_to_process.append(current_elem)\n",
    "\n",
    "            for elem in elements_to_process:\n",
    "                elem_text = elem.get_text(strip=True)\n",
    "                if 'job opportunities' in elem_text.lower() and len(elem_text) < 100:\n",
    "                    break\n",
    "                if elem.name in ['p', 'ul', 'ol', 'div']:\n",
    "                    if elem.name in ['ul','ol']:\n",
    "                        for item in elem.find_all('li'):\n",
    "                            item_text = clean_text(item.get_text(strip=True))\n",
    "                            if item_text and 'job opportunit' not in item_text.lower():\n",
    "                                syllabus_content.append(f\"- {item_text}\")\n",
    "                    else:\n",
    "                        paragraph_text = clean_text(elem.get_text(strip=True))\n",
    "                        if paragraph_text and 'job opportunit' not in paragraph_text.lower():\n",
    "                            syllabus_content.append(paragraph_text)\n",
    "            if syllabus_content:\n",
    "                return format_bullets(\"\\n\".join(syllabus_content))\n",
    "    return \"Not available\"\n",
    "\n",
    "def extract_old_syllabus(soup):\n",
    "    \"\"\"Old method for syllabus extraction\"\"\"\n",
    "    syllabus_content = []\n",
    "    stop_markers = [\n",
    "        \"After completing\", \"Graduates\",\n",
    "        \"After successful\", \"After \",\n",
    "        \"Job Opportunities\", \"job opportunities\"\n",
    "    ]\n",
    "\n",
    "    syllabus_section = soup.find(string=re.compile(\"Syllabus\", re.IGNORECASE))\n",
    "    if syllabus_section:\n",
    "        next_elem = syllabus_section.find_next()\n",
    "        while next_elem and next_elem.name in [\"p\", \"ul\", \"ol\"]:\n",
    "            text_content = next_elem.get_text(strip=True)\n",
    "\n",
    "            if any(marker.lower() in text_content.lower() for marker in stop_markers):\n",
    "                break\n",
    "\n",
    "            if next_elem.name in [\"ul\", \"ol\"]:\n",
    "                for item in next_elem.find_all(\"li\"):\n",
    "                    item_text = clean_text(item.get_text(strip=True))\n",
    "                    if not any(marker.lower() in item_text.lower() for marker in stop_markers):\n",
    "                        syllabus_content.append(f\"- {item_text}\")\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                if text_content:\n",
    "                    syllabus_content.append(f\"- {clean_text(text_content)}\")\n",
    "\n",
    "            next_elem = next_elem.find_next_sibling()\n",
    "\n",
    "    return format_bullets(\"\\n\".join(syllabus_content)) if syllabus_content else \"Not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name\n",
    "        course_name_tag = soup.find(\"h1\", class_=\"product_title entry-title\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. About Course\n",
    "        about_course = \"About course not found\"\n",
    "        about_sections = soup.find_all(\"p\")\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"objective\" in strong_tag.get_text(strip=True).lower():\n",
    "                about_course = clean_text(section.get_text(\" \", strip=True))\n",
    "                break\n",
    "        print(f\"üìù About Course: {about_course[:80]}...\")\n",
    "\n",
    "        # 3. Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"eligibility\" in strong_tag.get_text(strip=True).lower():\n",
    "                eligibility = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚úÖ Eligibility: {eligibility}\")\n",
    "\n",
    "        # 4. Duration\n",
    "        duration = \"Not available\"\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"duration\" in strong_tag.get_text(strip=True).lower():\n",
    "                duration = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚è±Ô∏è Duration: {duration}\")\n",
    "\n",
    "        # 5. Price\n",
    "        price = \"Not available\"\n",
    "        price_tag = soup.find(\"bdi\")\n",
    "        if price_tag:\n",
    "            price = price_tag.get_text(strip=True)\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # 6. Who Should Take It\n",
    "        who_content = []\n",
    "        who_strong = soup.find(\"strong\", string=re.compile(r\"Who Should Enroll|Who Should Take It\", re.IGNORECASE))\n",
    "        if who_strong:\n",
    "            next_ul = who_strong.find_next(\"ul\")\n",
    "            if next_ul:\n",
    "                for li in next_ul.find_all(\"li\"):\n",
    "                    who_content.append(clean_text(li.get_text(strip=True)))\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "        print(f\"üë• Who Should Take It: {who_should_take}\")\n",
    "\n",
    "        # 7A. Old syllabus extraction\n",
    "        syllabus_old = extract_old_syllabus(soup)\n",
    "        # 7B. New syllabus extraction\n",
    "        syllabus_new = extract_syllabus(soup)\n",
    "\n",
    "        print(f\"üìö Syllabus (Old): {syllabus_old[:80]}...\")\n",
    "        print(f\"üìö Syllabus (New): {syllabus_new[:80]}...\")\n",
    "\n",
    "        # 8. Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        cert_tag = soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "        if cert_tag and cert_tag.has_attr(\"href\"):\n",
    "            cert_link = cert_tag[\"href\"]\n",
    "        else:\n",
    "            cert_img = soup.find(\"img\", src=re.compile(r\"certificate\", re.IGNORECASE))\n",
    "            if cert_img and cert_img.has_attr(\"src\"):\n",
    "                cert_link = cert_img[\"src\"]\n",
    "        print(f\"üìú Certificate: {cert_link}\")\n",
    "\n",
    "        return course_name, about_course, eligibility, duration, price, who_should_take, syllabus_old, syllabus_new, cert_link\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path, url):\n",
    "    columns = [\n",
    "        \"Course Name\",\n",
    "        \"About Course\",\n",
    "        \"Eligibility\",\n",
    "        \"Duration\",\n",
    "        \"Price\",\n",
    "        \"Who Should Take It\",\n",
    "        \"Syllabus (Old)\",\n",
    "        \"Syllabus (New)\",\n",
    "        \"Certificate\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "    (course_name, about_course, eligibility, duration, price,\n",
    "     who_should_take, syllabus_old, syllabus_new, cert_link) = data\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # ‚úÖ Avoid duplicate entries\n",
    "        if url not in df[\"Course URL\"].values:\n",
    "            row = {\n",
    "                \"Course Name\": course_name,\n",
    "                \"About Course\": about_course,\n",
    "                \"Eligibility\": eligibility,\n",
    "                \"Duration\": duration,\n",
    "                \"Price\": price,\n",
    "                \"Who Should Take It\": who_should_take,\n",
    "                \"Syllabus (Old)\": syllabus_old,\n",
    "                \"Syllabus (New)\": syllabus_new,\n",
    "                \"Certificate\": cert_link,\n",
    "                \"Course URL\": url\n",
    "            }\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "            df.to_excel(file_path, index=False)\n",
    "            print(f\"üíæ Saved data for: {course_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped duplicate: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_excel = r\"C:\\Users\\taslim.siddiqui\\Downloads\\test for one.xlsx\"\n",
    "    df_urls = pd.read_excel(input_excel)\n",
    "    \n",
    "    if \"Course URL\" not in df_urls.columns:\n",
    "        raise ValueError(\"Excel file must contain a column named 'Course URL'\")\n",
    "\n",
    "    course_urls = df_urls[\"Course URL\"].dropna().tolist()\n",
    "    output_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\Surewin__.xlsx\"\n",
    "\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüîç Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            # Print all columns before saving\n",
    "            columns = [\n",
    "                \"Course Name\",\n",
    "                \"About Course\",\n",
    "                \"Eligibility\",\n",
    "                \"Duration\",\n",
    "                \"Price\",\n",
    "                \"Who Should Take It\",\n",
    "                \"Syllabus (Old)\",\n",
    "                \"Syllabus (New)\",\n",
    "                \"Certificate\"\n",
    "            ]\n",
    "            print(\"\\nüìÑ Course Data:\")\n",
    "            for col_name, value in zip(columns, course_data[:-1]):  # exclude last URL for printing\n",
    "                print(f\"{col_name}: {value}\\n\")\n",
    "            \n",
    "            save_to_excel(course_data, output_file, course_url)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Process completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605d970",
   "metadata": {},
   "source": [
    "Courses link surewin 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61c7579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üîç Processing: https://iisdt.com/course/big-data-analytics-hadoop-college\n",
      "üåê Accessing URL: https://iisdt.com/course/big-data-analytics-hadoop-college\n",
      "üìõ Course Name: Big Data Analytics (Hadoop)\n",
      "üó£Ô∏è Language: English\n",
      "üìù About Course: Big data analytics is the application of cutting-edge methods to various types o...\n",
      "‚úÖ Eligibility: Not available\n",
      "‚è±Ô∏è Duration: 4:14:04\n",
      "üí∞ Price: ‚Çπ2,500.00\n",
      "üë• Who Should Take It: Not available\n",
      "üìö Syllabus: - Introduction and outline of big data analytics\n",
      "- What is big data analytics in...\n",
      "üìú Certificate: https://iisdt.com/public/uploads/b2b_certificate/demo_certificate/27.png\n",
      "üö™ Browser closed\n",
      "‚è∞ Rounded Hours: 5\n",
      "üíæ Saved data for: Big Data Analytics (Hadoop)\n",
      "\n",
      "üìä Results:\n",
      "                   Course Name Duration  Round Hours\n",
      "0           Verify Certificate  6:44:06            7\n",
      "1  Big Data Analytics (Hadoop)  4:14:04            5\n",
      "\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")  # modern headless\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specified words from text\"\"\"\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- ROUND HOURS FUNCTION --------------------\n",
    "def roundup_hours_simple(time_value):\n",
    "    \"\"\"Simple function to handle all time formats\"\"\"\n",
    "    if pd.isna(time_value) or time_value == \"Not available\" or not time_value:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # Handle string formats\n",
    "        if isinstance(time_value, str):\n",
    "            # Remove any non-numeric characters except colons and decimal points\n",
    "            time_value = re.sub(r'[^\\d:.]', '', time_value)\n",
    "            \n",
    "            # Handle HH:MM:SS format (like \"0:53:00\")\n",
    "            if ':' in time_value:\n",
    "                parts = time_value.split(':')\n",
    "                h = int(parts[0]) if len(parts) > 0 and parts[0] else 0\n",
    "                m = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "                s = int(parts[2]) if len(parts) > 2 and parts[2] else 0\n",
    "                total_hours = h + m/60 + s/3600\n",
    "            \n",
    "            # Handle decimal format\n",
    "            elif '.' in time_value:\n",
    "                total_hours = float(time_value)\n",
    "            \n",
    "            # Handle pure number (assume hours)\n",
    "            else:\n",
    "                total_hours = float(time_value)\n",
    "        \n",
    "        # Handle numeric formats\n",
    "        else:\n",
    "            total_hours = float(time_value)\n",
    "        \n",
    "        # Round up to nearest whole hour\n",
    "        return math.ceil(total_hours)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error rounding hours for '{time_value}': {e}\")\n",
    "        return 0\n",
    "\n",
    "# -------------------- EXTRACT ABOUT COURSE --------------------\n",
    "def extract_about_course(soup):\n",
    "    \"\"\"Extract about course from course description sections\"\"\"\n",
    "    # Method 1: Look for course description in tab-pane overview\n",
    "    overview_tab = soup.find('div', class_='tab-pane overview-content active')\n",
    "    if overview_tab:\n",
    "        # Find the course description section\n",
    "        description_heading = overview_tab.find('h4', string=re.compile('Course description', re.IGNORECASE))\n",
    "        if description_heading:\n",
    "            # Get all paragraphs after the description heading\n",
    "            description_content = []\n",
    "            next_elem = description_heading.find_next_sibling()\n",
    "            while next_elem and next_elem.name != 'h4':\n",
    "                if next_elem.name == 'p':\n",
    "                    paragraph_text = next_elem.get_text(strip=True)\n",
    "                    if paragraph_text and paragraph_text not in ['', 'Tags']:\n",
    "                        description_content.append(paragraph_text)\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "            \n",
    "            if description_content:\n",
    "                return \" \".join(description_content)\n",
    "    \n",
    "    # Method 2: Look for general description in the content\n",
    "    description_keywords = ['course description', 'about this course', 'overview', 'introduction']\n",
    "    for keyword in description_keywords:\n",
    "        element = soup.find(string=re.compile(keyword, re.IGNORECASE))\n",
    "        if element:\n",
    "            parent = element.find_parent()\n",
    "            if parent:\n",
    "                # Get content after the heading\n",
    "                content_parts = []\n",
    "                next_sibling = parent.find_next_sibling()\n",
    "                while next_sibling and next_sibling.name in ['p', 'div']:\n",
    "                    text_content = next_sibling.get_text(strip=True)\n",
    "                    if text_content:\n",
    "                        content_parts.append(text_content)\n",
    "                    next_sibling = next_sibling.find_next_sibling()\n",
    "                \n",
    "                if content_parts:\n",
    "                    return \" \".join(content_parts)\n",
    "    \n",
    "    return \"About course not found\"\n",
    "\n",
    "# -------------------- EXTRACT PRICE --------------------\n",
    "def extract_price(soup):\n",
    "    \"\"\"Extract price from various price-related elements\"\"\"\n",
    "    # Method 1: Look for price-current class\n",
    "    price_current = soup.find('span', class_='price-current')\n",
    "    if price_current:\n",
    "        price_text = price_current.get_text(strip=True)\n",
    "        if price_text:\n",
    "            return price_text\n",
    "    \n",
    "    # Method 2: Look for bdi element (WordPress price)\n",
    "    price_bdi = soup.find('bdi')\n",
    "    if price_bdi:\n",
    "        price_text = price_bdi.get_text(strip=True)\n",
    "        if price_text:\n",
    "            return price_text\n",
    "    \n",
    "    # Method 3: Look for common price patterns\n",
    "    price_patterns = [\n",
    "        r'‚Çπ\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'$\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'‚Ç¨\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'¬£\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'\\d+[,\\d]*\\.?\\d*\\s*‚Çπ',\n",
    "        r'\\d+[,\\d]*\\.?\\d*\\s*$',\n",
    "    ]\n",
    "    \n",
    "    for pattern in price_patterns:\n",
    "        price_match = soup.find(string=re.compile(pattern))\n",
    "        if price_match:\n",
    "            return price_match.strip()\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT DURATION --------------------\n",
    "def extract_duration(soup):\n",
    "    \"\"\"Extract duration from course features format\"\"\"\n",
    "    # Method 1: Look for course-features-custom with duration\n",
    "    course_features = soup.find_all('div', class_='course-features-custom')\n",
    "    for feature in course_features:\n",
    "        duration_number = feature.find('div', class_='feature-custom-number')\n",
    "        feature_text = feature.find('div', class_='feature-custom-text')\n",
    "        \n",
    "        if duration_number and feature_text:\n",
    "            feature_text_text = feature_text.get_text(strip=True)\n",
    "            if feature_text_text and 'duration' in feature_text_text.lower():\n",
    "                duration_num_text = duration_number.get_text(strip=True)\n",
    "                if duration_num_text:\n",
    "                    return duration_num_text\n",
    "    \n",
    "    # Method 2: Look for duration in traditional format\n",
    "    about_sections = soup.find_all(\"p\")\n",
    "    for section in about_sections:\n",
    "        strong_tag = section.find(\"strong\")\n",
    "        if strong_tag and \"duration\" in strong_tag.get_text(strip=True).lower():\n",
    "            duration_text = clean_text(section.get_text(strip=True))\n",
    "            if duration_text:\n",
    "                return duration_text\n",
    "    \n",
    "    # Method 3: Look for common duration patterns\n",
    "    duration_patterns = [\n",
    "        r'\\d+\\s*(months?|weeks?|days?|hours?|minutes?)',\n",
    "        r'\\d+:\\d+:\\d+',  # HH:MM:SS format\n",
    "        r'\\d+:\\d+',      # HH:MM format\n",
    "    ]\n",
    "    \n",
    "    for pattern in duration_patterns:\n",
    "        duration_match = soup.find(string=re.compile(pattern, re.IGNORECASE))\n",
    "        if duration_match:\n",
    "            return duration_match.strip()\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT LANGUAGE --------------------\n",
    "def extract_language(soup):\n",
    "    \"\"\"Extract language information from the page\"\"\"\n",
    "    # Method 1: Look for language text with language icon\n",
    "    language_icon = soup.find('i', class_='fa-language')\n",
    "    if language_icon:\n",
    "        language_parent = language_icon.find_parent('p')\n",
    "        if language_parent:\n",
    "            language_text = language_parent.get_text(strip=True)\n",
    "            # Remove the icon text and keep only the language info\n",
    "            language_text = re.sub(r'.*Language', '', language_text).strip()\n",
    "            if language_text:\n",
    "                return language_text\n",
    "    \n",
    "    # Method 2: Look for common languages in the text\n",
    "    languages = ['Hindi', 'English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese']\n",
    "    for lang in languages:\n",
    "        lang_match = soup.find(string=re.compile(lang, re.IGNORECASE))\n",
    "        if lang_match:\n",
    "            return lang\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT SYLLABUS FROM BUTTONS --------------------\n",
    "def extract_syllabus_from_buttons(soup):\n",
    "    \"\"\"Extract syllabus content from button elements with video classes\"\"\"\n",
    "    syllabus_content = []\n",
    "    \n",
    "    # Find all buttons with video-related classes\n",
    "    syllabus_buttons = soup.find_all('a', class_=re.compile(r'button.*video|video.*button', re.IGNORECASE))\n",
    "    \n",
    "    for button in syllabus_buttons:\n",
    "        button_text = button.get_text(strip=True)\n",
    "        if button_text and button_text not in ['', 'Play', 'Watch']:\n",
    "            # Clean the text and add to syllabus\n",
    "            clean_button_text = clean_text(button_text)\n",
    "            if clean_button_text:\n",
    "                syllabus_content.append(f\"- {clean_button_text}\")\n",
    "    \n",
    "    # Also look for elements with play icons\n",
    "    play_icons = soup.find_all('i', class_=re.compile(r'fa-play|fa-video', re.IGNORECASE))\n",
    "    for icon in play_icons:\n",
    "        parent_text = icon.find_parent().get_text(strip=True)\n",
    "        if parent_text:\n",
    "            clean_parent_text = clean_text(parent_text.replace('Play', '').replace('Watch', '').strip())\n",
    "            if clean_parent_text and clean_parent_text not in [item.replace('- ', '') for item in syllabus_content]:\n",
    "                syllabus_content.append(f\"- {clean_parent_text}\")\n",
    "    \n",
    "    return \"\\n\".join(syllabus_content) if syllabus_content else \"Not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name - UPDATED to target h5 with mb-2 class\n",
    "        course_name_tag = soup.find(\"h5\", class_=\"mb-2\")\n",
    "        if not course_name_tag:\n",
    "            # Fallback methods if h5.mb-2 not found\n",
    "            course_name_tag = soup.find(\"h1\", class_=re.compile(r\"product_title|entry-title|title\", re.IGNORECASE))\n",
    "            if not course_name_tag:\n",
    "                course_name_tag = soup.find(\"h1\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. Language\n",
    "        language = extract_language(soup)\n",
    "        print(f\"üó£Ô∏è Language: {language}\")\n",
    "\n",
    "        # 3. About Course - UPDATED\n",
    "        about_course = extract_about_course(soup)\n",
    "        print(f\"üìù About Course: {about_course[:80]}...\")\n",
    "\n",
    "        # 4. Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        about_sections = soup.find_all(\"p\")\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"eligibility\" in strong_tag.get_text(strip=True).lower():\n",
    "                eligibility = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚úÖ Eligibility: {eligibility}\")\n",
    "\n",
    "        # 5. Duration - UPDATED to use new extraction method\n",
    "        duration = extract_duration(soup)\n",
    "        print(f\"‚è±Ô∏è Duration: {duration}\")\n",
    "\n",
    "        # 6. Price - UPDATED\n",
    "        price = extract_price(soup)\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # 7. Who Should Take It\n",
    "        who_content = []\n",
    "        who_strong = soup.find(\"strong\", string=re.compile(r\"Who Should Enroll|Who Should Take It\", re.IGNORECASE))\n",
    "        if who_strong:\n",
    "            next_ul = who_strong.find_next(\"ul\")\n",
    "            if next_ul:\n",
    "                for li in next_ul.find_all(\"li\"):\n",
    "                    who_content.append(clean_text(li.get_text(strip=True)))\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "        print(f\"üë• Who Should Take It: {who_should_take}\")\n",
    "\n",
    "        # 8. Syllabus extraction - From buttons only\n",
    "        syllabus_buttons = extract_syllabus_from_buttons(soup)\n",
    "        print(f\"üìö Syllabus: {syllabus_buttons[:80]}...\")\n",
    "\n",
    "        # 9. Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        # Method 1: Look for certificate images\n",
    "        cert_imgs = soup.find_all('img', src=re.compile(r'certificate', re.IGNORECASE))\n",
    "        for cert_img in cert_imgs:\n",
    "            if cert_img.has_attr('src'):\n",
    "                cert_link = cert_img['src']\n",
    "                break\n",
    "        \n",
    "        # Method 2: Look for PDF links\n",
    "        if cert_link == \"Certificate not available\":\n",
    "            cert_tag = soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "            if cert_tag and cert_tag.has_attr(\"href\"):\n",
    "                cert_link = cert_tag[\"href\"]\n",
    "        \n",
    "        print(f\"üìú Certificate: {cert_link}\")\n",
    "\n",
    "        return (course_name, language, about_course, eligibility, \n",
    "                duration, price, who_should_take, syllabus_buttons, cert_link)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path, url):\n",
    "    columns = [\n",
    "        \"Course Name\",\n",
    "        \"Language\", \n",
    "        \"About Course\",\n",
    "        \"Eligibility\",\n",
    "        \"Duration\", \n",
    "        \"Price\",\n",
    "        \"Who Should Take It\",\n",
    "        \"Syllabus\",\n",
    "        \"Certificate\",\n",
    "        \"Round Hours\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "    \n",
    "    (course_name, language, about_course, eligibility, \n",
    "     duration, price, who_should_take, syllabus_buttons, cert_link) = data\n",
    "     \n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Calculate rounded hours\n",
    "        rounded_hours = roundup_hours_simple(duration)\n",
    "        print(f\"‚è∞ Rounded Hours: {rounded_hours}\")\n",
    "\n",
    "        # ‚úÖ Avoid duplicate entries\n",
    "        if url not in df[\"Course URL\"].values:\n",
    "            row = {\n",
    "                \"Course Name\": course_name,\n",
    "                \"Language\": language,\n",
    "                \"About Course\": about_course,\n",
    "                \"Eligibility\": eligibility,\n",
    "                \"Duration\": duration,\n",
    "                \"Price\": price,\n",
    "                \"Who Should Take It\": who_should_take,\n",
    "                \"Syllabus\": syllabus_buttons,\n",
    "                \"Certificate\": cert_link,\n",
    "                \"Round Hours\": rounded_hours,\n",
    "                \"Course URL\": url\n",
    "            }\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "            df.to_excel(file_path, index=False)\n",
    "            print(f\"üíæ Saved data for: {course_name}\")\n",
    "            \n",
    "            # Print results for verification\n",
    "            print(\"\\nüìä Results:\")\n",
    "            print(df[['Course Name', 'Duration', 'Round Hours']].tail())\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped duplicate: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "        \"https://iisdt.com/course/big-data-analytics-hadoop-college\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\cours1.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüîç Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, file_path, course_url)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c1b14",
   "metadata": {},
   "source": [
    "Entire excel file  https://iisdt.com/course/starting-e-commerce-business-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dff3d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üîç Processing: https://iisdt.com/course/starting-e-commerce-business-from-scratch\n",
      "üåê Accessing URL: https://iisdt.com/course/starting-e-commerce-business-from-scratch\n",
      "üìõ Course Name: Starting E-Commerce Business From Scratch\n",
      "üó£Ô∏è Language: Hindi\n",
      "üìù About Course: If you think of an item and type the word into your web browser, you will mostly...\n",
      "‚úÖ Eligibility: Not available\n",
      "‚è±Ô∏è Duration: 6:44:06\n",
      "üí∞ Price: ‚Çπ2,500.00\n",
      "üë• Who Should Take It: Not available\n",
      "üìö Syllabus: - Introduction to the Program | Nishkarsh Sharma\n",
      "- Introduction to e-Commerce\n",
      "- ...\n",
      "üìú Certificate: https://iisdt.com/public/uploads/b2b_certificate/demo_certificate/27.png\n",
      "üö™ Browser closed\n",
      "‚è∞ Rounded Hours: 7\n",
      "üíæ Saved data for: Starting E-Commerce Business From Scratch\n",
      "\n",
      "üìä Results:\n",
      "                                 Course Name Duration Round Hours\n",
      "0  Starting E-Commerce Business From Scratch  6:44:06           7\n",
      "\n",
      "üîç Processing: https://iisdt.com/course/jewellery-crafting-handmade-2\n",
      "üåê Accessing URL: https://iisdt.com/course/jewellery-crafting-handmade-2\n",
      "üìõ Course Name: Jewellery Crafting (Handmade)\n",
      "üó£Ô∏è Language: English\n",
      "üìù About Course: Handmade jewelry (or handmade jewelry or handcrafted jewelry) is jewelry that ha...\n",
      "‚úÖ Eligibility: Not available\n",
      "‚è±Ô∏è Duration: 0:53:00\n",
      "üí∞ Price: ‚Çπ3,000.00\n",
      "üë• Who Should Take It: Not available\n",
      "üìö Syllabus: - Handmade jewellery Industry & Career opportunities\n",
      "- Industry Overview & It's ...\n",
      "üìú Certificate: https://iisdt.com/public/uploads/b2b_certificate/demo_certificate/27.png\n",
      "üö™ Browser closed\n",
      "‚è∞ Rounded Hours: 1\n",
      "üíæ Saved data for: Jewellery Crafting (Handmade)\n",
      "\n",
      "üìä Results:\n",
      "                                 Course Name Duration  Round Hours\n",
      "0  Starting E-Commerce Business From Scratch  6:44:06            7\n",
      "1              Jewellery Crafting (Handmade)  0:53:00            1\n",
      "\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")  # modern headless\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specified words from text\"\"\"\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- ROUND HOURS FUNCTION --------------------\n",
    "def roundup_hours_simple(time_value):\n",
    "    \"\"\"Simple function to handle all time formats\"\"\"\n",
    "    if pd.isna(time_value) or time_value == \"Not available\" or not time_value:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # Handle string formats\n",
    "        if isinstance(time_value, str):\n",
    "            # Remove any non-numeric characters except colons and decimal points\n",
    "            time_value = re.sub(r'[^\\d:.]', '', time_value)\n",
    "            \n",
    "            # Handle HH:MM:SS format (like \"0:53:00\")\n",
    "            if ':' in time_value:\n",
    "                parts = time_value.split(':')\n",
    "                h = int(parts[0]) if len(parts) > 0 and parts[0] else 0\n",
    "                m = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "                s = int(parts[2]) if len(parts) > 2 and parts[2] else 0\n",
    "                total_hours = h + m/60 + s/3600\n",
    "            \n",
    "            # Handle decimal format\n",
    "            elif '.' in time_value:\n",
    "                total_hours = float(time_value)\n",
    "            \n",
    "            # Handle pure number (assume hours)\n",
    "            else:\n",
    "                total_hours = float(time_value)\n",
    "        \n",
    "        # Handle numeric formats\n",
    "        else:\n",
    "            total_hours = float(time_value)\n",
    "        \n",
    "        # Round up to nearest whole hour\n",
    "        return math.ceil(total_hours)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error rounding hours for '{time_value}': {e}\")\n",
    "        return 0\n",
    "\n",
    "# -------------------- EXTRACT ABOUT COURSE --------------------\n",
    "def extract_about_course(soup):\n",
    "    \"\"\"Extract about course from course description sections\"\"\"\n",
    "    # Method 1: Look for course description in tab-pane overview\n",
    "    overview_tab = soup.find('div', class_='tab-pane overview-content active')\n",
    "    if overview_tab:\n",
    "        # Find the course description section\n",
    "        description_heading = overview_tab.find('h4', string=re.compile('Course description', re.IGNORECASE))\n",
    "        if description_heading:\n",
    "            # Get all paragraphs after the description heading\n",
    "            description_content = []\n",
    "            next_elem = description_heading.find_next_sibling()\n",
    "            while next_elem and next_elem.name != 'h4':\n",
    "                if next_elem.name == 'p':\n",
    "                    paragraph_text = next_elem.get_text(strip=True)\n",
    "                    if paragraph_text and paragraph_text not in ['', 'Tags']:\n",
    "                        description_content.append(paragraph_text)\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "            \n",
    "            if description_content:\n",
    "                return \" \".join(description_content)\n",
    "    \n",
    "    # Method 2: Look for general description in the content\n",
    "    description_keywords = ['course description', 'about this course', 'overview', 'introduction']\n",
    "    for keyword in description_keywords:\n",
    "        element = soup.find(string=re.compile(keyword, re.IGNORECASE))\n",
    "        if element:\n",
    "            parent = element.find_parent()\n",
    "            if parent:\n",
    "                # Get content after the heading\n",
    "                content_parts = []\n",
    "                next_sibling = parent.find_next_sibling()\n",
    "                while next_sibling and next_sibling.name in ['p', 'div']:\n",
    "                    text_content = next_sibling.get_text(strip=True)\n",
    "                    if text_content:\n",
    "                        content_parts.append(text_content)\n",
    "                    next_sibling = next_sibling.find_next_sibling()\n",
    "                \n",
    "                if content_parts:\n",
    "                    return \" \".join(content_parts)\n",
    "    \n",
    "    return \"About course not found\"\n",
    "\n",
    "# -------------------- EXTRACT PRICE --------------------\n",
    "def extract_price(soup):\n",
    "    \"\"\"Extract price from various price-related elements\"\"\"\n",
    "    # Method 1: Look for price-current class\n",
    "    price_current = soup.find('span', class_='price-current')\n",
    "    if price_current:\n",
    "        price_text = price_current.get_text(strip=True)\n",
    "        if price_text:\n",
    "            return price_text\n",
    "    \n",
    "    # Method 2: Look for bdi element (WordPress price)\n",
    "    price_bdi = soup.find('bdi')\n",
    "    if price_bdi:\n",
    "        price_text = price_bdi.get_text(strip=True)\n",
    "        if price_text:\n",
    "            return price_text\n",
    "    \n",
    "    # Method 3: Look for common price patterns\n",
    "    price_patterns = [\n",
    "        r'‚Çπ\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'$\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'‚Ç¨\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'¬£\\s*\\d+[,\\d]*\\.?\\d*',\n",
    "        r'\\d+[,\\d]*\\.?\\d*\\s*‚Çπ',\n",
    "        r'\\d+[,\\d]*\\.?\\d*\\s*$',\n",
    "    ]\n",
    "    \n",
    "    for pattern in price_patterns:\n",
    "        price_match = soup.find(string=re.compile(pattern))\n",
    "        if price_match:\n",
    "            return price_match.strip()\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT DURATION --------------------\n",
    "def extract_duration(soup):\n",
    "    \"\"\"Extract duration from course features format\"\"\"\n",
    "    # Method 1: Look for course-features-custom with duration\n",
    "    course_features = soup.find_all('div', class_='course-features-custom')\n",
    "    for feature in course_features:\n",
    "        duration_number = feature.find('div', class_='feature-custom-number')\n",
    "        feature_text = feature.find('div', class_='feature-custom-text')\n",
    "        \n",
    "        if duration_number and feature_text:\n",
    "            feature_text_text = feature_text.get_text(strip=True)\n",
    "            if feature_text_text and 'duration' in feature_text_text.lower():\n",
    "                duration_num_text = duration_number.get_text(strip=True)\n",
    "                if duration_num_text:\n",
    "                    return duration_num_text\n",
    "    \n",
    "    # Method 2: Look for duration in traditional format\n",
    "    about_sections = soup.find_all(\"p\")\n",
    "    for section in about_sections:\n",
    "        strong_tag = section.find(\"strong\")\n",
    "        if strong_tag and \"duration\" in strong_tag.get_text(strip=True).lower():\n",
    "            duration_text = clean_text(section.get_text(strip=True))\n",
    "            if duration_text:\n",
    "                return duration_text\n",
    "    \n",
    "    # Method 3: Look for common duration patterns\n",
    "    duration_patterns = [\n",
    "        r'\\d+\\s*(months?|weeks?|days?|hours?|minutes?)',\n",
    "        r'\\d+:\\d+:\\d+',  # HH:MM:SS format\n",
    "        r'\\d+:\\d+',      # HH:MM format\n",
    "    ]\n",
    "    \n",
    "    for pattern in duration_patterns:\n",
    "        duration_match = soup.find(string=re.compile(pattern, re.IGNORECASE))\n",
    "        if duration_match:\n",
    "            return duration_match.strip()\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT LANGUAGE --------------------\n",
    "def extract_language(soup):\n",
    "    \"\"\"Extract language information from the page\"\"\"\n",
    "    # Method 1: Look for language text with language icon\n",
    "    language_icon = soup.find('i', class_='fa-language')\n",
    "    if language_icon:\n",
    "        language_parent = language_icon.find_parent('p')\n",
    "        if language_parent:\n",
    "            language_text = language_parent.get_text(strip=True)\n",
    "            # Remove the icon text and keep only the language info\n",
    "            language_text = re.sub(r'.*Language', '', language_text).strip()\n",
    "            if language_text:\n",
    "                return language_text\n",
    "    \n",
    "    # Method 2: Look for common languages in the text\n",
    "    languages = ['Hindi', 'English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese']\n",
    "    for lang in languages:\n",
    "        lang_match = soup.find(string=re.compile(lang, re.IGNORECASE))\n",
    "        if lang_match:\n",
    "            return lang\n",
    "    \n",
    "    return \"Not available\"\n",
    "\n",
    "# -------------------- EXTRACT SYLLABUS FROM BUTTONS --------------------\n",
    "def extract_syllabus_from_buttons(soup):\n",
    "    \"\"\"Extract syllabus content from button elements with video classes\"\"\"\n",
    "    syllabus_content = []\n",
    "    \n",
    "    # Find all buttons with video-related classes\n",
    "    syllabus_buttons = soup.find_all('a', class_=re.compile(r'button.*video|video.*button', re.IGNORECASE))\n",
    "    \n",
    "    for button in syllabus_buttons:\n",
    "        button_text = button.get_text(strip=True)\n",
    "        if button_text and button_text not in ['', 'Play', 'Watch']:\n",
    "            # Clean the text and add to syllabus\n",
    "            clean_button_text = clean_text(button_text)\n",
    "            if clean_button_text:\n",
    "                syllabus_content.append(f\"- {clean_button_text}\")\n",
    "    \n",
    "    # Also look for elements with play icons\n",
    "    play_icons = soup.find_all('i', class_=re.compile(r'fa-play|fa-video', re.IGNORECASE))\n",
    "    for icon in play_icons:\n",
    "        parent_text = icon.find_parent().get_text(strip=True)\n",
    "        if parent_text:\n",
    "            clean_parent_text = clean_text(parent_text.replace('Play', '').replace('Watch', '').strip())\n",
    "            if clean_parent_text and clean_parent_text not in [item.replace('- ', '') for item in syllabus_content]:\n",
    "                syllabus_content.append(f\"- {clean_parent_text}\")\n",
    "    \n",
    "    return \"\\n\".join(syllabus_content) if syllabus_content else \"Not available\"\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 1. Course Name - UPDATED to target h5 with mb-2 class\n",
    "        course_name_tag = soup.find(\"h5\", class_=\"mb-2\")\n",
    "        if not course_name_tag:\n",
    "            # Fallback methods if h5.mb-2 not found\n",
    "            course_name_tag = soup.find(\"h1\", class_=re.compile(r\"product_title|entry-title|title\", re.IGNORECASE))\n",
    "            if not course_name_tag:\n",
    "                course_name_tag = soup.find(\"h1\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # 2. Language\n",
    "        language = extract_language(soup)\n",
    "        print(f\"üó£Ô∏è Language: {language}\")\n",
    "\n",
    "        # 3. About Course - UPDATED\n",
    "        about_course = extract_about_course(soup)\n",
    "        print(f\"üìù About Course: {about_course[:80]}...\")\n",
    "\n",
    "        # 4. Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        about_sections = soup.find_all(\"p\")\n",
    "        for section in about_sections:\n",
    "            strong_tag = section.find(\"strong\")\n",
    "            if strong_tag and \"eligibility\" in strong_tag.get_text(strip=True).lower():\n",
    "                eligibility = clean_text(section.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚úÖ Eligibility: {eligibility}\")\n",
    "\n",
    "        # 5. Duration - UPDATED to use new extraction method\n",
    "        duration = extract_duration(soup)\n",
    "        print(f\"‚è±Ô∏è Duration: {duration}\")\n",
    "\n",
    "        # 6. Price - UPDATED\n",
    "        price = extract_price(soup)\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        # 7. Who Should Take It\n",
    "        who_content = []\n",
    "        who_strong = soup.find(\"strong\", string=re.compile(r\"Who Should Enroll|Who Should Take It\", re.IGNORECASE))\n",
    "        if who_strong:\n",
    "            next_ul = who_strong.find_next(\"ul\")\n",
    "            if next_ul:\n",
    "                for li in next_ul.find_all(\"li\"):\n",
    "                    who_content.append(clean_text(li.get_text(strip=True)))\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "        print(f\"üë• Who Should Take It: {who_should_take}\")\n",
    "\n",
    "        # 8. Syllabus extraction - From buttons only\n",
    "        syllabus_buttons = extract_syllabus_from_buttons(soup)\n",
    "        print(f\"üìö Syllabus: {syllabus_buttons[:80]}...\")\n",
    "\n",
    "        # 9. Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        # Method 1: Look for certificate images\n",
    "        cert_imgs = soup.find_all('img', src=re.compile(r'certificate', re.IGNORECASE))\n",
    "        for cert_img in cert_imgs:\n",
    "            if cert_img.has_attr('src'):\n",
    "                cert_link = cert_img['src']\n",
    "                break\n",
    "        \n",
    "        # Method 2: Look for PDF links\n",
    "        if cert_link == \"Certificate not available\":\n",
    "            cert_tag = soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "            if cert_tag and cert_tag.has_attr(\"href\"):\n",
    "                cert_link = cert_tag[\"href\"]\n",
    "        \n",
    "        print(f\"üìú Certificate: {cert_link}\")\n",
    "\n",
    "        return (course_name, language, about_course, eligibility, \n",
    "                duration, price, who_should_take, syllabus_buttons, cert_link)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path, url):\n",
    "    columns = [\n",
    "        \"Course Name\",\n",
    "        \"Language\", \n",
    "        \"About Course\",\n",
    "        \"Eligibility\",\n",
    "        \"Duration\", \n",
    "        \"Price\",\n",
    "        \"Who Should Take It\",\n",
    "        \"Syllabus\",\n",
    "        \"Certificate\",\n",
    "        \"Round Hours\",\n",
    "        \"Course URL\"\n",
    "    ]\n",
    "    \n",
    "    (course_name, language, about_course, eligibility, \n",
    "     duration, price, who_should_take, syllabus_buttons, cert_link) = data\n",
    "     \n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Calculate rounded hours\n",
    "        rounded_hours = roundup_hours_simple(duration)\n",
    "        print(f\"‚è∞ Rounded Hours: {rounded_hours}\")\n",
    "\n",
    "        # ‚úÖ Avoid duplicate entries\n",
    "        if url not in df[\"Course URL\"].values:\n",
    "            row = {\n",
    "                \"Course Name\": course_name,\n",
    "                \"Language\": language,\n",
    "                \"About Course\": about_course,\n",
    "                \"Eligibility\": eligibility,\n",
    "                \"Duration\": duration,\n",
    "                \"Price\": price,\n",
    "                \"Who Should Take It\": who_should_take,\n",
    "                \"Syllabus\": syllabus_buttons,\n",
    "                \"Certificate\": cert_link,\n",
    "                \"Round Hours\": rounded_hours,\n",
    "                \"Course URL\": url\n",
    "            }\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "            df.to_excel(file_path, index=False)\n",
    "            print(f\"üíæ Saved data for: {course_name}\")\n",
    "            \n",
    "            # Print results for verification\n",
    "            print(\"\\nüìä Results:\")\n",
    "            print(df[['Course Name', 'Duration', 'Round Hours']].tail())\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped duplicate: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    input_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\course_duration_data.xlsx\"\n",
    "    output_file = r\"C:\\Users\\taslim.siddiqui\\Downloads\\surewin_output.xlsx\"\n",
    "    try:\n",
    "        df = pd.read_excel(input_file)\n",
    "        if \"Course Link\" not in df.columns:\n",
    "            raise ValueError(\"‚ùå Excel must contain a 'Course Link' column\")\n",
    "        course_urls = df[\"Course Link\"].dropna().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read input file: {e}\")\n",
    "        course_urls = []\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüîç Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, output_file, course_url)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "    print(\"\\n‚úÖ Process completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
