{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Fast Product Scanner for Skill Council India\n",
    "# ----------------------------------------------------\n",
    "def fast_scan_skillcouncil():\n",
    "    base_url = \"https://skillcouncil.in\"\n",
    "    all_courses = set()\n",
    "    \n",
    "    print(\"ğŸš€ FAST Scanning Skill Council India for courses...\")\n",
    "    \n",
    "    # Test website accessibility\n",
    "    try:\n",
    "        response = requests.get(base_url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ Website not accessible. Status code: {response.status_code}\")\n",
    "            return []\n",
    "        print(\"âœ… Website is accessible\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cannot connect to website: {e}\")\n",
    "        return []\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "\n",
    "    # Direct product listing pages (common in WooCommerce sites)\n",
    "    product_pages = [\n",
    "        f\"{base_url}/shop/\",\n",
    "        f\"{base_url}/products/\",\n",
    "        f\"{base_url}/courses/\",\n",
    "        f\"{base_url}/online-courses/\",\n",
    "        f\"{base_url}/all-courses/\",\n",
    "        f\"{base_url}/certification-courses/\",\n",
    "    ]\n",
    "\n",
    "    # Add category pages directly\n",
    "    categories = [\n",
    "        \"banking\", \"accounting\", \"finance\", \"administration\", \"clerical\",\n",
    "        \"management\", \"computer\", \"it\", \"information-technology\", \n",
    "        \"education\", \"training\", \"business\", \"construction\", \"civil\",\n",
    "        \"hotel\", \"tourism\", \"hospitality\", \"healthcare\", \"fitness\", \"medical\",\n",
    "        \"art\", \"design\", \"journalism\", \"media\", \"publishing\",\n",
    "        \"transportation\", \"logistics\", \"supply-chain\", \"fire\", \"safety\",\n",
    "        \"apparel\", \"textile\", \"fashion\", \"yoga\", \"meditation\", \"gsdci\"\n",
    "    ]\n",
    "\n",
    "    for category in categories:\n",
    "        product_pages.append(f\"{base_url}/product-category/{category}/\")\n",
    "        product_pages.append(f\"{base_url}/category/{category}/\")\n",
    "\n",
    "    def scan_product_page(page_url):\n",
    "        try:\n",
    "            print(f\"ğŸ” Scanning: {page_url}\")\n",
    "            response = session.get(page_url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Find product links - common WooCommerce selectors\n",
    "                product_selectors = [\n",
    "                    'a.woocommerce-LoopProduct-link',\n",
    "                    'a[href*=\"/product/\"]',\n",
    "                    '.product a[href]',\n",
    "                    '.course a[href]',\n",
    "                    '.product-item a[href]',\n",
    "                    'li.product a'\n",
    "                ]\n",
    "                \n",
    "                for selector in product_selectors:\n",
    "                    for link in soup.select(selector):\n",
    "                        href = link.get('href', '')\n",
    "                        if href and '/product/' in href:\n",
    "                            full_url = urljoin(base_url, href)\n",
    "                            clean_url = full_url.split('?')[0].split('#')[0]\n",
    "                            if clean_url not in all_courses:\n",
    "                                all_courses.add(clean_url)\n",
    "                                print(f\"   âœ… Found: {clean_url}\")\n",
    "                \n",
    "                # Check for pagination\n",
    "                next_links = soup.select('a.next, a.page-numbers:not(.current)')\n",
    "                for next_link in next_links:\n",
    "                    href = next_link.get('href', '')\n",
    "                    if href and 'page' in href.lower():\n",
    "                        full_next_url = urljoin(base_url, href)\n",
    "                        if full_next_url not in product_pages:\n",
    "                            product_pages.append(full_next_url)\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error scanning {page_url}: {e}\")\n",
    "    \n",
    "    # Scan all product pages\n",
    "    for page in product_pages:\n",
    "        scan_product_page(page)\n",
    "        time.sleep(0.5)  # Small delay to be respectful\n",
    "\n",
    "    print(f\"ğŸ“Š Total course URLs found: {len(all_courses)}\")\n",
    "    return list(all_courses)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Fast Course Details Extractor\n",
    "# ----------------------------------------------------\n",
    "def fast_get_course_details(course_urls, output_file):\n",
    "    if not course_urls:\n",
    "        print(\"âŒ No course URLs to process\")\n",
    "        return []\n",
    "\n",
    "    print(f\"ğŸ”„ Fast processing {len(course_urls)} courses...\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "\n",
    "    all_courses = []\n",
    "\n",
    "    def extract_course_info(url):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Quick title extraction\n",
    "                title_selectors = [\n",
    "                    'h1.product_title',\n",
    "                    'h1.entry-title', \n",
    "                    'h1.title',\n",
    "                    'h1',\n",
    "                    'title'\n",
    "                ]\n",
    "                \n",
    "                course_name = None\n",
    "                for selector in title_selectors:\n",
    "                    el = soup.select_one(selector)\n",
    "                    if el:\n",
    "                        course_name = el.get_text(strip=True)\n",
    "                        if course_name and len(course_name) > 3:\n",
    "                            break\n",
    "                \n",
    "                if not course_name:\n",
    "                    course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "                    course_name = re.sub(r'\\s+', ' ', course_name).strip()\n",
    "                \n",
    "                return {\n",
    "                    'course_name': course_name,\n",
    "                    'course_link': url\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error extracting {url}: {e}\")\n",
    "        \n",
    "        # Fallback\n",
    "        course_name = url.split('/')[-2].replace('-', ' ').title()\n",
    "        return {\n",
    "            'course_name': course_name,\n",
    "            'course_link': url\n",
    "        }\n",
    "\n",
    "    # Process in smaller batches for better performance\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(course_urls), batch_size):\n",
    "        batch = course_urls[i:i + batch_size]\n",
    "        print(f\"ğŸ“– Processing batch {i//batch_size + 1}/{(len(course_urls)-1)//batch_size + 1}\")\n",
    "        \n",
    "        for url in batch:\n",
    "            course_data = extract_course_info(url)\n",
    "            if course_data:\n",
    "                all_courses.append(course_data)\n",
    "                print(f\"âœ… {course_data['course_name']}\")\n",
    "        \n",
    "        # Save progress after each batch\n",
    "        try:\n",
    "            df = pd.DataFrame(all_courses)\n",
    "            df = df[['course_name', 'course_link']]\n",
    "            df.to_excel(output_file, index=False)\n",
    "            print(f\"ğŸ’¾ Saved {len(all_courses)} courses so far...\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error saving: {e}\")\n",
    "        \n",
    "        time.sleep(1)  # Small delay between batches\n",
    "\n",
    "    return all_courses\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Ultra Fast Main Function\n",
    "# ----------------------------------------------------\n",
    "def ultra_fast_main():\n",
    "    print(\"ğŸš€ ULTRA FAST Skill Council Scraper\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\skillcouncil_courses_fast.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Fast scanning\n",
    "        print(\"ğŸ¯ Step 1: Fast course discovery...\")\n",
    "        course_urls = fast_scan_skillcouncil()\n",
    "        \n",
    "        if not course_urls:\n",
    "            print(\"âŒ No courses found!\")\n",
    "            return\n",
    "            \n",
    "        print(f\"âœ… Found {len(course_urls)} course URLs\")\n",
    "        \n",
    "        # Step 2: Quick extraction\n",
    "        print(\"ğŸ¯ Step 2: Fast course extraction...\")\n",
    "        courses = fast_get_course_details(course_urls, save_path)\n",
    "        \n",
    "        # Final results\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if courses:\n",
    "            # Remove duplicates\n",
    "            unique_courses = []\n",
    "            seen_urls = set()\n",
    "            \n",
    "            for course in courses:\n",
    "                clean_url = course['course_link'].rstrip('/')\n",
    "                if clean_url not in seen_urls:\n",
    "                    seen_urls.add(clean_url)\n",
    "                    unique_courses.append(course)\n",
    "            \n",
    "            # Final save\n",
    "            df_final = pd.DataFrame(unique_courses)\n",
    "            df_final = df_final[['course_name', 'course_link']]\n",
    "            df_final.to_excel(save_path, index=False)\n",
    "            \n",
    "            print(f\"\\nğŸ‰ COMPLETED IN {end_time - start_time:.1f} SECONDS!\")\n",
    "            print(f\"ğŸ“Š Final Results: {len(unique_courses)} unique courses\")\n",
    "            print(f\"ğŸ’¾ Saved to: {save_path}\")\n",
    "            \n",
    "            # Show sample\n",
    "            print(f\"\\nğŸ“‹ SAMPLE (first 5 courses):\")\n",
    "            print(\"=\" * 50)\n",
    "            for i, course in enumerate(unique_courses[:5], 1):\n",
    "                print(f\"{i}. {course['course_name']}\")\n",
    "                print(f\"   {course['course_link']}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "        else:\n",
    "            print(\"âŒ No courses extracted\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ Stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# SUPER FAST VERSION - Minimal scanning\n",
    "# ----------------------------------------------------\n",
    "def super_fast_version():\n",
    "    \"\"\"Even faster version - scans only main product pages\"\"\"\n",
    "    print(\"âš¡ SUPER FAST Version - Scanning main pages only\")\n",
    "    \n",
    "    base_url = \"https://skillcouncil.in\"\n",
    "    all_courses = set()\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    })\n",
    "    \n",
    "    # Only scan these key pages\n",
    "    key_pages = [\n",
    "        f\"{base_url}/shop/\",\n",
    "        f\"{base_url}/products/\", \n",
    "        f\"{base_url}/courses/\",\n",
    "        f\"{base_url}/online-courses/\",\n",
    "    ]\n",
    "    \n",
    "    for page in key_pages:\n",
    "        try:\n",
    "            print(f\"ğŸ” Scanning: {page}\")\n",
    "            response = session.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Get all links with /product/ in them\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if '/product/' in href and 'surewinindia.com' not in href:\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    clean_url = full_url.split('?')[0].split('#')[0]\n",
    "                    all_courses.add(clean_url)\n",
    "                    print(f\"âœ… Found: {clean_url}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error: {e}\")\n",
    "    \n",
    "    course_list = list(all_courses)\n",
    "    print(f\"ğŸ“Š Found {len(course_list)} courses\")\n",
    "    \n",
    "    # Quick extraction\n",
    "    courses_data = []\n",
    "    for url in course_list:\n",
    "        try:\n",
    "            response = session.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            title_el = soup.find('h1') or soup.find('title')\n",
    "            name = title_el.get_text(strip=True) if title_el else url.split('/')[-2].replace('-', ' ').title()\n",
    "            courses_data.append({'course_name': name, 'course_link': url})\n",
    "            print(f\"âœ… Extracted: {name}\")\n",
    "        except:\n",
    "            name = url.split('/')[-2].replace('-', ' ').title()\n",
    "            courses_data.append({'course_name': name, 'course_link': url})\n",
    "    \n",
    "    # Save\n",
    "    df = pd.DataFrame(courses_data)\n",
    "    df = df[['course_name', 'course_link']]\n",
    "    save_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\skillcouncil_courses_super_fast.xlsx\"\n",
    "    df.to_excel(save_path, index=False)\n",
    "    \n",
    "    print(f\"ğŸ‰ DONE! Saved {len(courses_data)} courses to {save_path}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Choose which version to run\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose scanning speed:\")\n",
    "    print(\"1. Ultra Fast (Recommended)\")\n",
    "    print(\"2. Super Fast (Quickest)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        super_fast_version()\n",
    "    else:\n",
    "        ultra_fast_main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
