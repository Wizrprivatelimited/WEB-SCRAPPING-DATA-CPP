{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7238a76d",
   "metadata": {},
   "source": [
    "Test for one course "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aab623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting scraping process...\n",
      "\n",
      "üîç Processing: https://skillcouncil.in/product/certified-certified-forklift-operator/\n",
      "üåê Accessing URL: https://skillcouncil.in/product/certified-certified-forklift-operator/\n",
      "üìë Clicking on tab: Description\n",
      "üìõ Course Name: Certified Certified Forklift Operator\n",
      "üìù About Course: The Certified Forklift Operator course is designed to equip participants with the knowledge and hand...\n",
      "‚úÖ Eligibility: Graduation or Equivalent is required.\n",
      "‚è±Ô∏è Duration: One Month.\n",
      "üí∞ Price: ‚Çπ6,000\n",
      "üí≥ Fee Structure: ‚Çπ6,000 \n",
      "- All other fees remain unchanged\n",
      "- Education loans are available through leading banks and NBFCs.\n",
      "üë• Who Should Take It: Not available\n",
      "üîç Searching for syllabus modules...\n",
      "‚úÖ Found module: Module 1: Introduction to Forklift Operations and ...\n",
      "‚úÖ Found module: Module 2: Pre-Operational Checks and Safety Inspec...\n",
      "‚úÖ Found module: Module 3: Safe Driving and Maneuvering Techniques:...\n",
      "‚úÖ Found module: Module 4: Load Handling, Stacking, and Unloading P...\n",
      "‚úÖ Found module: Module 5: Workplace Hazards, Emergency Situations,...\n",
      "‚úÖ Found module: Module 6: Hands-On Practical Training and Certific...\n",
      "üìä Total modules found: 6\n",
      "üìö Found 6 modules in syllabus\n",
      "üìú Certificate: https://skillcouncil.in/wp-content/uploads/2025/09/GSDCI-Certiifcate-Final.pdf\n",
      "üö™ Browser closed\n",
      "üóëÔ∏è Removed existing file to avoid duplicates\n",
      "üíæ Saved data for: Certified Certified Forklift Operator\n",
      "üìä Syllabus column contains: 2634 characters\n",
      "\n",
      "‚úÖ Process completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove specified words from text\"\"\"\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- IMPROVED SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    \"\"\"Extract all modules from the specific HTML structure\"\"\"\n",
    "    modules = []\n",
    "    \n",
    "    print(\"üîç Searching for syllabus modules...\")\n",
    "    \n",
    "    # Strategy 1: Look for the specific structure with data-start and data-end attributes\n",
    "    module_paragraphs = soup.find_all('p', attrs={'data-start': True, 'data-end': True})\n",
    "    \n",
    "    for p_tag in module_paragraphs:\n",
    "        strong_tag = p_tag.find('strong')\n",
    "        if strong_tag:\n",
    "            module_title = strong_tag.get_text(strip=True)\n",
    "            full_text = p_tag.get_text(strip=True)\n",
    "            module_content = full_text.replace(module_title, '').strip()\n",
    "            module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "            \n",
    "            if module_title and module_content:\n",
    "                modules.append({\n",
    "                    \"title\": clean_text(module_title),\n",
    "                    \"content\": clean_text(module_content)\n",
    "                })\n",
    "                print(f\"‚úÖ Found module: {module_title[:50]}...\")\n",
    "    \n",
    "    # Strategy 2: span with font-size 12pt\n",
    "    if not modules:\n",
    "        spans_with_strong = soup.find_all('span', style=re.compile(r'font-size:\\s*12pt'))\n",
    "        for span in spans_with_strong:\n",
    "            strong_tag = span.find('strong')\n",
    "            if strong_tag:\n",
    "                module_title = strong_tag.get_text(strip=True)\n",
    "                if 'module' in module_title.lower():\n",
    "                    full_text = span.get_text(strip=True)\n",
    "                    module_content = full_text.replace(module_title, '').strip()\n",
    "                    module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "                    \n",
    "                    if module_title and module_content:\n",
    "                        modules.append({\n",
    "                            \"title\": clean_text(module_title),\n",
    "                            \"content\": clean_text(module_content)\n",
    "                        })\n",
    "    \n",
    "    # Strategy 3: any strong tag containing \"Module\"\n",
    "    if not modules:\n",
    "        all_strong_tags = soup.find_all('strong')\n",
    "        for strong in all_strong_tags:\n",
    "            text = strong.get_text(strip=True)\n",
    "            if 'module' in text.lower():\n",
    "                parent = strong.find_parent(['p', 'div'])\n",
    "                if parent:\n",
    "                    full_text = parent.get_text(strip=True)\n",
    "                    module_content = full_text.replace(text, '').strip()\n",
    "                    module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "                    \n",
    "                    if text and module_content:\n",
    "                        modules.append({\n",
    "                            \"title\": clean_text(text),\n",
    "                            \"content\": clean_text(module_content)\n",
    "                        })\n",
    "    \n",
    "    print(f\"üìä Total modules found: {len(modules)}\")\n",
    "    \n",
    "    # Filter non-module content\n",
    "    filtered_modules = []\n",
    "    for module in modules:\n",
    "        title_lower = module['title'].lower()\n",
    "        if ('module' in title_lower and \n",
    "            not any(word in title_lower for word in ['job', 'career', 'opportunit', 'placement'])):\n",
    "            filtered_modules.append(module)\n",
    "    \n",
    "    return filtered_modules if filtered_modules else [{\"title\": \"Syllabus not available\", \"content\": \"Could not extract syllabus content\"}]\n",
    "\n",
    "# -------------------- CLEAN MODULE TITLES --------------------\n",
    "def clean_module_title(title):\n",
    "    \"\"\"Remove duplicate 'Module X:' from titles\"\"\"\n",
    "    title = re.sub(r'^(Module\\s+\\d+:\\s*)+', '', title)\n",
    "    return title.strip()\n",
    "\n",
    "# -------------------- FORMAT SYLLABUS FOR EXCEL --------------------\n",
    "def format_syllabus_for_excel(modules):\n",
    "    \"\"\"Format the modules into a clean syllabus structure for Excel\"\"\"\n",
    "    if not modules or (len(modules) == 1 and \"not available\" in modules[0][\"title\"].lower()):\n",
    "        return \"Syllabus not available\"\n",
    "    \n",
    "    syllabus_text = \"\"\n",
    "    \n",
    "    for i, module in enumerate(modules, 1):\n",
    "        clean_title = clean_module_title(module['title'])\n",
    "        content = module['content']\n",
    "        if content and content != \"Content not available\":\n",
    "            syllabus_text += f\"Module {i}: {clean_title}: {content}\\n\\n\"\n",
    "        else:\n",
    "            syllabus_text += f\"Module {i}: {clean_title}: Content details to be provided\\n\\n\"\n",
    "    \n",
    "    return syllabus_text.strip()\n",
    "\n",
    "# -------------------- SYLLABUS TRANSFORM --------------------\n",
    "def transform_syllabus(text: str) -> str:\n",
    "    \"\"\"Replace '::' after module headers with a newline, keeping data intact.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    MODULE_HEADER_RE = re.compile(r\"(Module\\s*\\d+\\s*:\\s*[^:]+?)::\\s*\", flags=re.IGNORECASE)\n",
    "\n",
    "    def _repl(m: re.Match) -> str:\n",
    "        return m.group(1) + \"\\n \"\n",
    "\n",
    "    return MODULE_HEADER_RE.sub(_repl, text)\n",
    "\n",
    "# -------------------- IMPROVED SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        print(f\"üåê Accessing URL: {url}\")\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Click description/syllabus tab if available\n",
    "        try:\n",
    "            tabs = driver.find_elements(By.CSS_SELECTOR, \".woocommerce-tabs li a, .tabs li a, .wc-tabs li a\")\n",
    "            for tab in tabs:\n",
    "                if 'description' in tab.text.lower() or 'syllabus' in tab.text.lower() or 'curriculum' in tab.text.lower():\n",
    "                    print(f\"üìë Clicking on tab: {tab.text}\")\n",
    "                    driver.execute_script(\"arguments[0].click();\", tab)\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ÑπÔ∏è No tabs found or clickable: {e}\")\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Course Name\n",
    "        course_name_tag = soup.find(\"h1\", class_=\"product_title entry-title\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "        print(f\"üìõ Course Name: {course_name}\")\n",
    "\n",
    "        # About Course\n",
    "        about_course = \"About course not found\"\n",
    "        about_patterns = [\n",
    "            (\"strong\", re.compile(r\"objective|about|course overview\", re.IGNORECASE)),\n",
    "            (\"h2\", re.compile(r\"course objective|about\", re.IGNORECASE)),\n",
    "            (\"h3\", re.compile(r\"objective|overview\", re.IGNORECASE))\n",
    "        ]\n",
    "        for tag_name, pattern in about_patterns:\n",
    "            elements = soup.find_all(tag_name, string=pattern)\n",
    "            for element in elements:\n",
    "                parent = element.find_parent(['p', 'div'])\n",
    "                if parent:\n",
    "                    about_course = clean_text(parent.get_text(\" \", strip=True))\n",
    "                    break\n",
    "            if about_course != \"About course not found\":\n",
    "                break\n",
    "        print(f\"üìù About Course: {about_course[:100]}...\")\n",
    "\n",
    "        # Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        eligibility_elements = soup.find_all(['strong', 'h3', 'h4'], string=re.compile(r\"eligibility\", re.IGNORECASE))\n",
    "        for element in eligibility_elements:\n",
    "            parent = element.find_parent(['p', 'div'])\n",
    "            if parent:\n",
    "                eligibility = clean_text(parent.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚úÖ Eligibility: {eligibility}\")\n",
    "\n",
    "        # Duration\n",
    "        duration = \"Not available\"\n",
    "        duration_elements = soup.find_all(['strong', 'h3', 'h4'], string=re.compile(r\"duration\", re.IGNORECASE))\n",
    "        for element in duration_elements:\n",
    "            parent = element.find_parent(['p', 'div'])\n",
    "            if parent:\n",
    "                duration = clean_text(parent.get_text(strip=True))\n",
    "                break\n",
    "        print(f\"‚è±Ô∏è Duration: {duration}\")\n",
    "\n",
    "        # Price\n",
    "        price = \"Not available\"\n",
    "        price_selectors = [\".price .amount\", \".woocommerce-Price-amount\", \"bdi\", \".product_price\", \".course-price\"]\n",
    "        for selector in price_selectors:\n",
    "            price_tag = soup.select_one(selector)\n",
    "            if price_tag:\n",
    "                price = price_tag.get_text(strip=True)\n",
    "                if price and price != \"Not available\":\n",
    "                    break\n",
    "        print(f\"üí∞ Price: {price}\")\n",
    "\n",
    "        fee_structure = (\n",
    "            f\"{price} \\n- All other fees remain unchanged\\n\"\n",
    "            \"- Education loans are available through leading banks and NBFCs.\"\n",
    "        ) if price and price != \"Not available\" else (\n",
    "            \"- All other fees remain unchanged\\n\"\n",
    "            \"- Education loans are available through leading banks and NBFCs.\"\n",
    "        )\n",
    "        print(f\"üí≥ Fee Structure: {fee_structure}\")\n",
    "\n",
    "        # Who Should Take It\n",
    "        who_content = []\n",
    "        who_patterns = [\"strong\", \"h3\", \"h4\"]\n",
    "        for pattern in who_patterns:\n",
    "            who_elements = soup.find_all(pattern, string=re.compile(r\"who should|target audience|audience\", re.IGNORECASE))\n",
    "            for element in who_elements:\n",
    "                next_ul = element.find_next(\"ul\")\n",
    "                if next_ul:\n",
    "                    for li in next_ul.find_all(\"li\"):\n",
    "                        who_content.append(clean_text(li.get_text(strip=True)))\n",
    "                else:\n",
    "                    parent = element.find_parent(['p', 'div'])\n",
    "                    if parent:\n",
    "                        text = parent.get_text(strip=True).replace(element.get_text(strip=True), \"\").strip()\n",
    "                        if text:\n",
    "                            who_content.append(text)\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "        print(f\"üë• Who Should Take It: {who_should_take}\")\n",
    "\n",
    "        # Syllabus\n",
    "        modules = extract_syllabus(soup)\n",
    "        syllabus_content = format_syllabus_for_excel(modules)\n",
    "        syllabus_content = transform_syllabus(syllabus_content)\n",
    "        print(f\"üìö Found {len(modules)} modules in syllabus\")\n",
    "\n",
    "        # Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        cert_selectors = [\"a[href*='certificate']\", \"a[href*='.pdf']\", \"img[src*='certificate']\", \"a[href*='certif']\"]\n",
    "        for selector in cert_selectors:\n",
    "            cert_element = soup.select_one(selector)\n",
    "            if cert_element:\n",
    "                if cert_element.has_attr('href'):\n",
    "                    cert_link = cert_element['href']\n",
    "                elif cert_element.has_attr('src'):\n",
    "                    cert_link = cert_element['src']\n",
    "                break\n",
    "        print(f\"üìú Certificate: {cert_link}\")\n",
    "\n",
    "        return course_name, about_course, eligibility, duration, price, who_should_take, syllabus_content, cert_link, fee_structure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"üö™ Browser closed\")\n",
    "\n",
    "# -------------------- SAVE TO EXCEL --------------------\n",
    "def save_to_excel(data, file_path, url):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Eligibility\", \"Duration\",\n",
    "        \"Price\", \"Who Should Take It\", \"syllabus\", \"Certificate\",\n",
    "        \"Fee Structure\", \"Course URL\"\n",
    "    ]\n",
    "    \n",
    "    (course_name, about_course, eligibility, duration, price,\n",
    "     who_should_take, syllabus_content, cert_link, fee_structure) = data\n",
    "     \n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(\"üóëÔ∏è Removed existing file to avoid duplicates\")\n",
    "        \n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        row = {\n",
    "            \"Course Name\": course_name,\n",
    "            \"About Course\": about_course,\n",
    "            \"Eligibility\": eligibility,\n",
    "            \"Duration\": duration,\n",
    "            \"Price\": price,\n",
    "            \"Who Should Take It\": who_should_take,\n",
    "            \"syllabus\": syllabus_content,\n",
    "            \"Certificate\": cert_link,\n",
    "            \"Fee Structure\": fee_structure,\n",
    "            \"Course URL\": url\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"üíæ Saved data for: {course_name}\")\n",
    "        print(f\"üìä Syllabus column contains: {len(syllabus_content)} characters\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Excel save error: {e}\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    course_urls = [\n",
    "        \"https://skillcouncil.in/product/certified-certified-forklift-operator/\"\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ Starting scraping process...\")\n",
    "    file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\course_test.xlsx\"\n",
    "\n",
    "    for course_url in course_urls:\n",
    "        print(f\"\\nüîç Processing: {course_url}\")\n",
    "        course_data = scrape_course_data(course_url)\n",
    "        if all(item != \"Error\" for item in course_data):\n",
    "            save_to_excel(course_data, file_path, course_url)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to scrape complete data for {course_url}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Process completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71a4aa",
   "metadata": {},
   "source": [
    "Entire Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676eec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting batch course scraping...\n",
      "\n",
      "üîç [1/1] Scraping: https://skillcouncil.in/product/certified-industrial-designer/\n",
      "üíæ Saved batch to: C:\\Users\\taslim.siddiqui\\Downloads\\lot.xlsx\n",
      "\n",
      "üìå Preview of first courses:\n",
      "\n",
      "[1] Course Name: Certified Industrial Designer\n",
      "Syllabus (first 200 chars): Module 1: Fundamentals of Industrial Design\n",
      " Design principles, Elements of design, Design thinking process, Ergonomics and human factors, Materials and manufacturing basics, History of industrial des...\n",
      "‚úÖ Process completed!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------- DRIVER SETUP --------------------\n",
    "def get_driver(headless=False):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--window-size=1280,720\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(text):\n",
    "    words_to_remove = [\n",
    "        \"Objective:\", \"Objective :\",\n",
    "        \"Eligibility:\", \"Eligibility :\",\n",
    "        \"Duration:\", \"Duration :\",\n",
    "        \"Professional Skills\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        text = text.replace(word, \"\")\n",
    "    return text.strip()\n",
    "\n",
    "# -------------------- SYLLABUS EXTRACTION --------------------\n",
    "def extract_syllabus(soup):\n",
    "    modules = []\n",
    "    module_paragraphs = soup.find_all('p', attrs={'data-start': True, 'data-end': True})\n",
    "    for p_tag in module_paragraphs:\n",
    "        strong_tag = p_tag.find('strong')\n",
    "        if strong_tag:\n",
    "            module_title = strong_tag.get_text(strip=True)\n",
    "            full_text = p_tag.get_text(strip=True)\n",
    "            module_content = full_text.replace(module_title, '').strip()\n",
    "            module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "            if module_title and module_content:\n",
    "                modules.append({\n",
    "                    \"title\": clean_text(module_title),\n",
    "                    \"content\": clean_text(module_content)\n",
    "                })\n",
    "    # Strategy 2: span with font-size 12pt\n",
    "    if not modules:\n",
    "        spans_with_strong = soup.find_all('span', style=re.compile(r'font-size:\\s*12pt'))\n",
    "        for span in spans_with_strong:\n",
    "            strong_tag = span.find('strong')\n",
    "            if strong_tag:\n",
    "                module_title = strong_tag.get_text(strip=True)\n",
    "                if 'module' in module_title.lower():\n",
    "                    full_text = span.get_text(strip=True)\n",
    "                    module_content = full_text.replace(module_title, '').strip()\n",
    "                    module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "                    if module_title and module_content:\n",
    "                        modules.append({\n",
    "                            \"title\": clean_text(module_title),\n",
    "                            \"content\": clean_text(module_content)\n",
    "                        })\n",
    "    # Strategy 3: any strong tag containing \"Module\"\n",
    "    if not modules:\n",
    "        all_strong_tags = soup.find_all('strong')\n",
    "        for strong in all_strong_tags:\n",
    "            text = strong.get_text(strip=True)\n",
    "            if 'module' in text.lower():\n",
    "                parent = strong.find_parent(['p', 'div'])\n",
    "                if parent:\n",
    "                    full_text = parent.get_text(strip=True)\n",
    "                    module_content = full_text.replace(text, '').strip()\n",
    "                    module_content = re.sub(r'^:\\s*', '', module_content)\n",
    "                    if text and module_content:\n",
    "                        modules.append({\n",
    "                            \"title\": clean_text(text),\n",
    "                            \"content\": clean_text(module_content)\n",
    "                        })\n",
    "    filtered_modules = []\n",
    "    for module in modules:\n",
    "        title_lower = module['title'].lower()\n",
    "        if ('module' in title_lower and not any(word in title_lower for word in ['job', 'career', 'opportunit', 'placement'])):\n",
    "            filtered_modules.append(module)\n",
    "    return filtered_modules if filtered_modules else [{\"title\": \"Syllabus not available\", \"content\": \"Could not extract syllabus content\"}]\n",
    "\n",
    "# -------------------- CLEAN MODULE TITLES --------------------\n",
    "def clean_module_title(title):\n",
    "    title = re.sub(r'^(Module\\s+\\d+:\\s*)+', '', title)\n",
    "    return title.strip()\n",
    "\n",
    "# -------------------- FORMAT SYLLABUS FOR EXCEL --------------------\n",
    "def format_syllabus_for_excel(modules):\n",
    "    if not modules or (len(modules) == 1 and \"not available\" in modules[0][\"title\"].lower()):\n",
    "        return \"Syllabus not available\"\n",
    "    syllabus_text = \"\"\n",
    "    for i, module in enumerate(modules, 1):\n",
    "        clean_title = clean_module_title(module['title'])\n",
    "        content = module['content']\n",
    "        if content and content != \"Content not available\":\n",
    "            syllabus_text += f\"Module {i}: {clean_title}: {content}\\n\\n\"\n",
    "        else:\n",
    "            syllabus_text += f\"Module {i}: {clean_title}: Content details to be provided\\n\\n\"\n",
    "    return syllabus_text.strip()\n",
    "\n",
    "# -------------------- SYLLABUS TRANSFORM --------------------\n",
    "def transform_syllabus(text: str) -> str:\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return text\n",
    "    MODULE_HEADER_RE = re.compile(r\"(Module\\s*\\d+\\s*:\\s*[^:]+?)::\\s*\", flags=re.IGNORECASE)\n",
    "    def _repl(m: re.Match) -> str:\n",
    "        return m.group(1) + \"\\n \"\n",
    "    return MODULE_HEADER_RE.sub(_repl, text)\n",
    "\n",
    "# -------------------- SCRAPER --------------------\n",
    "def scrape_course_data(url):\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(5)\n",
    "        # Click syllabus/description tab if exists\n",
    "        try:\n",
    "            tabs = driver.find_elements(By.CSS_SELECTOR, \".woocommerce-tabs li a, .tabs li a, .wc-tabs li a\")\n",
    "            for tab in tabs:\n",
    "                if 'description' in tab.text.lower() or 'syllabus' in tab.text.lower() or 'curriculum' in tab.text.lower():\n",
    "                    driver.execute_script(\"arguments[0].click();\", tab)\n",
    "                    time.sleep(3)\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Course Name\n",
    "        course_name_tag = soup.find(\"h1\", class_=\"product_title entry-title\")\n",
    "        course_name = course_name_tag.get_text(strip=True) if course_name_tag else \"Course name not found\"\n",
    "\n",
    "        # About Course\n",
    "        about_course = \"About course not found\"\n",
    "        about_patterns = [\n",
    "            (\"strong\", re.compile(r\"objective|about|course overview\", re.IGNORECASE)),\n",
    "            (\"h2\", re.compile(r\"course objective|about\", re.IGNORECASE)),\n",
    "            (\"h3\", re.compile(r\"objective|overview\", re.IGNORECASE))\n",
    "        ]\n",
    "        for tag_name, pattern in about_patterns:\n",
    "            elements = soup.find_all(tag_name, string=pattern)\n",
    "            for element in elements:\n",
    "                parent = element.find_parent(['p', 'div'])\n",
    "                if parent:\n",
    "                    about_course = clean_text(parent.get_text(\" \", strip=True))\n",
    "                    break\n",
    "            if about_course != \"About course not found\":\n",
    "                break\n",
    "\n",
    "        # Eligibility\n",
    "        eligibility = \"Not available\"\n",
    "        eligibility_elements = soup.find_all(['strong', 'h3', 'h4'], string=re.compile(r\"eligibility\", re.IGNORECASE))\n",
    "        for element in eligibility_elements:\n",
    "            parent = element.find_parent(['p', 'div'])\n",
    "            if parent:\n",
    "                eligibility = clean_text(parent.get_text(strip=True))\n",
    "                break\n",
    "\n",
    "        # Duration\n",
    "        duration = \"Not available\"\n",
    "        duration_elements = soup.find_all(['strong', 'h3', 'h4'], string=re.compile(r\"duration\", re.IGNORECASE))\n",
    "        for element in duration_elements:\n",
    "            parent = element.find_parent(['p', 'div'])\n",
    "            if parent:\n",
    "                duration = clean_text(parent.get_text(strip=True))\n",
    "                break\n",
    "\n",
    "        # Price\n",
    "        price = \"Not available\"\n",
    "        price_selectors = [\".price .amount\", \".woocommerce-Price-amount\", \"bdi\", \".product_price\", \".course-price\"]\n",
    "        for selector in price_selectors:\n",
    "            price_tag = soup.select_one(selector)\n",
    "            if price_tag:\n",
    "                price = price_tag.get_text(strip=True)\n",
    "                if price and price != \"Not available\":\n",
    "                    break\n",
    "\n",
    "        fee_structure = (\n",
    "            f\"{price} \\n- All other fees remain unchanged\\n- Education loans are available through leading banks and NBFCs.\"\n",
    "        ) if price and price != \"Not available\" else \"- All other fees remain unchanged\\n- Education loans are available through leading banks and NBFCs.\"\n",
    "\n",
    "        # Who Should Take It\n",
    "        who_content = []\n",
    "        who_patterns = [\"strong\", \"h3\", \"h4\"]\n",
    "        for pattern in who_patterns:\n",
    "            who_elements = soup.find_all(pattern, string=re.compile(r\"who should|target audience|audience\", re.IGNORECASE))\n",
    "            for element in who_elements:\n",
    "                next_ul = element.find_next(\"ul\")\n",
    "                if next_ul:\n",
    "                    for li in next_ul.find_all(\"li\"):\n",
    "                        who_content.append(clean_text(li.get_text(strip=True)))\n",
    "                else:\n",
    "                    parent = element.find_parent(['p', 'div'])\n",
    "                    if parent:\n",
    "                        text = parent.get_text(strip=True).replace(element.get_text(strip=True), \"\").strip()\n",
    "                        if text:\n",
    "                            who_content.append(text)\n",
    "        who_should_take = \"\\n\".join([f\"- {item}\" for item in who_content]) if who_content else \"Not available\"\n",
    "\n",
    "        # Syllabus\n",
    "        modules = extract_syllabus(soup)\n",
    "        syllabus_content = format_syllabus_for_excel(modules)\n",
    "        syllabus_content = transform_syllabus(syllabus_content)\n",
    "\n",
    "        # Certificate\n",
    "        cert_link = \"Certificate not available\"\n",
    "        cert_selectors = [\"a[href*='certificate']\", \"a[href*='.pdf']\", \"img[src*='certificate']\", \"a[href*='certif']\"]\n",
    "        for selector in cert_selectors:\n",
    "            cert_element = soup.select_one(selector)\n",
    "            if cert_element:\n",
    "                if cert_element.has_attr('href'):\n",
    "                    cert_link = cert_element['href']\n",
    "                elif cert_element.has_attr('src'):\n",
    "                    cert_link = cert_element['src']\n",
    "                break\n",
    "\n",
    "        return course_name, about_course, eligibility, duration, price, who_should_take, syllabus_content, cert_link, fee_structure\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Scraping failed for {url}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return [\"Error\"] * 9\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# -------------------- READ URLS --------------------\n",
    "def read_course_urls(input_file_path):\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"‚ùå Input file not found: {input_file_path}\")\n",
    "        return []\n",
    "    df = pd.read_excel(input_file_path)\n",
    "    if 'Course URL' not in df.columns:\n",
    "        print(\"‚ùå 'Course URL' column missing in input Excel\")\n",
    "        return []\n",
    "    return df['Course URL'].dropna().tolist()\n",
    "\n",
    "# -------------------- SAVE BATCH TO EXCEL --------------------\n",
    "def save_batch_to_excel(all_course_data, output_file_path):\n",
    "    columns = [\n",
    "        \"Course Name\", \"About Course\", \"Eligibility\", \"Duration\",\n",
    "        \"Price\", \"Who Should Take It\", \"syllabus\", \"Certificate\",\n",
    "        \"Fee Structure\", \"Course URL\"\n",
    "    ]\n",
    "    df = pd.DataFrame(all_course_data, columns=columns)\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"üíæ Saved batch to: {output_file_path}\")\n",
    "\n",
    "# -------------------- OUTPUT PREVIEW --------------------\n",
    "def print_output_preview(all_course_data, n=3):\n",
    "    print(\"\\nüìå Preview of first courses:\")\n",
    "    for i, row in enumerate(all_course_data[:n], 1):\n",
    "        print(f\"\\n[{i}] Course Name: {row[0]}\")\n",
    "        print(f\"Syllabus (first 200 chars): {row[6][:200]}...\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "def main():\n",
    "    input_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\excel.xlsx\"\n",
    "    output_file_path = r\"C:\\Users\\taslim.siddiqui\\Downloads\\lot.xlsx\"\n",
    "\n",
    "    print(\"üöÄ Starting batch course scraping...\")\n",
    "    course_urls = read_course_urls(input_file_path)\n",
    "    if not course_urls:\n",
    "        return\n",
    "\n",
    "    all_course_data = []\n",
    "    for i, url in enumerate(course_urls, 1):\n",
    "        print(f\"\\nüîç [{i}/{len(course_urls)}] Scraping: {url}\")\n",
    "        data = scrape_course_data(url)\n",
    "        full_data = list(data) + [url]\n",
    "        all_course_data.append(full_data)\n",
    "        time.sleep(2)\n",
    "\n",
    "    save_batch_to_excel(all_course_data, output_file_path)\n",
    "    print_output_preview(all_course_data)\n",
    "    print(\"‚úÖ Process completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
