{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e74950",
   "metadata": {},
   "source": [
    "# Global Institute of Regulatory link extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_courses_from_boxes(soup, base_url):\n",
    "    link_to_name = {}\n",
    "    for box in soup.select('.single-causes-box'):\n",
    "        link = box.select_one('h4 a')\n",
    "        if link:\n",
    "            course_name = link.get_text(strip=True)\n",
    "            course_link = urljoin(base_url, link['href']).strip()\n",
    "            if course_link not in link_to_name:\n",
    "                link_to_name[course_link] = course_name\n",
    "    return link_to_name\n",
    "\n",
    "def extract_courses_from_footer(soup, base_url):\n",
    "    link_to_name = {}\n",
    "    for footer_section in soup.select('.f-widget'):\n",
    "        if \"Our Courses\" in footer_section.get_text():\n",
    "            for link in footer_section.select('a[href]'):\n",
    "                course_name = link.get_text(strip=True)\n",
    "                course_link = urljoin(base_url, link['href']).strip()\n",
    "                if course_link not in link_to_name:\n",
    "                    link_to_name[course_link] = course_name\n",
    "    return link_to_name\n",
    "\n",
    "def extract_courses_from_courses_page(soup, base_url):\n",
    "    link_to_name = {}\n",
    "    for link in soup.select('a[href]'):\n",
    "        course_name = link.get_text(strip=True)\n",
    "        href = link.get('href')\n",
    "        if href and course_name and (\n",
    "            '/course' in href or '/diploma' in href or '/training' in href or '/validation' in href\n",
    "        ):\n",
    "            course_link = urljoin(base_url, href).strip()\n",
    "            if course_link not in link_to_name:\n",
    "                link_to_name[course_link] = course_name\n",
    "    return link_to_name\n",
    "\n",
    "def collect_all_links(soup, base_url):\n",
    "    all_links = set()\n",
    "    for a in soup.select('a[href]'):\n",
    "        href = urljoin(base_url, a['href']).strip()\n",
    "        all_links.add(href)\n",
    "    return all_links\n",
    "\n",
    "def find_links_with_multiple_names(soup, base_url):\n",
    "    link_names = defaultdict(list)\n",
    "    for a in soup.select('a[href]'):\n",
    "        href = urljoin(base_url, a['href']).strip()\n",
    "        name = a.get_text(strip=True)\n",
    "        if href and name:\n",
    "            link_names[href].append(name)\n",
    "    \n",
    "    for link, names in link_names.items():\n",
    "        if len(set(names)) > 1:\n",
    "            print(f\"‚ö†Ô∏è Link with multiple names: {link} -> {set(names)}\")\n",
    "\n",
    "def extract_all_courses(html_content, base_url):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    combined = {}\n",
    "\n",
    "    # Combine from all sources\n",
    "    sources = [\n",
    "        extract_courses_from_boxes(soup, base_url),\n",
    "        extract_courses_from_footer(soup, base_url),\n",
    "        extract_courses_from_courses_page(soup, base_url),\n",
    "    ]\n",
    "\n",
    "    for source in sources:\n",
    "        for link, name in source.items():\n",
    "            if link not in combined:\n",
    "                combined[link] = name\n",
    "\n",
    "    # Log multiple names for same link\n",
    "    find_links_with_multiple_names(soup, base_url)\n",
    "\n",
    "    # Log all found links for audit\n",
    "    all_links = collect_all_links(soup, base_url)\n",
    "    print(f\"üîç Total unique hrefs found on page (for audit): {len(all_links)}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame([(name, link) for link, name in combined.items()],\n",
    "                      columns=[\"Course Name\", \"Course Link\"])\n",
    "    df = df.sort_values(\"Course Name\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    file_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\Global Institute of Regulatory Affairs-Pharmaceutical Training Courses.html\"\n",
    "    base_url = \"https://www.regulatoryinstitute.com/\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    df = extract_all_courses(html, base_url)\n",
    "\n",
    "    print(f\"\\n‚úÖ Found {len(df)} unique course links:\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    output_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\GIRA_Courses_UniqueByLink.xlsx\"\n",
    "    df.to_excel(output_path, index=False)\n",
    "    print(f\"\\nüìÇ Saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
