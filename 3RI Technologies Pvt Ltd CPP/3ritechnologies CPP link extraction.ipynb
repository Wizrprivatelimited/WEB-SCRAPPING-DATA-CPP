{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532bd678",
   "metadata": {},
   "source": [
    "# CPP 3ritechnologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6772496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configure base URL and headers\n",
    "base_url = \"https://www.3ritechnologies.com\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "course_data = []\n",
    "\n",
    "def scrape_courses():\n",
    "    try:\n",
    "        print(f\"üåê Connecting to {base_url}...\")\n",
    "        response = requests.get(base_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # First try: Check if courses are listed in navigation menu\n",
    "        print(\"üîç Checking main navigation for courses...\")\n",
    "        nav_menu = soup.select('.menu-item a')\n",
    "        for item in nav_menu:\n",
    "            href = item.get('href', '')\n",
    "            if '/course/' in href or '/training/' in href:\n",
    "                course_name = item.get_text(strip=True)\n",
    "                course_url = urljoin(base_url, href)\n",
    "                if not any(c['url'] == course_url for c in course_data):\n",
    "                    course_data.append({\n",
    "                        'name': course_name,\n",
    "                        'url': course_url\n",
    "                    })\n",
    "                    print(f\"‚úÖ Found in menu: {course_name} - {course_url}\")\n",
    "        \n",
    "        # Second try: Check for course cards/grids\n",
    "        print(\"üîç Scanning for course cards...\")\n",
    "        course_cards = soup.select('.elementor-widget-wrap a, .course-card a, .eael-course a')\n",
    "        for card in course_cards:\n",
    "            href = card.get('href', '')\n",
    "            if ('/course/' in href or '/training/' in href) and not href.endswith('/course/'):\n",
    "                course_name = card.get_text(strip=True)\n",
    "                if len(course_name) > 3:  # Filter out very short names\n",
    "                    course_url = urljoin(base_url, href)\n",
    "                    if not any(c['url'] == course_url for c in course_data):\n",
    "                        course_data.append({\n",
    "                            'name': course_name,\n",
    "                            'url': course_url\n",
    "                        })\n",
    "                        print(f\"‚úÖ Found in cards: {course_name} - {course_url}\")\n",
    "        \n",
    "        # Third try: Check dedicated course pages\n",
    "        course_pages = [\n",
    "            '/courses/',\n",
    "            '/training/',\n",
    "            '/programs/',\n",
    "            '/it-courses/'\n",
    "        ]\n",
    "        \n",
    "        for page in course_pages:\n",
    "            print(f\"üîç Scanning {page} page...\")\n",
    "            page_url = urljoin(base_url, page)\n",
    "            try:\n",
    "                page_response = requests.get(page_url, headers=headers, timeout=10)\n",
    "                if page_response.status_code == 200:\n",
    "                    page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "                    courses = page_soup.select('a[href*=\"/course/\"], a[href*=\"/training/\"]')\n",
    "                    for course in courses:\n",
    "                        href = course.get('href', '')\n",
    "                        if not href.endswith(('/course/', '/courses/', '/training/')):\n",
    "                            course_name = course.get_text(strip=True)\n",
    "                            if len(course_name) > 3:\n",
    "                                course_url = urljoin(base_url, href)\n",
    "                                if not any(c['url'] == course_url for c in course_data):\n",
    "                                    course_data.append({\n",
    "                                        'name': course_name,\n",
    "                                        'url': course_url\n",
    "                                    })\n",
    "                                    print(f\"‚úÖ Found on {page}: {course_name} - {course_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error scanning {page_url}: {str(e)}\")\n",
    "        \n",
    "        # Save results\n",
    "        if course_data:\n",
    "            df = pd.DataFrame(course_data)\n",
    "            df = df.drop_duplicates(subset=['url'])\n",
    "            output_path = \"C:\\\\Users\\\\taslim.siddiqui\\\\Downloads\\\\3RI_Courses_List.xlsx\"\n",
    "            df.to_excel(output_path, index=False, columns=['name', 'url'])\n",
    "            \n",
    "            print(\"\\nüìã Final Course List:\")\n",
    "            for course in course_data:\n",
    "                print(f\"{course['name']}\\t{course['url']}\")\n",
    "                \n",
    "            print(f\"\\n‚úÖ Success! Found {len(df)} courses. Saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"‚ùå No courses found. The website structure may have changed.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {str(e)}\")\n",
    "\n",
    "# Run the scraper\n",
    "scrape_courses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
